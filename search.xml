<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HTTPS加密过程]]></title>
    <url>%2F2019%2F05%2F29%2FHTTPS%E5%8A%A0%E5%AF%86%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[认证服务器浏览器内置一个受信任的CA机构列表，并保存了这些CA机构的证书。第一阶段服务器会提供经CA机构认证颁发的服务器证书，如果认证该服务器证书的CA机构，存在于浏览器的受信任CA机构列表中，并且服务器证书中的信息与当前正在访问的网站（域名等）一致，那么浏览器就认为服务端是可信的，并从服务器证书中取得服务器公钥，用于后续流程。否则，浏览器将提示用户，根据用户的选择，决定是否继续。当然，我们可以管理这个受信任CA机构列表，添加我们想要信任的CA机构，或者移除我们不信任的CA机构。 协商会话密钥客户端在认证完服务器，获得服务器的公钥之后，利用该公钥与服务器进行加密通信，协商出两个会话密钥，分别是用于加密客户端往服务端发送数据的客户端会话密钥，用于加密服务端往客户端发送数据的服务端会话密钥。在已有服务器公钥，可以加密通讯的前提下，还要协商两个对称密钥的原因，是因为非对称加密相对复杂度更高，在数据传输过程中，使用对称加密，可以节省计算资源。另外，会话密钥是随机生成，每次协商都会有不一样的结果，所以安全性也比较高。 加密通讯此时客户端服务器双方都有了本次通讯的会话密钥，之后传输的所有的Http数据，都通过会话密钥加密。这样网路上的其它用户，将很难窃取和篡改客户端和服务端之间传输的数据，从而保证了数据的私密性和完整性。]]></content>
      <categories>
        <category>Https</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[快速了解23种设计模式]]></title>
    <url>%2F2019%2F05%2F15%2F%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A323%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[设计模式的分类设计模式有两种分类方法，即根据模式的目的来分和根据模式的作用的范围来分。 根据目的来分根据模式是用来完成什么工作来划分，这种方式可分为创建型模式、结构型模式和行为型模式3种。 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。GoF 中提供了单例、原型、工厂方法、抽象工厂、建造者等 5 种创建型模式。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构，GoF 中提供了代理、适配器、桥接、装饰、外观、享元、组合等 7 种结构型模式。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。GoF 中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等 11 种行为型模式。 根据作用范围来分根据模式是主要用于类上还是主要用于对象上来分，这种方式可分为类模式和对象模式两种。 类模式：用于处理类与子类之间的关系，这些关系通过继承来建立，是静态的，在编译时刻便确定下来了。GoF中的工厂方法、（类）适配器、模板方法、解释器属于该模式。 对象模式：用于处理对象之间的关系，这些关系可以通过组合或聚合来实现，在运行时刻是可以变化的，更具动态性。GoF 中除了以上 4 种，其他的都是对象模式。 设计模式的六大原则总原则：开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等。 单一职责原则不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。 里氏替换原则（Liskov Substitution Principle）里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。—— From Baidu 百科 历史替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 依赖倒转原则（Dependence Inversion Principle）这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 接口隔离原则（Interface Segregation Principle）这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 迪米特法则（最少知道原则）（Demeter Principle）就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。 最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。 合成复用原则（Composite Reuse Principle）原则是尽量首先使用合成/聚合的方式，而不是使用继承。 Java的23中设计模式 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 总结设计模式的本质是面向对象设计原则的实际运用，是对类的封装性、继承性和多态性以及类的关联关系和组合关系的充分理解。正确使用设计模式具有以下优点。 可以提高程序员的思维能力、编程能力和设计能力。 使程序设计更加标准化、代码编制更加工程化，使软件开发效率大大提高，从而缩短软件的开发周期。 使设计的代码可重用性高、可读性强、可靠性高、灵活性好、可维护性强。 当然，软件设计模式只是一个引导。在具体的软件幵发中，必须根据设计的应用系统的特点和要求来恰当选择。对于简单的程序开发，苛能写一个简单的算法要比引入某种设计模式更加容易。但对大项目的开发或者框架设计，用设计模式来组织代码显然更好。 参考博客：https://blog.csdn.net/tsite/article/details/62420091https://www.cnblogs.com/geek6/p/3951677.htmlhttp://c.biancheng.net/view/1320.html]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基准测试神器-JMH]]></title>
    <url>%2F2019%2F05%2F14%2F%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%A5%9E%E5%99%A8-JMH%2F</url>
    <content type="text"><![CDATA[原文地址：https://sq.163yun.com/blog/article/179671960481783808 概述性能测试这个话题非常庞大，我们可以从网络聊到操作系统，再从操作系统聊到内核，再从内核聊到你怀疑人生有木有。 先拍几个砖出来吧，我在写代码的时候经常有这种怀疑：写法A快还是写法B快，某个位置是用ArrayList还是LinkedList，HashMap还是TreeMap，HashMap的初始化size要不要指定，指定之后究竟比默认的DEFAULT_SIZE性能好多少。。。 如果你还是通过for循环或者手撸method来测试你的内容的话，那么JMH就是你必须要明白的内容了，因为已经有人把基准测试的轮子造好了，接下来我们就一起看看这个轮子怎么用： JMH只适合细粒度的方法测试，并不适用于系统之间的链路测试！JMH只适合细粒度的方法测试，并不适用于系统之间的链路测试！JMH只适合细粒度的方法测试，并不适用于系统之间的链路测试！ JMH入门JMH是一个工具包，如果我们要通过JMH进行基准测试的话，直接在我们的pom文件中引入JMH的依赖即可： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-core&lt;/artifactId&gt; &lt;version&gt;1.19&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-generator-annprocess&lt;/artifactId&gt; &lt;version&gt;1.19&lt;/version&gt;&lt;/dependency&gt; 通过一个HelloWorld程序来看一下JMH如果工作： @Warmup(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)public class JMHSample_01_HelloWorld { static class Demo { int id; String name; public Demo(int id, String name) { this.id = id; this.name = name; } } static List&lt;Demo&gt; demoList; static { demoList = new ArrayList(); for (int i = 0; i &lt; 10000; i ++) { demoList.add(new Demo(i, &quot;test&quot;)); } } @Benchmark @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.MICROSECONDS) public void testHashMapWithoutSize() { Map map = new HashMap(); for (Demo demo : demoList) { map.put(demo.id, demo.name); } } @Benchmark @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.MICROSECONDS) public void testHashMap() { Map map = new HashMap((int)(demoList.size() / 0.75f) + 1); for (Demo demo : demoList) { map.put(demo.id, demo.name); } } public static void main(String[] args) throws RunnerException { Options opt = new OptionsBuilder() .include(JMHSample_01_HelloWorld.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); }} ======================================执行结果======================================Benchmark Mode Cnt Score Error UnitschargeProject.List.JMHSample_01_HelloWorld.testHashMap avgt 5 130.865 ± 5.851 us/opchargeProject.List.JMHSample_01_HelloWorld.testHashMapWithoutSize avgt 5 172.087 ± 66.252 us/op======================================执行结果====================================== 上面的代码用中文翻译一下：分别定义两个基准测试的方法testHashMapWithoutSize和 testHashMap，这两个基准测试方法执行流程是：每个方法执行前都进行5次预热执行，每隔1秒进行一次预热操作，预热执行结束之后进行5次实际测量执行，每隔1秒进行一次实际执行，我们此次基准测试测量的是平均响应时长，单位是us。 预热？为什么要预热？因为 JVM 的 JIT 机制的存在，如果某个函数被调用多次之后，JVM 会尝试将其编译成为机器码从而提高执行速度。为了让 benchmark 的结果更加接近真实情况就需要进行预热。 从上面的执行结果我们看出，针对一个Map的初始化参数的给定其实有很大影响，当我们给定了初始化参数执行执行的速度是没给定参数的2/3，这个优化速度还是比较明显的，所以以后大家在初始化Map的时候能给定参数最好都给定了，代码是处处优化的，积少成多。 通过上面的内容我们已经基本可以看出来JMH的写法雏形了，后面的介绍主要是一些注解的使用： @Benchmark@Benchmark标签是用来标记测试方法的，只有被这个注解标记的话，该方法才会参与基准测试，但是有一个基本的原则就是被@Benchmark标记的方法必须是public的。 @Warmup@Warmup用来配置预热的内容，可用于类或者方法上，越靠近执行方法的地方越准确。一般配置warmup的参数有这些： iterations：预热的次数。 time：每次预热的时间。 timeUnit：时间单位，默认是s。 batchSize：批处理大小，每次操作调用几次方法。（后面用到） @Measurement用来控制实际执行的内容，配置的选项本warmup一样。 @BenchmarkMode@BenchmarkMode主要是表示测量的纬度，有以下这些纬度可供选择： Mode.Throughput 吞吐量纬度 Mode.AverageTime 平均时间 Mode.SampleTime 抽样检测 Mode.SingleShotTime 检测一次调用 Mode.All 运用所有的检测模式 在方法级别指定@BenchmarkMode的时候可以一定指定多个纬度，例如： @BenchmarkMode({Mode.Throughput, Mode.AverageTime, Mode.SampleTime, Mode.SingleShotTime})，代表同时在多个纬度对目标方法进行测量。 @OutputTimeUnit@OutputTimeUnit代表测量的单位，比如秒级别，毫秒级别，微妙级别等等。一般都使用微妙和毫秒级别的稍微多一点。该注解可以用在方法级别和类级别，当用在类级别的时候会被更加精确的方法级别的注解覆盖，原则就是离目标更近的注解更容易生效。 @State在很多时候我们需要维护一些状态内容，比如在多线程的时候我们会维护一个共享的状态，这个状态值可能会在每隔线程中都一样，也有可能是每个线程都有自己的状态，JMH为我们提供了状态的支持。该注解只能用来标注在类上，因为类作为一个属性的载体。 @State的状态值主要有以下几种： Scope.Benchmark 该状态的意思是会在所有的Benchmark的工作线程中共享变量内容。 Scope.Group 同一个Group的线程可以享有同样的变量 Scope.Thread 每隔线程都享有一份变量的副本，线程之间对于变量的修改不会相互影响。 下面看两个常见的@State的写法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541.直接在内部类中使用@State作为“PropertyHolder”public class JMHSample_03_States &#123; @State(Scope.Benchmark) public static class BenchmarkState &#123; volatile double x = Math.PI; &#125; @State(Scope.Thread) public static class ThreadState &#123; volatile double x = Math.PI; &#125; @Benchmark public void measureUnshared(ThreadState state) &#123; state.x++; &#125; @Benchmark public void measureShared(BenchmarkState state) &#123; state.x++; &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_03_States.class.getSimpleName()) .threads(4) .forks(1) .build(); new Runner(opt).run(); &#125;&#125;2.在Main类中直接使用@State作为注解，是Main类直接成为“PropertyHolder”@State(Scope.Thread)public class JMHSample_04_DefaultState &#123; double x = Math.PI; @Benchmark public void measure() &#123; x++; &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_04_DefaultState.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); &#125;&#125; 我们试想以下@State的含义，它主要是方便框架来控制变量的过程逻辑，通过@State标示的类都被用作属性的容器，然后框架可以通过自己的控制来配置不同级别的隔离情况。被@Benchmark标注的方法可以有参数，但是参数必须是被@State注解的，就是为了要控制参数的隔离。 但是有些情况下我们需要对参数进行一些初始化或者释放的操作，就像Spring提供的一些init和destory方法一样，JHM也提供有这样的钩子： @Setup 必须标示在@State注解的类内部，表示初始化操作 @TearDown 必须表示在@State注解的类内部，表示销毁操作 初始化和销毁的动作都只会执行一次。 1234567891011121314151617181920212223242526272829@State(Scope.Thread)public class JMHSample_05_StateFixtures &#123; double x; @Setup public void prepare() &#123; x = Math.PI; &#125; @TearDown public void check() &#123; assert x &gt; Math.PI : &quot;Nothing changed?&quot;; &#125; @Benchmark public void measureRight() &#123; x++; &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_05_StateFixtures.class.getSimpleName()) .forks(1) .jvmArgs(&quot;-ea&quot;) .build(); new Runner(opt).run(); &#125;&#125; 虽然我们可以执行初始化和销毁的动作，但是总是感觉还缺点啥？对，就是初始化的粒度。因为基准测试往往会执行多次，那么能不能保证每次执行方法的时候都初始化一次变量呢？ @Setup和@TearDown提供了以下三种纬度的控制： Level.Trial 只会在个基础测试的前后执行。包括Warmup和Measurement阶段，一共只会执行一次。 Level.Iteration 每次执行记住测试方法的时候都会执行，如果Warmup和Measurement都配置了2次执行的话，那么@Setup和@TearDown配置的方法的执行次数就4次。 Level.Invocation 每个方法执行的前后执行（一般不推荐这么用） @Param在很多情况下，我们需要测试不同的参数的不同结果，但是测试的了逻辑又都是一样的，因此如果我们编写镀铬benchmark的话会造成逻辑的冗余，幸好JMH提供了@Param参数来帮助我们处理这个事情，被@Param注解标示的参数组会一次被benchmark消费到。 1234567891011121314151617181920@State(Scope.Benchmark)public class ParamTest &#123; @Param(&#123;&quot;1&quot;, &quot;2&quot;, &quot;3&quot;&#125;) int testNum; @Benchmark public String test() &#123; return String.valueOf(testNum); &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(ParamTest.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); &#125;&#125; @Threads测试线程的数量，可以配置在方法或者类上，代表执行测试的线程数量。 通常看到这里我们会比较迷惑Iteration和Invocation区别，我们在配置Warmup的时候默认的时间是的1s，即1s的执行作为一个Iteration，假设每次方法的执行是100ms的话，那么1个Iteration就代表10个Invocation。 JMH进阶通过以上的内容我们已经基本可以掌握JMH的使用了，下面就主要介绍一下JMH提供的一些高级特性了。 不要编写无用代码 因为现代的编译器非常聪明，如果我们在代码使用了没有用处的变量的话，就容易被编译器优化掉，这就会导致实际的测量结果可能不准确，因为我们要在测量的方法中避免使用void方法，然后记得在测量的结束位置返回结果。这么做的目的很明确，就是为了与编译器斗智斗勇，让编译器不要改变这段代码执行的初衷。 Blackhole介绍Blackhole会消费传进来的值，不提供任何信息来确定这些值是否在之后被实际使用。 Blackhole处理的事情主要有以下几种： 死代码消除：入参应该在每次都被用到，因此编译器就不会把这些参数优化为常量或者在计算的过程中对他们进行其他优化。 处理内存壁：我们需要尽可能减少写的量，因为它会干扰缓存，污染写缓冲区等。 这很可能导致过早地撞到内存壁 我们在上面说到需要消除无用代码，那么其中一种方式就是通过Blackhole，我们可以用Blackhole来消费这些返回的结果。 1234567891011121:返回测试结果，防止编译器优化@Benchmarkpublic double measureRight_1() &#123; return Math.log(x1) + Math.log(x2);&#125;2.通过Blackhole消费中间结果，防止编译器优化@Benchmarkpublic void measureRight_2(Blackhole bh) &#123; bh.consume(Math.log(x1)); bh.consume(Math.log(x2));&#125; 循环处理我们虽然可以在Benchmark中定义循环逻辑，但是这么做其实是不合适的，因为编译器可能会将我们的循环进行展开或者做一些其他方面的循环优化，所以JHM建议我们不要在Beanchmark中使用循环，如果我们需要处理循环逻辑了，可以结合@BenchmarkMode(Mode.SingleShotTime)和@Measurement(batchSize = N)来达到同样的效果. 123456789101112131415161718192021222324252627282930@State(Scope.Thread)public class JMHSample_26_BatchSize &#123; List&lt;String&gt; list = new LinkedList&lt;&gt;(); // 每个iteration中做5000次Invocation @Benchmark @Warmup(iterations = 5, batchSize = 5000) @Measurement(iterations = 5, batchSize = 5000) @BenchmarkMode(Mode.SingleShotTime) public List&lt;String&gt; measureRight() &#123; list.add(list.size() / 2, &quot;something&quot;); return list; &#125; @Setup(Level.Iteration) public void setup()&#123; list.clear(); &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_26_BatchSize.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); &#125;&#125; 方法内联方法内联：如果JVM监测到一些小方法被频繁的执行，它会把方法的调用替换成方法体本身。比如说下面这个： 1234567private int add4(int x1, int x2, int x3, int x4) &#123; return add2(x1, x2) + add2(x3, x4); &#125; private int add2(int x1, int x2) &#123; return x1 + x2; &#125; 运行一段时间后JVM会把add2方法去掉，并把你的代码翻译成： 123private int add4(int x1, int x2, int x3, int x4) &#123; return x1 + x2 + x3 + x4; &#125; JMH提供了CompilerControl注解来控制方法内联，但是实际上我感觉比较有用的就是两个了： CompilerControl.Mode.DONT_INLINE：强制限制不能使用内联 CompilerControl.Mode.INLINE：强制使用内联 看一下官方提供的例子把： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@State(Scope.Thread)@BenchmarkMode(Mode.AverageTime)@OutputTimeUnit(TimeUnit.NANOSECONDS)public class JMHSample_16_CompilerControl &#123; public void target_blank() &#123; &#125; @CompilerControl(CompilerControl.Mode.DONT_INLINE) public void target_dontInline() &#123; &#125; @CompilerControl(CompilerControl.Mode.INLINE) public void target_inline() &#123; &#125; @Benchmark public void baseline() &#123; &#125; @Benchmark public void dontinline() &#123; target_dontInline(); &#125; @Benchmark public void inline() &#123; target_inline(); &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_16_CompilerControl.class.getSimpleName()) .warmupIterations(0) .measurementIterations(3) .forks(1) .build(); new Runner(opt).run(); &#125;&#125;======================================执行结果==============================Benchmark Mode Cnt Score Error UnitsJMHSample_16_CompilerControl.baseline avgt 3 0.896 ± 3.426 ns/opJMHSample_16_CompilerControl.dontinline avgt 3 0.344 ± 0.126 ns/opJMHSample_16_CompilerControl.inline avgt 3 0.391 ± 2.622 ns/op======================================执行结果==============================]]></content>
      <categories>
        <category>JMH</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[八大排序算法总结基于java实现]]></title>
    <url>%2F2019%2F05%2F13%2F%E5%85%AB%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%E5%9F%BA%E4%BA%8Ejava%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[原文链接：https://github.com/iTimeTraveler/SortAlgorithms 概述对常用的八大排序算法进行总结。 直接插入排序 希尔排序 简单选择排序 堆排序 冒泡排序 快速排序 归并排序 基数排序 它们都属于内部排序，也就是只考虑数据量较小仅需要使用内存的排序算法，他们之间关系如下： 直接插入排序（Insertion Sort）插入排序的设计初衷是往有序的数组中快速插入一个新的元素。它的算法思想是：把要排序的数组分为了两个部分, 一部分是数组的全部元素(除去待插入的元素), 另一部分是待插入的元素; 先将第一部分排序完成, 然后再插入这个元素.。其中第一部分的排序也是通过再次拆分为两部分来进行的。 插入排序由于操作不尽相同, 可分为 直接插入排序 , 折半插入排序(又称二分插入排序), 链表插入排序 , 希尔排序 。我们先来看下直接插入排序。 基本思想直接插入排序的基本思想是：将数组中的所有元素依次跟前面已经排好的元素相比较，如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过为止。 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： ①. 从第一个元素开始，该元素可以认为已经被排序②. 取出下一个元素，在已经排序的元素序列中从后向前扫描③. 如果该元素（已排序）大于新元素，将该元素移到下一位置④. 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置⑤. 将新元素插入到该位置后⑥. 重复步骤②~⑤ 算法实现中比较有意思的一点是，在每次比较操作发现取出来的新元素小于等于已排序的元素时，可以将已排序的元素移到下一位置，然后将取出来的新元素插入该位置（即相邻位置对调），接着再与前面的已排序的元素进行比较，如上图所示，这样做缺点是交换操作代价比较大。另一种做法是：将新元素取出（挖坑），从左到右依次与已排序的元素比较，如果已排序的元素大于取出的新元素，那么将该元素移动到下一个位置（填坑），接着再与前面的已排序的元素比较，直到找到已排序的元素小于等于新元素的位置，这时再将新元素插入进去。就像基本思想中的动图演示的那样。 如果比较操作的代价比交换操作大的话，可以采用二分查找法来减少比较操作的数目。可以认为是插入排序的一个变种，称为二分查找插入排序。 代码实现1234567891011121314151617181920212223242526272829303132333435/** * 插入排序 * * 1. 从第一个元素开始，该元素可以认为已经被排序 * 2. 取出下一个元素，在已经排序的元素序列中从后向前扫描 * 3. 如果该元素（已排序）大于新元素，将该元素移到下一位置 * 4. 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 * 5. 将新元素插入到该位置后 * 6. 重复步骤2~5 * @param arr 待排序数组 */public static void insertionSort(int[] arr)&#123; for( int i = 1; i &lt; arr.length; i++ ) &#123; int temp = arr[i]; // 取出下一个元素，在已经排序的元素序列中从后向前扫描 for( int j = i; j &gt;= 0; j-- ) &#123; if( j &gt; 0 &amp;&amp; arr[j-1] &gt; temp ) &#123; arr[j] = arr[j-1]; // 如果该元素（已排序）大于取出的元素temp，将该元素移到下一位置 System.out.println(&quot;Temping: &quot; + Arrays.toString(arr)); &#125; else &#123; // 将新元素插入到该位置后 arr[j] = temp; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); break; &#125; &#125; &#125;&#125;// 交换次数较多的实现public static void insertionSort(int[] arr)&#123; for( int i=0; i&lt;arr.length-1; i++ ) &#123; for( int j=i+1; j&gt;0; j-- ) &#123; if( arr[j-1] &lt;= arr[j] ) break; int temp = 复杂度直接插入排序复杂度如下： 最好情况下，排序前对象已经按照要求的有序。比较次数(KCN)：[Math Processing Error]；移动次数(RMN)为[Math Processing Error]。则对应的时间复杂度为[Math Processing Error]。 最坏情况下，排序前对象为要求的顺序的反序。第i趟时第i个对象必须与前面i个对象都做排序码比较，并且每做1次比较就要做1次数据移动（从上面给出的代码中看出）。比较次数(KCN)：[Math Processing Error] ; 移动次数(RMN)为：[Math Processing Error]。则对应的时间复杂度为[Math Processing Error]。 如果排序记录是随机的，那么根据概率相同的原则，在平均情况下的排序码比较次数和对象移动次数约为[Math Processing Error]，因此，直接插入排序的平均时间复杂度为[Math Processing Error]。 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n) O(n²) O(1) Tips: 由于直接插入排序每次只移动一个元素的位， 并不会改变值相同的元素之间的排序， 因此它是一种稳定排序。 希尔排序（Shell Sort）希尔排序，也称递减增量排序算法，1959年Shell发明。是插入排序的一种高速而稳定的改进版本。 希尔排序是先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。 第一个突破O(n^2)的排序算法；是简单插入排序的改进版；它与插入排序的不同之处在于，它会优先比较距离较远的元素。基本思想将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；每次再将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。 可以看到步长的选择是希尔排序的重要部分。只要最终步长为1任何步长序列都可以工作。一般来说最简单的步长取值是初次取数组长度的一半为增量，之后每次再减半，直到增量为1。更好的步长序列取值可以参考维基百科。 算法描述①. 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；（一般初次取数组半长，之后每次再减半，直到增量为1）②. 按增量序列个数k，对序列进行k 趟排序；③. 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 代码实现以下是我自己的实现，可以看到实现很幼稚，但是好处是理解起来很简单。因为没有经过任何的优化，所以不建议大家直接使用。建议对比下方的维基百科官方实现代码，特别是步长取值策略部分。 123456789101112131415161718192021222324/** * 希尔排序 * * 1. 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；（一般初次取数组半长，之后每次再减半，直到增量为1） * 2. 按增量序列个数k，对序列进行k 趟排序； * 3. 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。 * 仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 * @param arr 待排序数组 */public static void shellSort(int[] arr)&#123; int gap = arr.length / 2; for (; gap &gt; 0; gap /= 2) &#123; //不断缩小gap，直到1为止 for (int j = 0; (j+gap) &lt; arr.length; j++)&#123; //使用当前gap进行组内插入排序 for(int k = 0; (k+gap)&lt; arr.length; k += gap)&#123; if(arr[k] &gt; arr[k+gap]) &#123; int temp = arr[k+gap]; //交换操作 arr[k+gap] = arr[k]; arr[k] = temp; System.out.println(&quot; Sorting: &quot; + Arrays.toString(arr)); &#125; &#125; &#125; &#125;&#125; 注意： ①. 第一层for循环表示一共有多少个增量。增量的序列的个数，就是希尔排序的趟数。上面的增量序列为： arr.length/2, arr.length/2/2, arr.length/2/2/2, …. 2, 1②. 里层的两个for循环，实际上就是以一个gap拆分为一组的组内插入排序。 下面是维基百科官方实现，大家注意gap步长取值部分： 1234567891011121314151617181920212223/** * 希尔排序（Wiki官方版） * * 1. 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；（注意此算法的gap取值） * 2. 按增量序列个数k，对序列进行k 趟排序； * 3. 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。 * 仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 * @param arr 待排序数组 */public static void shell_sort(int[] arr) &#123; int gap = 1, i, j, len = arr.length; int temp; while (gap &lt; len / 3) gap = gap * 3 + 1; // &lt;O(n^(3/2)) by Knuth,1973&gt;: 1, 4, 13, 40, 121, ... for (; gap &gt; 0; gap /= 3) &#123; for (i = gap; i &lt; len; i++) &#123; temp = arr[i]; for (j = i - gap; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j -= gap) arr[j + gap] = arr[j]; arr[j + gap] = temp; &#125; &#125;&#125; 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog2 n) O(nlog2 n) O(nlog2 n) O(1) 选择排序（Selection Sort）从算法逻辑上看，选择排序是一种简单直观的排序算法，在简单选择排序过程中，所需移动记录的次数比较少。 基本思想选择排序的基本思想：比较 + 交换。 在未排序序列中找到最小（大）元素，存放到未排序序列的起始位置。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。 算法描述①. 从待排序序列中，找到关键字最小的元素；②. 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换；③. 从余下的 N - 1 个元素中，找出关键字最小的元素，重复①、②步，直到排序结束。 代码实现选择排序比较简单，以下是我自己的实现，跟官方版差不多，所以完全可以参考。 12345678910111213141516171819202122232425/** * 选择排序 * * 1. 从待排序序列中，找到关键字最小的元素； * 2. 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换； * 3. 从余下的 N - 1 个元素中，找出关键字最小的元素，重复①、②步，直到排序结束。 * 仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 * @param arr 待排序数组 */public static void selectionSort(int[] arr)&#123; for(int i = 0; i &lt; arr.length-1; i++)&#123; int min = i; for(int j = i+1; j &lt; arr.length; j++)&#123; //选出之后待排序中值最小的位置 if(arr[j] &lt; arr[min])&#123; min = j; &#125; &#125; if(min != i)&#123; int temp = arr[min]; //交换操作 arr[min] = arr[i]; arr[i] = temp; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); &#125; &#125;&#125; 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n²) O(n²) O(1) 选择排序的简单和直观名副其实，这也造就了它”出了名的慢性子”，无论是哪种情况，哪怕原数组已排序完成，它也将花费将近n²/2次遍历来确认一遍。即便是这样，它的排序结果也还是不稳定的。 唯一值得高兴的是，它并不耗费额外的内存空间。 堆排序（Heap Sort）1991年的计算机先驱奖获得者、斯坦福大学计算机科学系教授罗伯特·弗洛伊德(Robert W．Floyd) 和威廉姆斯(J．Williams) 在1964年共同发明了著名的堆排序算法(Heap Sort). 堆的定义如下：n个元素的序列{k1,k2,···,kn}，当且仅当满足下关系时，称之为堆。 ki &lt;= k(2i) 且 ki &lt;= k(2i+1) 或： ki &gt;= k(2i) 且 ki &gt;= k(2i+1) 把此序列对应的二维数组看成一个完全二叉树。那么堆的含义就是完全二叉树中任何一个非叶子节点的值均不大于（或不小于）其左，右孩子节点的值。由上述性质可知大顶堆的堆顶的关键字肯定是所有关键字中最大的，小顶堆的堆顶的关键字是所有关键字中最小的。因此我们可使用大顶堆进行升序排序, 使用小顶堆进行降序排序。 基本思想此处以大顶堆为例，堆排序的过程就是将待排序的序列构造成一个堆，选出堆中最大的移走，再把剩余的元素调整成堆，找出最大的再移走，重复直至有序。 算法描述①. 先将初始序列K[1..n]建成一个大顶堆, 那么此时第一个元素K1最大, 此堆为初始的无序区.②. 再将关键字最大的记录K1 (即堆顶, 第一个元素)和无序区的最后一个记录 Kn 交换, 由此得到新的无序区K[1..n-1]和有序区K[n], 且满足K[1..n-1].keys &lt;= K[n].key③. 交换K1 和 Kn 后, 堆顶可能违反堆性质, 因此需将K[1..n-1]调整为堆. 然后重复步骤②, 直到无序区只有一个元素时停止. 动图效果如下所示： 代码实现从算法描述来看，堆排序需要两个过程，一是建立堆，二是堆顶与堆的最后一个元素交换位置。所以堆排序有两个函数组成。一是建堆函数，二是反复调用建堆函数以选择出剩余未排元素中最大的数来实现排序的函数。 总结起来就是定义了以下几种操作： 最大堆调整（Max_Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点创建最大堆（Build_Max_Heap）：将堆所有数据重新排序堆排序（HeapSort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算对于堆节点的访问： 父节点i的左子节点在位置：(2*i+1); 父节点i的右子节点在位置：(2*i+2); 子节点i的父节点在位置：floor((i-1)/2);1234567891011121314151617181920212223242526272829303132333435363738/** * 堆排序 * * 1. 先将初始序列K[1..n]建成一个大顶堆, 那么此时第一个元素K1最大, 此堆为初始的无序区. * 2. 再将关键字最大的记录K1 (即堆顶, 第一个元素)和无序区的最后一个记录 Kn 交换, 由此得到新的无序区K[1..n−1]和有序区K[n], 且满足K[1..n−1].keys⩽K[n].key * 3. 交换K1 和 Kn 后, 堆顶可能违反堆性质, 因此需将K[1..n−1]调整为堆. 然后重复步骤②, 直到无序区只有一个元素时停止. * @param arr 待排序数组 */public static void heapSort(int[] arr)&#123; for(int i = arr.length; i &gt; 0; i--)&#123; max_heapify(arr, i); int temp = arr[0]; //堆顶元素(第一个元素)与Kn交换 arr[0] = arr[i-1]; arr[i-1] = temp; &#125;&#125;private static void max_heapify(int[] arr, int limit)&#123; if(arr.length &lt;= 0 || arr.length &lt; limit) return; int parentIdx = limit / 2; for(; parentIdx &gt;= 0; parentIdx--)&#123; if(parentIdx * 2 &gt;= limit)&#123; continue; &#125; int left = parentIdx * 2; //左子节点位置 int right = (left + 1) &gt;= limit ? left : (left + 1); //右子节点位置，如果没有右节点，默认为左节点位置 int maxChildId = arr[left] &gt;= arr[right] ? left : right; if(arr[maxChildId] &gt; arr[parentIdx])&#123; //交换父节点与左右子节点中的最大值 int temp = arr[parentIdx]; arr[parentIdx] = arr[maxChildId]; arr[maxChildId] = temp; &#125; &#125; System.out.println(&quot;Max_Heapify: &quot; + Arrays.toString(arr));&#125; 注: x&gt;&gt;1 是位运算中的右移运算, 表示右移一位, 等同于x除以2再取整, 即 x&gt;&gt;1 == Math.floor(x/2) . 复杂度以上, ①. 建立堆的过程, 从length/2 一直处理到0, 时间复杂度为O(n); ②. 调整堆的过程是沿着堆的父子节点进行调整, 执行次数为堆的深度, 时间复杂度为O(lgn); ③. 堆排序的过程由n次第②步完成, 时间复杂度为O(nlgn). 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog2 n) O(nlog2 n) O(nlog2 n) O(1) Tips: 由于堆排序中初始化堆的过程比较次数较多, 因此它不太适用于小序列。同时由于多次任意下标相互交换位置, 相同元素之间原本相对的顺序被破坏了, 因此, 它是不稳定的排序. #### 冒泡排序（Bubble Sort） 我想对于它每个学过C语言的都会了解，这可能是很多人接触的第一个排序算法。 基本思想冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述冒泡排序算法的运作如下： ①. 比较相邻的元素。如果第一个比第二个大，就交换他们两个。②. 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。③. 针对所有的元素重复以上的步骤，除了最后一个。④. 持续每次对越来越少的元素重复上面的步骤①~③，直到没有任何一对数字需要比较。 代码实现冒泡排序需要两个嵌套的循环. 其中, 外层循环移动游标; 内层循环遍历游标及之后(或之前)的元素, 通过两两交换的方式, 每次只确保该内循环结束位置排序正确, 然后内层循环周期结束, 交由外层循环往后(或前)移动游标, 随即开始下一轮内层循环, 以此类推, 直至循环结束. 123456789101112131415161718192021/** * 冒泡排序 * * ①. 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 * ②. 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 * ③. 针对所有的元素重复以上的步骤，除了最后一个。 * ④. 持续每次对越来越少的元素重复上面的步骤①~③，直到没有任何一对数字需要比较。 * @param arr 待排序数组 */public static void bubbleSort(int[] arr)&#123; for (int i = arr.length - 1; i &gt; 0; i--) &#123; //外层循环移动游标 for(int j = 0; j &lt; i; j++)&#123; //内层循环遍历游标及之后(或之前)的元素 if(arr[j] &gt; arr[j+1])&#123; int temp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = temp; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); &#125; &#125; &#125;&#125; 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n) O(n²) O(1) 冒泡排序是最容易实现的排序, 最坏的情况是每次都需要交换, 共需遍历并交换将近n²/2次, 时间复杂度为O(n²). 最佳的情况是内循环遍历一次后发现排序是对的, 因此退出循环, 时间复杂度为O(n). 平均来讲, 时间复杂度为O(n²). 由于冒泡排序中只有缓存的temp变量需要内存空间, 因此空间复杂度为常量O(1). Tips: 由于冒泡排序只在相邻元素大小不符合要求时才调换他们的位置, 它并不改变相同元素之间的相对顺序, 因此它是稳定的排序算法. 快速排序（Quick Sort）快速排序（Quicksort）是对冒泡排序的一种改进，借用了分治的思想，由C. A. R. Hoare在1962年提出。 基本思想快速排序的基本思想：挖坑填数+分治法。 首先选一个轴值(pivot，也有叫基准的)，通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 算法描述快速排序使用分治策略来把一个序列（list）分为两个子序列（sub-lists）。步骤为： ①. 从数列中挑出一个元素，称为”基准”（pivot）。②. 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。③. 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归到最底部时，数列的大小是零或一，也就是已经排序好了。这个算法一定会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 代码实现用伪代码描述如下： ①. i = L; j = R; 将基准数挖出形成第一个坑a[i]。②．j–，由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。③．i++，由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。④．再重复执行②，③二步，直到i==j，将基准数填入a[i]中 1234567891011121314151617181920212223242526272829303132/** * 快速排序（递归） * * ①. 从数列中挑出一个元素，称为&quot;基准&quot;（pivot）。 * ②. 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 * ③. 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 * @param arr 待排序数组 * @param low 左边界 * @param high 右边界 */public static void quickSort(int[] arr, int low, int high)&#123; if(arr.length &lt;= 0) return; if(low &gt;= high) return; int left = low; int right = high; int temp = arr[left]; //挖坑1：保存基准的值 while (left &lt; right)&#123; while(left &lt; right &amp;&amp; arr[right] &gt;= temp)&#123; //坑2：从后向前找到比基准小的元素，插入到基准位置坑1中 right--; &#125; arr[left] = arr[right]; while(left &lt; right &amp;&amp; arr[left] &lt;= temp)&#123; //坑3：从前往后找到比基准大的元素，放到刚才挖的坑2中 left++; &#125; arr[right] = arr[left]; &#125; arr[left] = temp; //基准值填补到坑3中，准备分治递归快排 System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); quickSort(arr, low, left-1); quickSort(arr, left+1, high);&#125; 上面是递归版的快速排序：通过把基准temp插入到合适的位置来实现分治，并递归地对分治后的两个划分继续快排。那么非递归版的快排如何实现呢？ 因为递归的本质是栈，所以我们非递归实现的过程中，可以借助栈来保存中间变量就可以实现非递归了。在这里中间变量也就是通过Pritation函数划分区间之后分成左右两部分的首尾指针，只需要保存这两部分的首尾指针即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 快速排序（非递归） * * ①. 从数列中挑出一个元素，称为&quot;基准&quot;（pivot）。 * ②. 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 * ③. 把分区之后两个区间的边界（low和high）压入栈保存，并循环①、②步骤 * @param arr 待排序数组 */public static void quickSortByStack(int[] arr)&#123; if(arr.length &lt;= 0) return; Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); //初始状态的左右指针入栈 stack.push(0); stack.push(arr.length - 1); while(!stack.isEmpty())&#123; int high = stack.pop(); //出栈进行划分 int low = stack.pop(); int pivotIdx = partition(arr, low, high); //保存中间变量 if(pivotIdx &gt; low) &#123; stack.push(low); stack.push(pivotIdx - 1); &#125; if(pivotIdx &lt; high &amp;&amp; pivotIdx &gt;= 0)&#123; stack.push(pivotIdx + 1); stack.push(high); &#125; &#125;&#125;private static int partition(int[] arr, int low, int high)&#123; if(arr.length &lt;= 0) return -1; if(low &gt;= high) return -1; int l = low; int r = high; int pivot = arr[l]; //挖坑1：保存基准的值 while(l &lt; r)&#123; while(l &lt; r &amp;&amp; arr[r] &gt;= pivot)&#123; //坑2：从后向前找到比基准小的元素，插入到基准位置坑1中 r--; &#125; arr[l] = arr[r]; while(l &lt; r &amp;&amp; arr[l] &lt;= pivot)&#123; //坑3：从前往后找到比基准大的元素，放到刚才挖的坑2中 l++; &#125; arr[r] = arr[l]; &#125; arr[l] = pivot; //基准值填补到坑3中，准备分治递归快排 return l;&#125; 复杂度快速排序是通常被认为在同数量级（O(nlog2n)）的排序方法中平均性能最好的。但若初始序列按关键码有序或基本有序时，快排序反而蜕化为冒泡排序。为改进之，通常以“三者取中法”来选取基准记录，即将排序区间的两个端点与中点三个记录关键码居中的调整为支点记录。快速排序是一个不稳定的排序方法。 以下是快速排序算法复杂度:|平均时间复杂度| 最好情况 |最坏情况 |空间复杂度|| — | — | — | — ||O(nlog₂n) |O(nlog₂n) |O(n²) |O(1)（原地分区递归版）| 快速排序排序效率非常高。 虽然它运行最糟糕时将达到O(n²)的时间复杂度, 但通常平均来看, 它的时间复杂为O(nlogn), 比同样为O(nlogn)时间复杂度的归并排序还要快. 快速排序似乎更偏爱乱序的数列, 越是乱序的数列, 它相比其他排序而言, 相对效率更高. Tips: 同选择排序相似, 快速排序每次交换的元素都有可能不是相邻的, 因此它有可能打破原来值为相同的元素之间的顺序. 因此, 快速排序并不稳定. 归并排序（Merging Sort）归并排序是建立在归并操作上的一种有效的排序算法，1945年由约翰·冯·诺伊曼首次提出。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用，且各层分治递归可以同时进行。 基本思想归并排序算法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 算法描述归并排序可通过两种方式实现： 自上而下的递归自下而上的迭代一、递归法（假设序列共有n个元素）： ①. 将序列每相邻两个数字进行归并操作，形成 floor(n/2)个序列，排序后每个序列包含两个元素；②. 将上述序列再次归并，形成 floor(n/4)个序列，每个序列包含四个元素；③. 重复步骤②，直到所有元素排序完毕。二、迭代法 ①. 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列②. 设定两个指针，最初位置分别为两个已经排序序列的起始位置③. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置④. 重复步骤③直到某一指针到达序列尾⑤. 将另一序列剩下的所有元素直接复制到合并序列尾 代码实现归并排序其实要做两件事： 分解：将序列每次折半拆分合并：将划分后的序列段两两排序合并因此，归并排序实际上就是两个操作，拆分+合并 如何合并？ L[first…mid]为第一段，L[mid+1…last]为第二段，并且两端已经有序，现在我们要将两端合成达到L[first…last]并且也有序。 首先依次从第一段与第二段中取出元素比较，将较小的元素赋值给temp[]重复执行上一步，当某一段赋值结束，则将另一段剩下的元素赋值给temp[]此时将temp[]中的元素复制给L[]，则得到的L[first…last]有序 如何分解？ 在这里，我们采用递归的方法，首先将待排序列分成A,B两组；然后重复对A、B序列 分组；直到分组后组内只有一个元素，此时我们认为组内所有元素有序，则分组结束。 这里我写了递归算法如下： 12345678910111213141516171819202122232425262728293031323334353637/** * 归并排序（递归） * * ①. 将序列每相邻两个数字进行归并操作，形成 floor(n/2)个序列，排序后每个序列包含两个元素； * ②. 将上述序列再次归并，形成 floor(n/4)个序列，每个序列包含四个元素； * ③. 重复步骤②，直到所有元素排序完毕。 * @param arr 待排序数组 */public static int[] mergingSort(int[] arr)&#123; if(arr.length &lt;= 1) return arr; int num = arr.length &gt;&gt; 1; int[] leftArr = Arrays.copyOfRange(arr, 0, num); int[] rightArr = Arrays.copyOfRange(arr, num, arr.length); System.out.println(&quot;split two array: &quot; + Arrays.toString(leftArr) + &quot; And &quot; + Arrays.toString(rightArr)); return mergeTwoArray(mergingSort(leftArr), mergingSort(rightArr)); //不断拆分为最小单元，再排序合并&#125;private static int[] mergeTwoArray(int[] arr1, int[] arr2)&#123; int i = 0, j = 0, k = 0; int[] result = new int[arr1.length + arr2.length]; //申请额外的空间存储合并之后的数组 while(i &lt; arr1.length &amp;&amp; j &lt; arr2.length)&#123; //选取两个序列中的较小值放入新数组 if(arr1[i] &lt;= arr2[j])&#123; result[k++] = arr1[i++]; &#125;else&#123; result[k++] = arr2[j++]; &#125; &#125; while(i &lt; arr1.length)&#123; //序列1中多余的元素移入新数组 result[k++] = arr1[i++]; &#125; while(j &lt; arr2.length)&#123; //序列2中多余的元素移入新数组 result[k++] = arr2[j++]; &#125; System.out.println(&quot;Merging: &quot; + Arrays.toString(result)); return result;&#125; 由上, 长度为n的数组, 最终会调用mergeSort函数2n-1次。通过自上而下的递归实现的归并排序, 将存在堆栈溢出的风险。 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(n) 从效率上看，归并排序可算是排序算法中的”佼佼者”. 假设数组长度为n，那么拆分数组共需logn，, 又每步都是一个普通的合并子数组的过程， 时间复杂度为O(n)， 故其综合时间复杂度为O(nlogn)。另一方面， 归并排序多次递归过程中拆分的子数组需要保存在内存空间， 其空间复杂度为O(n)。 和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(n log n）的时间复杂度。代价是需要额外的内存空间。 基数排序（Radix Sort）基数排序的发明可以追溯到1887年赫尔曼·何乐礼在打孔卡片制表机（Tabulation Machine）, 排序器每次只能看到一个列。它是基于元素值的每个位上的字符来排序的。 对于数字而言就是分别基于个位，十位， 百位或千位等等数字来排序。 基数排序（Radix sort）是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 基本思想它是这样实现的：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。 基数排序按照优先从高位或低位来排序有两种实现方案： MSD（Most significant digital） 从最左侧高位开始进行排序。先按k1排序分组, 同一组中记录, 关键码k1相等, 再对各组按k2排序分成子组, 之后, 对后面的关键码继续这样的排序分组, 直到按最次位关键码kd对各子组排序后. 再将各组连接起来, 便得到一个有序序列。MSD方式适用于位数多的序列。 LSD （Least significant digital）从最右侧低位开始进行排序。先从kd开始排序，再对kd-1进行排序，依次重复，直到对k1排序后便得到一个有序序列。LSD方式适用于位数少的序列。 算法描述我们以LSD为例，从最低位开始，具体算法描述如下： ①. 取得数组中的最大数，并取得位数；②. arr为原始数组，从最低位开始取每个位组成radix数组；③. 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 代码实现基数排序：通过序列中各个元素的值，对排序的N个元素进行若干趟的“分配”与“收集”来实现排序。 分配：我们将L[i]中的元素取出，首先确定其个位上的数字，根据该数字分配到与之序号相同的桶中 收集：当序列中所有的元素都分配到对应的桶中，再按照顺序依次将桶中的元素收集形成新的一个待排序列L[]。对新形成的序列L[]重复执行分配和收集元素中的十位、百位…直到分配完该序列中的最高位，则排序结束 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 基数排序（LSD 从低位开始） * * 基数排序适用于： * (1)数据范围较小，建议在小于1000 * (2)每个数值都要大于等于0 * * ①. 取得数组中的最大数，并取得位数； * ②. arr为原始数组，从最低位开始取每个位组成radix数组； * ③. 对radix进行计数排序（利用计数排序适用于小范围数的特点）； * @param arr 待排序数组 */public static void radixSort(int[] arr)&#123; if(arr.length &lt;= 1) return; //取得数组中的最大数，并取得位数 int max = 0; for(int i = 0; i &lt; arr.length; i++)&#123; if(max &lt; arr[i])&#123; max = arr[i]; &#125; &#125; int maxDigit = 1; while(max / 10 &gt; 0)&#123; maxDigit++; max = max / 10; &#125; System.out.println(&quot;maxDigit: &quot; + maxDigit); //申请一个桶空间 int[][] buckets = new int[10][arr.length]; int base = 10; //从低位到高位，对每一位遍历，将所有元素分配到桶中 for(int i = 0; i &lt; maxDigit; i++)&#123; int[] bktLen = new int[10]; //存储各个桶中存储元素的数量 //分配：将所有元素分配到桶中 for(int j = 0; j &lt; arr.length; j++)&#123; int whichBucket = (arr[j] % base) / (base / 10); buckets[whichBucket][bktLen[whichBucket]] = arr[j]; bktLen[whichBucket]++; &#125; //收集：将不同桶里数据挨个捞出来,为下一轮高位排序做准备,由于靠近桶底的元素排名靠前,因此从桶底先捞 int k = 0; for(int b = 0; b &lt; buckets.length; b++)&#123; for(int p = 0; p &lt; bktLen[b]; p++)&#123; arr[k++] = buckets[b][p]; &#125; &#125; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); base *= 10; &#125;&#125; 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(d*(n+r)) O(d*(n+r)) O(d*(n+r)) O(n+r) 其中，d 为位数，r 为基数，n 为原数组个数。在基数排序中，因为没有比较操作，所以在复杂上，最好的情况与最坏的情况在时间上是一致的，均为 O(d*(n + r))。 基数排序更适合用于对时间, 字符串等这些整体权值未知的数据进行排序。 Tips: 基数排序不改变相同元素之间的相对顺序，因此它是稳定的排序算法。 基数排序 vs 计数排序 vs 桶排序 这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异： 基数排序：根据键值的每位数字来分配桶 计数排序：每个桶只存储单一键值 桶排序：每个桶存储一定范围的数值 总结 排序类型 平均情况 最好情况 最坏情况 辅助空间 稳定性 冒泡排序 O(n²) O(n) O(n²) O(1) 稳定 选择排序 O(n²) O(n²) O(n²) O(1) 不稳定 直接插入排序 O(n²) O(n) O(n²) O(1) 稳定 折半插入排序 O(n²) O(n) O(n²) O(1) 稳定 希尔排序 O(n^1.3) O(nlogn) O(n²) O(1) 不稳定 归并排序 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(n) 稳定 快速排序 O(nlog₂n) O(nlog₂n) O(n²) O(nlog₂n) 不稳定 堆排序 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(1) 不稳定 计数排序 O(n+k) O(n+k) O(n+k) O(k) 稳定 桶排序 O(n+k) O(n+k) O(n²) O(n+k) (不)稳定 基数排序 O(d(n+k)) O(d(n+k)) O(d(n+kd)) O(n+kd) 稳定 从时间复杂度来说： (1). 平方阶O(n²)排序：各类简单排序：直接插入、直接选择和冒泡排序； (2). 线性对数阶O(nlog₂n)排序： 快速排序、堆排序和归并排序； (3). O(n1+§))排序，§是介于0和1之间的常数：希尔排序 (4). 线性阶O(n)排序：基数排序，此外还有桶、箱排序。 到此，很多人会注意到基数排序的时间复杂度是最小的，那么为什么却没有快排、堆排序流行呢？我们看看下图算法导论的相关说明： 基数排序只适用于有基数的情况，而基于比较的排序适用范围就广得多。另一方面是内存上的考虑。作为一种通用的排序方法，最好不要带来意料之外的内存开销，所以各语言的默认实现都没有用基数排序，但是不能否认基数排序在各领域的应用。 时间复杂度极限当被排序的数有一些性质的时候（比如是整数，比如有一定的范围），排序算法的复杂度是可以小于O(nlgn)的。比如： 计数排序 复杂度O( k+n) 要求：被排序的数是0~k范围内的整数 基数排序 复杂度O( d(k+n) ) 要求：d位数，每个数位有k个取值 桶排序 复杂度 O( n ) （平均） 要求：被排序数在某个范围内，并且服从均匀分布 但是，当被排序的数不具有任何性质的时候，一般使用基于比较的排序算法，而基于比较的排序算法时间复杂度的下限必须是O(nlgn)。 参考很多高效排序算法的代价是 nlogn，难道这是排序算法的极限了吗？ 说明 当原表有序或基本有序时，直接插入排序和冒泡排序将大大减少比较次数和移动记录的次数，时间复杂度可降至O（n）； 而快速排序则相反，当原表基本有序时，将蜕化为冒泡排序，时间复杂度提高为O（n2）； 原表是否有序，对简单选择排序、堆排序、归并排序和基数排序的时间复杂度影响不大。]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-HashMap、Hashtable、ConcurrentHashMap的区别]]></title>
    <url>%2F2019%2F05%2F12%2FJava-HashMap%E3%80%81Hashtable%E3%80%81ConcurrentHashMap%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[以下是基于java1.8源码进行对比： 类型 HashMap Hashtable ConcurrentHashMap 数据结构 数组+链表+红黑树 数组+链表 数组+链表+红黑树 数组容量 默认容量为16，且要求底层数组的容量一定为2的整数次幂 默认容量为11，且不要求底层数组的容量一定为2的整数次幂 默认容量为16，且要求底层数组的容量一定为2的整数次幂 是否支持NULL值 支持 不支持 不支持 size获取 size字段值 count字段值 通过累加baseCount和CounterCell数组中的数量，得到元素的总个数 hash计算方式 key.hashCode() ^ (key.hashCode() &gt;&gt;&gt; 16) key.hashCode() (key.hashCode() ^ (key.hashCode() &gt;&gt;&gt; 16)) &amp; 0x7fffffff index计算方式 (n - 1) &amp; hash (hash &amp; 0x7FFFFFFF) % tab.length (hash &amp; 0x7FFFFFFF) % tab.length 是否线程安全 否 是 是 性能高低 高 低 中 线程安全实现方式 无 synchronized对象锁，get、add、remove等方法不能同时进行 CAS + synchronized保证多线程安全，可以同时执行多个方法 扩容方式 单线程进行扩容（多线程进行扩容，可能会导致死循环造成CPU100%） 单线程进行扩容 单线程对新数组进行初始化，可多线程同步复制不同节点数据到新数组 总结： HashTable是已过时的类，不建议再使用; 需要线程安全的场景，使用ConcurrentHashMap; 无需线程安全，使用HashMap;]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-ConcurrentHashMap在JDK1.7和JDK1.8的差异]]></title>
    <url>%2F2019%2F05%2F12%2FJava-ConCurrentHashMap%E5%9C%A8JDK1-7%E5%92%8CJDK1-8%E7%9A%84%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[版本 JDK1.7 JDK1.8 概览 同步机制 分段锁，每个segment继承ReentrantLock CAS + synchronized保证并发更新 数据结构 数组+链表 数组+链表+红黑树 键值对 HashEntry Node put操作 当执行put方法插入数据时，根据key的hash值，在Segment数组中找到相应的位置，如果相应位置的Segment还未初始化，则通过CAS进行赋值，接着执行Segment对象的put方法通过加锁机制插入数据。多个线程同时竞争获取同一个segment锁，获取成功的线程更新map；失败的线程尝试多次获取锁仍未成功，则挂起线程，等待释放锁 访问相应的bucket时，使用sychronizeded关键字，防止多个线程同时操作同一个bucket，如果该节点的hash不小于0，则遍历链表更新节点或插入新节点；如果该节点是TreeBin类型的节点，说明是红黑树结构，则通过putTreeVal方法往红黑树中插入节点；更新了节点数量，还要考虑扩容和链表转红黑树 size实现 统计每个Segment对象中的元素个数，然后进行累加，但是这种方式计算出来的结果并不一样的准确的。先采用不加锁的方式，连续计算元素的个数，最多计算3次：如果前后两次计算结果相同，则说明计算出来的元素个数是准确的；如果前后两次计算结果都不同，则给每个Segment进行加锁，再计算一次元素的个数； 使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据或者删除数据时，会通过addCount()方法更新baseCount。通过累加baseCount和CounterCell数组中的数量，即可得到元素的总个数； 扩容实现 某个线程获得了锁之后，单线程执行扩容操作。将数组大小翻倍，复制数据到新数组。 支持并发迁移节点，遍历table的每一个节点。如果该节点被其他线程处理过了，就会创建一个 ForwardingNode 放到该节点原位置，hash值为-1。其他线程通过判断是 ForwardingNode 就知道是否已被处理过。在复制节点数据的过程中，会通过 synchronized 锁防止多个线程同时复制同一个节点的数据。 参考链接：https://www.jianshu.com/p/e694f1e868ec]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-ConcurrentHashMap工作原理及实现]]></title>
    <url>%2F2019%2F05%2F12%2FJava-ConCurrentHashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[概述关于Java集合的小抄是这么描述： 并发优化的HashMap。 在JDK5里的经典设计，默认16把写锁（可以设置更多），有效分散了阻塞的概率。数据结构为Segment[]，每个Segment一把锁。Segment里面才是哈希桶数组。Key先算出它在哪个Segment里，再去算它在哪个哈希桶里。 也没有读锁，因为put/remove动作是个原子动作（比如put的整个过程是一个对数组元素/Entry 指针的赋值操作），读操作不会看到一个更新动作的中间状态。 但在JDK8里，Segment[]的设计被抛弃了，改为精心设计的，只在需要锁的时候加锁。 支持ConcurrentMap接口，如putIfAbsent（key，value）与相反的replace（key，value）与以及实现CAS的replace（key, oldValue, newValue）。 成员变量 table：默认为null，初始化发生在第一次插入操作，默认大小为16的数组，用来存储Node节点数据，扩容时大小总是2的幂次方。 nextTable：默认为null，扩容时新生成的数组，其大小为原数组的两倍。 sizeCtl ：默认为0，用来控制table的初始化和扩容操作，具体应用在后续会体现出来。 1234等于-1时，代表 table 正在初始化 等于-N时，表示有N-1个线程正在进行扩容操作 如果table未初始化，表示table需要初始化的大小。 如果table初始化完成，表示table的容量，默认是table大小的0.75倍，居然用这个公式算0.75（n - (n &gt;&gt;&gt; 2)）。 Node：保存key，value及key的hash值的数据结构。 其中value和next都用volatile修饰，保证并发的可见性。 1234567class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; //... 省略部分代码&#125; ForwardingNode：一个特殊的Node节点，hash值为-1，其中存储nextTable的引用。 只有table发生扩容的时候，ForwardingNode才会发挥作用，作为一个占位符放在table中表示当前节点为null或则已经被移动。 1234567final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125;&#125; 初始化数组初始化数组的时候需要判断是否有其他线程正在执行初始化，采用CAS操作更新 sizeCtl 的值。具体代码如下： 1234567891011121314151617181920212223242526private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; //如果一个线程发现sizeCtl&lt;0，意味着另外的线程执行CAS操作成功正在初始化表，当前线程只需要让出cpu时间片 if ((sc = sizeCtl) &lt; 0) Thread.yield(); //SIZECTL：表示当前对象的内存偏移量，sc表示期望值，-1表示要替换的值，设定为-1表示要初始化表了。执行CAS操作 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; //没有指定初始化容量大小，则默认为16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; //0.75 * capacity sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; put函数与 HashMap 的 put 操作类似，主要增加多线程情况的判断。具体实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public V put(K key, V value) &#123; return putVal(key, value, false);&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果tab为空则初始化table if (tab == null || (n = tab.length) == 0) tab = initTable(); //如果对应数组位置为空，则通过CAS操作进行赋值（这时候是不会加锁的）。 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // 当前hash为MOVED表示Map在扩容，先协助扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; //hash冲突，通过 synchronized 加锁当前位置对象防止多线程更新 V oldVal = null; synchronized (f) &#123; //再次判断是否已被其他线程更新值 if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; //从链表头节点开始遍历 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; // 节点已经存在，修改链表节点的值 oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; // 节点不存在，新增节点到链表末尾 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //链表节点 &gt;= 8个则已经转换为红黑树，遍历寻找进行替换值或新增节点 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //如果链表节点个数 &gt;= 8，则转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 统计节点个数，检查是否需要扩容 addCount(1L, binCount); return null;&#125; size与mappingCount为了更好地统计size，ConcurrentHashMap提供了baseCount、counterCells两个辅助变量和一个CounterCell辅助内部类。 123456789@sun.misc.Contended static final class CounterCell &#123; volatile long value; CounterCell(long x) &#123; value = x; &#125; &#125; //ConcurrentHashMap中元素个数,但返回的不一定是当前Map的真实元素个数。基于CAS无锁更新 private transient volatile long baseCount; private transient volatile CounterCell[] counterCells; mappingCount与size方法的类似 从Java工程师给出的注释来看，应该使用mappingCount代替size方法 两个方法都没有直接返回basecount 而是统计一次这个值，而这个值其实也是一个大概的数值，因此可能在统计的时候有其他线程正在执行插入或删除操作。 12345678910111213141516171819202122232425public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125;//返回容器的大小。这个方法应该被用来代替size()方法，因为 ConcurrentHashMap的容量大小可能会大于int的最大值。public long mappingCount() &#123; long n = sumCount(); return (n &lt; 0L) ? 0L : n;&#125;//迭代counterCells来统计sum的过程final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value;//所有counter的值求和 &#125; &#125; return sum;&#125; 数组扩容当ConcurrentHashMap中table元素个数达到了容量阈值（sizeCtl）时，则需要进行扩容操作。在put操作时最后一个会调用addCount(long x, int check)，该方法主要做两个工作：1.更新baseCount；2.检测是否需要扩容操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; // s = b + x，完成baseCount++操作； if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; // 多线程CAS发生失败时执行 fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; //判断是否进行扩容 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; //有线程正在进行扩容初始化nextTable操作，直接break if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //nextTable已经初始化，协助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //扩容，对nextTable进行初始化 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125; &#125; addCount 函数保证只有单线程对 nextTable 进行初始化操作。 transfer()方法为ConcurrentHashMap扩容操作的核心方法。由于ConcurrentHashMap支持多线程扩容，而且也没有进行加锁，所以实现会变得有点儿复杂。整个扩容分为两部分： 构建一个nextTable，大小为table的两倍。 把table的数据复制到nextTable中。(这里允许多线程同时复制)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; // 每核处理的量小于16，则强制赋值16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(&quot;unchecked&quot;) //构建一个nextTable对象，其容量为原来容量的两倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; // 连接点指针，用于标志位（fwd的hash值为-1，fwd.nextTable=nextTab） ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // 当advance == true时，表明该节点已经处理过了 boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 控制 --i ,遍历原hash表中的节点 while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; // 用CAS计算得到的transferIndex else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; // 已经完成所有节点复制了 if (finishing) &#123; nextTable = null; table = nextTab; // table 指向nextTable sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); //扩容阈值设置为原来容量的1.5倍 依然相当于现在容量的0.75倍 return; // 跳出死循环， &#125; // CAS 更扩容阈值，在这里面sizectl值减一，说明新加入一个线程参与到扩容操作 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; // 遍历的节点为null，则放入到ForwardingNode 指针节点 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // f.hash == -1 表示遍历到了ForwardingNode节点，意味着该节点已经处理过了 // 这里是控制并发扩容的核心 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; // 节点加锁,避免多线程复制同一个节点 synchronized (f) &#123; // 节点复制工作 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // fh &gt;= 0 ,表示为链表节点 if (fh &gt;= 0) &#123; // 构造两个链表 一个是原链表 另一个是原链表的反序排列 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // 在nextTable i 位置处插上链表 setTabAt(nextTab, i, ln); // 在nextTable i + n 位置处插上链表 setTabAt(nextTab, i + n, hn); // 在table i 位置处插上ForwardingNode 表示该节点已经处理过了 setTabAt(tab, i, fwd); // advance = true 可以执行--i动作，遍历节点 advance = true; &#125; // 如果是TreeBin，则按照红黑树进行处理，处理逻辑与上面一致 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // 扩容后树节点个数若&lt;=6，将树转链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125; &#125; 上面的源码有点儿长，稍微复杂了一些，在这里我们抛弃它多线程环境，我们从单线程角度来看： 为每个内核分任务，并保证其不小于16 检查nextTable是否为null，如果是，则初始化nextTable，使其容量为table的两倍 循环变量直到finished，利用tabAt方法获得i位置的元素（支持多线程复制） 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上。并将ForwardingNode 插入原节点位置，代表已经处理过了 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要unTreeify()操作，把处理的结果分别放在nextTable的i和i+n的位置上。并插入ForwardingNode 节点 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。 在多线程环境下，ConcurrentHashMap用两点来保证正确性：ForwardingNode和synchronized。 当一个线程遍历到的节点如果是ForwardingNode，则继续往后遍历。 如果不是，则将该节点加锁，防止其他线程进入，完成后设置ForwardingNode节点。 当其他线程处理该节点时可以看到已经处理过了，如此交叉进行，高效而又安全。 helpTransfer在添加、删除等方法里面都会调用，当前优先协助扩容。helpTransfer()方法为协助扩容方法，当调用该方法的时候，nextTable一定已经创建了，所以该方法主要则是进行复制工作。 12345678910111213141516171819final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; int sc; if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; int rs = resizeStamp(tab.length); while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table; &#125; 转换红黑树用于将过长的链表转换为TreeBin对象。但是他并不是直接转换，而是进行一次容量判断。 如果容量没有达到转换的要求(table.length&lt;64)，直接进行扩容操作并返回； 如果满足条件才链表的结构抓换为TreeBin ，这与HashMap不同的是： 1.根据table中index位置Node链表，重新生成一个hd为头结点的TreeNode 2.根据hd头结点，生成TreeBin树结构，并用TreeBin替换掉原来的Node对象。 123456789101112131415161718192021222324252627private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY)//如果table.length&lt;64 就扩大一倍 返回 tryPresize(n &lt;&lt; 1); else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; //构造了一个TreeBin对象 把所有Node节点包装成TreeNode放进去 for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null);//这里只是利用了TreeNode封装 而没有利用TreeNode的next域和parent域 if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; //在原来index的位置 用TreeBin替换掉原来的Node对象 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125; &#125; get函数读取操作，不需要同步控制，比较简单 空tab，直接返回null 计算hash值，找到相应的bucket位置，为node节点直接返回，否则返回null12345678910111213141516171819public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 参考链接：https://blog.csdn.net/programmer_at/article/details/79715177#141-addcounthttps://blog.csdn.net/u010723709/article/details/48007881https://www.jianshu.com/p/c0642afe03e0http://cmsblogs.com/?p=2283]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-HashSet工作原理及实现]]></title>
    <url>%2F2019%2F05%2F12%2FJava-HashSet%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Set概述关于Java集合的小抄是这么描述：所有Set几乎都是内部用一个Map来实现, 因为Map里的KeySet就是一个Set，而value是假值，全部使用同一个Object即可。 Set的特征也继承了那些内部的Map实现的特征。 HashSet：内部是HashMap。 LinkedHashSet：内部是LinkedHashMap。 TreeSet：内部是TreeMap的SortedSet。 ConcurrentSkipListSet：内部是ConcurrentSkipListMap的并发优化的SortedSet。 CopyOnWriteArraySet：内部是CopyOnWriteArrayList的并发优化的Set，利用其addIfAbsent（）方法实现元素去重，如前所述该方法的性能很一般。 基本操作我们可以看到HashSet类有这么一个属性 1private transient HashMap&lt;E,Object&gt; map; HashSet 所有的操作都是基于这个 map 展开的，map 的 value 值是空的 Object 对象。 1234567891011121314private static final Object PRESENT = new Object();public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125;public boolean remove(Object o) &#123; return map.remove(o)==PRESENT;&#125;public boolean contains(Object o) &#123; return map.containsKey(o);&#125;public int size() &#123; return map.size();&#125; 总结HashSet 利用 HashMap 的 key 不能重复特性实现，所有操作都是基于 HashMap 实现的。其他 Set 的实现方法也都是基于不同的 Map 实现的，就不一一讲述了。]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-TreeMap工作原理及实现]]></title>
    <url>%2F2019%2F05%2F09%2FJava-TreeMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[TreeMap概述以红黑树实现，红黑树又叫自平衡二叉树： 对于任一节点而言，其到叶节点的每一条路径都包含相同数目的黑结点。 上面的规定，使得树的层数不会差的太远，使得所有操作的复杂度不超过 O（lgn），但也使得插入，修改时要复杂的左旋右旋来保持树的平衡。 支持iterator（）时按Key值排序，可按实现了Comparable接口的Key的升序排序，或由传入的Comparator控制。可想象的，在树上插入/删除元素的代价一定比HashMap的大。 支持SortedMap接口，如firstKey（），lastKey（）取得最大最小的key，或sub（fromKey, toKey）, tailMap（fromKey）剪取Map的某一段。 put函数如果存在的话，old value被替换；如果不存在的话，则新添一个节点，然后对做红黑树的平衡操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V put(K key, V value) &#123; //获取根节点 TreeMap.Entry&lt;K,V&gt; t = root; //如果根节点为空则新建树 if (t == null) &#123; compare(key, key); // type (and possibly null) check root = new TreeMap.Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; &#125; int cmp; TreeMap.Entry&lt;K,V&gt; parent; // 根据comparator获取key的父节点 Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) &#123; do &#123; parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; else &#123; if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do &#123; parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; //节点不存在则新建 TreeMap.Entry&lt;K,V&gt; e = new TreeMap.Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) parent.left = e; else parent.right = e; //红黑树平衡调整 fixAfterInsertion(e); size++; modCount++; return null;&#125; get函数通过Comparable对比节点，当比较值等于0时就是key值对应的Entry。get函数则相对来说比较简单，以log(n)的复杂度进行get。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public V get(Object key) &#123; Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value);&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; // Offload comparator-based version for sake of performance if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; while (p != null) &#123; int cmp = k.compareTo(p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; &#125; return null;&#125;final Entry&lt;K,V&gt; getEntryUsingComparator(Object key) &#123; @SuppressWarnings(&quot;unchecked&quot;) K k = (K) key; Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) &#123; Entry&lt;K,V&gt; p = root; while (p != null) &#123; int cmp = cpr.compare(k, p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; &#125; &#125; return null;&#125; successor后继TreeMap是如何保证其迭代输出是有序的呢？其实从宏观上来讲，就相当于树的中序遍历(LDR)。我们先看一下迭代输出的步骤 123for(Entry&lt;Integer, String&gt; entry : tmap.entrySet()) &#123; System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 根据The enhanced for statement，for语句会做如下转换为： 1234for(Iterator&lt;Map.Entry&lt;String, String&gt;&gt; it = tmap.entrySet().iterator() ; tmap.hasNext(); ) &#123; Entry&lt;Integer, String&gt; entry = it.next(); System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 在it.next()的调用中会使用nextEntry调用successor这个是过的后继的重点，具体实现如下： 1234567891011121314151617181920212223static &lt;K,V&gt; TreeMap.Entry&lt;K,V&gt; successor(Entry&lt;K,V&gt; t) &#123; if (t == null) return null; else if (t.right != null) &#123; // 有右子树的节点，后继节点就是右子树的“最左节点” // 因为“最左子树”是右子树的最小节点 Entry&lt;K,V&gt; p = t.right; while (p.left != null) p = p.left; return p; &#125; else &#123; // 如果右子树为空，则寻找当前节点所在左子树的第一个祖先节点 // 因为左子树找完了，根据LDR该D了 Entry&lt;K,V&gt; p = t.parent; Entry&lt;K,V&gt; ch = t; // 保证左子树 while (p != null &amp;&amp; ch == p.right) &#123; ch = p; p = p.parent; &#125; return p; &#125;&#125; 怎么理解这个successor呢？只要记住，这个是中序遍历就好了，L-D-R。具体细节如下： a. 空节点，没有后继b. 有右子树的节点，后继就是右子树的“最左节点”c. 无右子树的节点，后继就是该节点所在左子树的第一个祖先节点 a.好理解，不过b, c，有点像绕口令啊，没关系，上图举个例子就懂了！ 有右子树的节点，节点的下一个节点，肯定在右子树中，而右子树中“最左”的那个节点则是右子树中最小的一个，那么当然是右子树的“最左节点”，就好像下图所示：无右子树的节点，先找到这个节点所在的左子树(右图)，那么这个节点所在的左子树的父节点(绿色节点)，就是下一个节点。 参考博客：https://yikun.github.io/2015/04/06/Java-TreeMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/http://calvin1978.blogcn.com/articles/collection.html]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-LinkedList工作原理及实现]]></title>
    <url>%2F2019%2F05%2F09%2FJava-LinkedList%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[LinkedList概述以双向链表实现。链表无容量限制，但双向链表本身使用了更多空间，也需要额外的链表指针操作。 按下标访问元素—get(i)/set(i,e) 要悲剧的遍历链表将指针移动到位(如果i&gt;数组大小的一半，会从末尾移起)。 插入、删除元素时修改前后节点的指针即可，但还是要遍历部分链表的指针才能移动到下标所指的位置，只有在链表两头的操作—add(), addFirst(),removeLast()或用iterator()上的remove()能省掉指针的移动。 LinkedList是一个简单的数据结构，与ArrayList不同的是，他是基于链表实现的。 Node类LinkedList的Node类是一个基本的双向链表 1234567891011private static class Node&lt;E&gt; &#123; E item; LinkedList.Node&lt;E&gt; next; LinkedList.Node&lt;E&gt; prev; Node(LinkedList.Node&lt;E&gt; prev, E element, LinkedList.Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; add函数将新增的Node关联到链表最后的位置，实现如下： 123456789101112131415public boolean add(E e) &#123; linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; get函数12345678910111213141516171819public E get(int index) &#123; //判断是否下标越界 checkElementIndex(index); return node(index).item;&#125;Node&lt;E&gt; node(int index) &#123; //判断index是前半部分还是后半部分，找到对应位置数据 if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; remove函数主要是通过prev.next = next;next.prev = prev;将前后Node关联起来，将index对应的Node从链表内移除。 12345678910111213141516171819202122232425262728public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125;E unlink(Node&lt;E&gt; x) &#123; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 参考博客：http://calvin1978.blogcn.com/articles/collection.html]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-ArrayList工作原理及实现]]></title>
    <url>%2F2019%2F05%2F09%2FJava-ArrayList%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[ArrayList描述以数组实现。节约空间，但数组有容量限制。超出限制时会增加50%容量，用System.arraycopy()复制到新的数组，因此最好能给出数组大小的预估值。默认第一次插入元素时创建大小为10的数组。 按数组下标访问元素—get(i)/set(i,e) 的性能很高，这是数组的基本优势。 直接在数组末尾加入元素—add(e)的性能也高，但如果按下标插入、删除元素—add(i,e), remove(i), remove(e)，则要用System.arraycopy()来移动部分受影响的元素，性能就变差了，这是基本劣势。 add函数add函数代码如下： 12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; ensureCapacityInternal自动扩容核心代码，具体实现如下： 123456789101112131415161718192021222324252627282930private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; //判断elementData是否等于空数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; //需求容量和默认容量相比，返回比较大的 return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 需求容量超过了数组容量，进行扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; // 扩展为原来的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 如果扩为1.5倍还不满足需求，直接扩为需求容量 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 将数据拷贝到新数组上 elementData = Arrays.copyOf(elementData, newCapacity);&#125; ArrayList大小不满足需求时，大小将会自动扩容为原来的1.5倍。 get函数实现就比较简单，直接获取数组指定下标的元素返回即可。代码实现如下： 12345678910public E get(int index) &#123; //判断是否下标越界 rangeCheck(index); return elementData(index);&#125;E elementData(int index) &#123; //直接返回下标对应数据 return (E) elementData[index];&#125; remove函数12345678910111213141516public E remove(int index) &#123; //判断是否下标越界 rangeCheck(index); modCount++; //获取对应下标数据 E oldValue = elementData(index); //将下标后的数据前进一格 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 其他函数还有一些其他函数，就简单说明下其实现。 contains：遍历数组，判断是否存在indexOf：遍历数组，返回下标addAll：判断数组大小是否满足需求，不满足则自动进行扩容。 参考博客：http://yikun.github.io/2015/04/04/Java-ArrayList%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-HashMap工作原理及实现]]></title>
    <url>%2F2019%2F05%2F09%2FHashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[概述 从本文你可以学习到： 什么时候会使用HashMap？他有什么特点？ 你知道HashMap的工作原理吗？ 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？ 你知道hash的实现吗？为什么要这样实现？ 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？ 当我们执行下面的操作时： 123456789101112HashMap&amp;lt;String, Integer&amp;gt; map = new HashMap&amp;lt;String, Integer&amp;gt;();map.put(&quot;语文&quot;, 1);map.put(&quot;数学&quot;, 2);map.put(&quot;英语&quot;, 3);map.put(&quot;历史&quot;, 4);map.put(&quot;政治&quot;, 5);map.put(&quot;地理&quot;, 6);map.put(&quot;生物&quot;, 7);map.put(&quot;化学&quot;, 8);for(Entry&amp;lt;String, Integer&amp;gt; entry : map.entrySet()) &#123;System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 运行结果是 12345678政治: 5生物: 7历史: 4数学: 2化学: 8语文: 1英语: 3地理: 6 发生了什么呢？下面是一个大致的结构，希望我们对HashMap的结构有一个感性的认识： 在官方文档中是这样描述HashMap的：Hash table based implementation of the Map interface. This implementation provides all of the optional map operations, and permits null values and the null key. (The HashMap class is roughly equivalent to Hashtable, except that it is unsynchronized and permits nulls.) This class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time. 几个关键的信息：基于Map接口实现、允许null键/值、非同步、不保证有序(比如插入的顺序)、也不保证序不随时间变化。 两个重要的参数 在HashMap中有两个很重要的参数，容量(Capacity)和负载因子(Load factor)Initial capacity The capacity is the number of buckets in the hash table, The initial capacity is simply the capacity at the time the hash table is created.Load factor The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased.简单的说，Capacity就是buckets的数目，Load factor就是buckets填满程度的最大比例。如果对迭代性能要求很高的话不要把capacity设置过大，也不要把load factor设置过小。当bucket填充的数目（即hashmap中元素的个数）大于capacity * load factor时就需要调整buckets的数目为当前的2倍。 put函数的实现 put函数大致的思路为： 对key的hashCode()做hash，然后再计算index; 如果没碰撞直接放到bucket里； 如果碰撞了，以链表的形式存在buckets后； 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换old value(保证key的唯一性) 如果bucket满了(超过load factor * current capacity)，就要resize。具体代码的实现如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 计算key对应的index if ((p = tab[i = (n - 1) &amp; hash]) == null) //节点不存在，直接添加到哈希桶数组 tab[i] = newNode(hash, key, value, null); else &#123; // 节点存在（哈希碰撞） Node&lt;K,V&gt; e; K k; //如果链表的第一个就是查询值则直接返回 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 该链表转化为红黑树（jdk1.8之后） else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 依然是链表，链表数据没有超过8个 else &#123; //单向链表，一直向后获取直到查询到对应key值 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 覆盖旧值并返回 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 超过 容量*负载因子(0.75)，调用 resize() 容量扩容为原来的两倍 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; get函数的实现在理解了put之后，get就很简单了。大致思路如下： bucket里的第一个节点，直接命中； 如果有冲突，则通过key.equals(k)去查找对应的entry若为树，则在树中通过key.equals(k)查找，O(logn)；若为链表，则在链表中通过key.equals(k)查找，O(n)。 具体代码的实现如下： 1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; // 对key的hashCode()做hash return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //tab如果为空直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 直接命中对应index上的值 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 未命中 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; hash函数的实现 在get和put的过程中，计算下标时，先对hashCode进行hash操作，然后再通过hash值进一步计算下标，如下图所示： 在对hashCode()计算hash时具体实现是这样的： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &amp;gt;&amp;gt;&amp;gt; 16);&#125; 可以看到这个函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或。其中代码注释是这样写的：Computes key.hashCode() and spreads (XORs) higher bits of hash to lower. Because the table uses power-of-two masking, sets of hashes that vary only in bits above the current mask will always collide. (Among known examples are sets of Float keys holding consecutive whole numbers in small tables.) So we apply a transform that spreads the impact of higher bits downward. There is a tradeoff between speed, utility, and quality of bit-spreading. Because many common sets of hashes are already reasonably distributed (so don’t benefit from spreading), and because we use trees to handle large sets of collisions in bins, we just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds.在设计hash函数时，因为目前的table长度n为2的幂，而计算下标的时候，是这样实现的(使用&amp;位操作，而非%求余)： 1(n - 1) &amp;amp; hash 设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。如果还是产生了频繁的碰撞，会发生什么问题呢？作者注释说，他们使用树来处理频繁的碰撞(we use trees to handle large sets of collisions in bins)，在JEP-180中，描述了这个问题：Improve the performance of java.util.HashMap under high hash-collision conditions by using balanced trees rather than linked lists to store map entries. Implement the same improvement in the LinkedHashMap class. 之前已经提过，在获取HashMap的元素时，基本分两步： 首先根据hashCode()做hash，然后确定bucket的index； 如果bucket的节点的key不是我们需要的，则通过keys.equals()在链中找。 在Java 8之前的实现中是用链表解决冲突的，在产生碰撞的情况下，进行get时，两步的时间复杂度是O(1)+O(n)。因此，当碰撞很厉害的时候n很大，O(n)的速度显然是影响速度的。因此在Java 8中，利用红黑树替换链表，这样复杂度就变成了O(1)+O(logn)了，这样在n很大的时候，能够比较理想的解决这个问题，在Java 8：HashMap的性能提升一文中有性能测试的结果。 RESIZE的实现 当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。resize的注释是这样描述的：Initializes or doubles table size. If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, the elements from each bin must either stay at same index, or move with a power of two offset in the new table.大致意思就是说，当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。怎么理解呢？例如我们从16扩展为32时，具体的变化如下所示： 因此元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 下面是代码的具体实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果有设置初始容量大小 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults //创建默认capacity = 16的初始容量大小 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 总结 我们现在可以回答开始的几个问题，加深对HashMap的理解： 什么时候会使用HashMap？他有什么特点？ 是基于Map接口的实现，存储键值对时，它可以接收null的键值，是非同步的，HashMap存储着Entry(hash, key, value, next)对象。 你知道HashMap的工作原理吗？ 通过hash的方法，通过put和get存储和获取对象。存储对象时，我们将K/V传给put方法时，它调用hashCode计算hash从而得到bucket位置，进一步存储，HashMap会根据当前bucket的占用情况自动调整容量(超过Load Facotr则resize为原来的2倍)。获取对象时，我们将K传给get，它调用hashCode计算hash从而得到bucket位置，并进一步调用equals()方法确定键值对。如果发生碰撞的时候，Hashmap通过链表将产生碰撞冲突的元素组织起来，在Java 8中，如果一个bucket中碰撞冲突的元素超过某个限制(默认是8)，则使用红黑树来替换链表，从而提高速度。 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？ 通过对key的hashCode()进行hashing，并计算下标( n-1 &amp; hash)，从而获得buckets的位置。如果产生碰撞，则利用key.equals()方法去链表或树中去查找对应的节点 你知道hash的实现吗？为什么要这样实现？ 在Java 1.8的实现中，是通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在bucket的n比较小的时候，也能保证考虑到高低bit都参与到hash的计算中，同时不会有太大的开销。 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？ 如果超过了负载因子(默认0.75)，则会重新resize一个原来长度两倍的HashMap，并且重新调用hash方法。 关于Java集合的小抄中是这样描述的： 12345678910111213以Entry[]数组实现的哈希桶数组，用Key的哈希值取模桶数组的大小可得到数组下标。插入元素时，如果两条Key落在同一个桶（比如哈希值1和17取模16后都属于第一个哈希桶），我们称之为哈希冲突。JDK的做法是链表法，Entry用一个next属性实现多个Entry以单向链表存放。查找哈希值为17的key时，先定位到哈希桶，然后链表遍历桶里所有元素，逐个比较其Hash值然后key值。在JDK8里，新增默认为8的阈值，当一个桶里的Entry超过閥值，就不以单向链表而以红黑树来存放以加快Key的查找速度。当然，最好还是桶里只有一个元素，不用去比较。所以默认当Entry数量达到桶数量的75%时，哈希冲突已比较严重，就会成倍扩容桶数组，并重新分配所有原来的Entry。扩容成本不低，所以也最好有个预估值。取模用与操作（hash &amp; （arrayLength-1））会比较快，所以数组的大小永远是2的N次方， 你随便给一个初始值比如17会转为32。默认第一次放入元素时的初始值是16。iterator（）时顺着哈希桶数组来遍历，看起来是个乱序 本文转载自：https://yikun.github.io/2015/04/01/Java-HashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot生产环境快速禁用Swagger2]]></title>
    <url>%2F2019%2F05%2F08%2FSpringBoot%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E5%BF%AB%E9%80%9F%E7%A6%81%E7%94%A8Swagger2%2F</url>
    <content type="text"><![CDATA[方法一使用注解@Profile({&quot;dev&quot;,&quot;test&quot;}) 表示在开发或测试环境开启，而在生产关闭。@Profile使用的值是根据spring.profiles.active指定的环境参数，可以参考上一篇博客SpringBoot入门篇之多环境配置文件。 简单介绍下@Profile注解@Profile 注解的作用在不同的场景下，给出不同的类实例。比如在生产环境中给出的 DataSource 实例和测试环境给出的 DataSource 实例是不同的。 @Profile 的使用时，一般是在@Configuration 下使用，标注在类或者方法上，标注的时候填入一个字符串（例如”dev”），作为一个场景,或者一个区分。 实际上，很少通过上面的方式激活 Spring 容器中的 Profile，通常都是让 Spring 容器自动去读取 Profile 的值，然后自动设置。这些实现通常是具体框架实现或者虚拟机参数/环境变量等相关。 方法二使用注解@ConditionalOnProperty(name = &quot;swagger.enable&quot;, havingValue = &quot;true&quot;) 然后在对应的application.properties/application.yml配置文件中添加 swagger.enable = true即可开启，生产环境不填则默认关闭Swagger。 以上两种方法都可以成功根据当前环境禁用swagger2,效果如下： 参考博客：https://cloud.tencent.com/developer/article/1362768https://www.jb51.net/article/153492.htm]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot入门篇之多环境配置文件]]></title>
    <url>%2F2019%2F05%2F08%2FSpringBoot%E5%85%A5%E9%97%A8%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[大多数系统都具有2个及以上的环境，测试和生产的最低配置吧。那我们如何在不同环境下使用不同的配置文件？ 使用spring.profiles.active来分区配置spring boot允许你通过命名约定按照一定的格式(application-{profile}.properties)来定义多个配置文件，然后在application.properties通过 spring.profiles.active来具体激活一个或者多个配置文件，如果没有没有指定任何profile的配置文件的话，spring boot默认会启动application-default.properties。 配置文件的优先级application.properties和application.yml文件可以放在一下四个位置： 外置，在相对于应用程序运行目录的/congfig子目录里。 外置，在应用程序运行的目录里 内置，在config包内 内置，在Classpath根目录 这个列表按照优先级排序，也就是说，src/main/resources/config下application.properties覆盖src/main/resources下application.properties中相同的属性，如图：此外，如果你在相同优先级位置同时有application.properties和application.yml，那么application.yml里面的属性就会覆盖application.properties里的属性。 通过命令行设置属性值给不同的环境添加不同的端口属性server.port，然后根据指定不同的spring.profiles.active来切换使用。java -jar xxx.jar –spring.profiles.active=test在命令行运行时，连续的两个减号–就是对application.properties中的属性值进行赋值的标识。所以，java -jar xxx.jar –spring.profiles.active=test 命令，等价于我们在application.properties/application.yml中添加属性spring.profiles.active=test。 参考博客：https://www.toutiao.com/i6392145028591911425/?group_id=6392140402656805122&amp;group_flags=0]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法笔记]]></title>
    <url>%2F2019%2F05%2F07%2FMarkdown%E8%AF%AD%E6%B3%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[快速入门MarkDown语法，常用的格式。 标题 1 到 6 个 # ，对应到标题 1 到 6 阶 列表* - + 都可以，格式如下 123* Candy.* Gum.* Booze. 效果： Candy. Gum. Booze. 有序列表1231. Red2. Green3. Blue 效果： Red Green Blue 链接格式：[链接](地址) 效果：博客 图片格式：![](图片地址)效果： 代码格式：使用``进行包围 代码块格式：使用前后各四个`包围 加粗格式：**加粗**效果：加粗 斜体格式：*斜体*效果：斜体 下划线格式：&lt;u&gt;下划线&lt;/u&gt;效果：下划线 水平线格式：* * *效果： 任务格式：* [ ] 任务效果： 任务 表格格式：| 1 | 2 || --- | --- || 3 | 4 |效果: 1 2 3 4]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
  </entry>
</search>
