<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深入JVM-内存模型]]></title>
    <url>%2F2020%2F01%2F20%2F%E6%B7%B1%E5%85%A5JVM-%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本文讨论以 JDK8 版本展开 Java虚拟机栈栈帧栈帧:栈帧(Stack Frame)是用于支持虚拟机进行方法调用和方法执行的数据结构。栈帧存储了方法的局部变量表、操作数栈、动态连接、方法返回地址和附件信息。每一个方法从调用至执行完成的过程，都对应着一个栈帧在虚拟机栈里从入栈到出栈的过程。 栈对应线程，栈帧对应方法 在活动线程中， 只有位于栈顶的帧才是有效的。称为当前栈帧，正在执行的方法称为当前方法，定义当前方法的类是当前类。在执行引擎运行时，所有指令都只能针对当前栈帧进行操作。而StackOverflowError 表示请求的栈溢出，导致内存耗尽，通常出现在递归方法中。虚拟机栈通过pop和push的方式，对每个方法对应的活动栈帧进行运算处理，方法正常执行结束，肯定会跳转到另一个栈帧上。在执行的过程中，如果出现了异常，会进行异常回溯，返回地址通过异常处理表确定。如果当前方法调用另一个方法完成，则该方法将不再是当前方法。调用方法时，将创建新栈帧，并在控制权转移到新方法时对应的栈帧。在方法返回时，当前栈帧将其方法调用的结果（如果有的话）传递回前一帧。当前一帧变为当前帧时，当前帧将被丢弃。 局部变量 每个栈帧都包含一个称为局部变量的变量数组，存放方法参数和方法内部定义的局部变量的区域。 局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。局部变量表中的变量不可直接使用，如需要使用的话，必须通过相关指令将其加载至操作数栈中作为操作数使用。Java虚拟机使用局部变量在方法调用时传递参数。在类方法调用时，所有参数都将从连续的局部变量（从局部变量0开始）传递。在调用实例方法时，始终使用局部变量0将引用传递给在其上调用实例方法的对象。随后将任何参数传递到从局部变量1开始的连续局部变量中。 1234public int test(int a, int b) &#123; Object obj = new Object(); return a + b;&#125; 如果局部变量是Java的8种基本基本数据类型，则存在局部变量表中，如果是引用类型。如new出来的String，局部变量表中存的是引用，而实例在堆中。 操作数栈 每个栈帧均包含一个后进先出（LIFO）堆栈，称为其操作数堆栈。框架的最大操作数堆栈深度是在编译时确定的。 Java虚拟机的解释执行引擎称为“基于栈的执行引擎”，其中所指的“栈”就是操作数栈。当JVM为方法创建栈帧的时候，在栈帧中为方法创建一个操作数栈，保证方法内指令可以完成工作。创建包含操作数堆栈的栈帧时，该操作数堆栈为空。Java虚拟机提供了将局部变量或字段中的常量或值加载到操作数堆栈上的指令。其他Java虚拟机指令从操作数堆栈中获取操作数，对其进行操作，然后将结果压回操作数堆栈。操作数堆栈还用于准备要传递给方法的参数并接收方法结果。 123456public class OperandStackTest &#123; public int sum(int a, int b) &#123; return a + b; &#125;&#125; 编译生成.class文件之后，再反汇编查看汇编指令 12&gt; javac OperandStackTest.java&gt; javap -v OperandStackTest.class &gt; 1.txt 例如，IADD 指令将两个int值加在一起。它要求int将要相加的值是操作数堆栈的前两个值，并由前面的指令压入该值。这两个int值都从操作数堆栈中弹出。将它们相加，然后将它们的总和推回操作数堆栈。子计算可嵌套在操作数堆栈上，从而产生可被包含计算使用的值。 1234567891011public int sum(int, int); descriptor: (II)I flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=3 // 最大栈深度为2 局部变量个数为3 0: iload_0 // 从[局部变量0]中装载int类型值入栈 1: iload_1 // 从[局部变量1]中装载int类型值入栈 2: iadd // 将栈顶元素弹出栈，执行int类型的加法，结果入栈 3: ireturn //从方法中返回int类型的数据 LineNumberTable: line 10: 0 动态链接 每个栈帧都包含一个指向运行时常量池中该栈帧所属方法的引用，持有这个引用是为了支持方法调用过程中的动态连接(Dynamic Linking)。 动态链接将这些符号引用转换为具体的方法引用，根据需要加载类以解析尚未定义的符号，并将变量访问转换为与这些变量的运行时位置关联的存储结构中的适当偏移量。 方法返回地址 当一个方法开始执行后,只有两种方式可以退出，一种是遇到方法返回的字节码指令;一种是遇见异常，并且 这个异常没有在方法体内得到处理。 无论何种退出情况，都将返回至方法当前被调用的位置。方法退出的过程相当于弹出当前栈帧，退出可能有三种方式： 返回值压入上层调用栈帧 异常信息抛给能够处理的栈帧 PC 计数器指向方法调用后的下一条指令 Java对象内存布局Java虚拟机栈的局部变量中引用类型是指向堆中的对象，元数据区中的静态变量如果是引用类型也会指向堆中的对象。那堆中的对象会指向元数据区嘛？元数据区中会包含类的信息，堆中会有对象，那怎么知道对象是哪个类创建的呢? 一个Java对象在内存中包括3个部分:对象头、实例数据和对齐填充。 对象头Mark Word：对象自身的运行时数据（Mark Word） 如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等 该部分数据被设计成1个非固定的数据结构 以便在极小的空间存储尽量多的信息（会根据对象状态复用存储空间） Class Pointer：对象类型指针 即对象指向它的类元数据的指针 虚拟机通过这个指针来确定这个对象是哪个类的实例 Length：保存数组长度 如果对象是数组，那么在对象头中还必须有一块用于记录数组长度的数据。 因为虚拟机可以通过普通Java对象的元数据信息确定对象的大小，但是从数组的元数据中却无法确定数组的大小。 实例数据存储的信息：对象真正有效的信息 包括了对象的所有成员变量，其大小由各个成员变量的大小决定，比如：byte和boolean是1个字节，short和char是2个字节，int和float是4个字节，long和double是8个字节，reference是4个字节（64位系统中是8个字节）。 这部分数据的存储顺序会受到虚拟机分配参数（FieldAllocationStyle）和字段在Java源码中定义顺序的影响。 1234567// HotSpot虚拟机默认的分配策略如下：longs/doubles、ints、shorts/chars、bytes/booleans、oop(Ordinary Object Pointers)// 从分配策略中可以看出，相同宽度的字段总是被分配到一起// 在满足这个前提的条件下，父类中定义的变量会出现在子类之前CompactFields = true；// 如果 CompactFields 参数值为true，那么子类之中较窄的变量也可能会插入到父类变量的空隙之中。 对齐填充存储的信息：占位符 占位作用 Java对象占用空间是8字节对齐的，即所有Java对象占用bytes数必须是8的倍数。例如，一个包含两个属性的对象：int和byte，这个对象需要占用8+4+1=13个字节，这时就需要加上大小为3字节的padding进行8字节对齐，最终占用大小为16个字节。 总结 内存模型 认识 一块是非堆区，一块是堆区。 堆区分为两大块，一个是Old区，一个是Young区。 Young区分为两大块，一个是Survivor区(S0+S1)，一块是Eden区。Eden:S0:S1=8:1:1 S0和S1一样大，也可以叫From和To。Eden区一般情况下，新创建的对象都会被分配到Eden区，一些特殊的大的对象会直接分配到Old区。 比如有对象A，B，C等创建在Eden区，但是Eden区的内存空间肯定有限，比如有100M，假如已经使用了 100M或者达到一个设定的临界值，这时候就需要对Eden内存空间进行清理，即垃圾收集(Garbage Collect)， 这样的GC我们称之为Minor GC，Minor GC指得是Young区的GC。经过GC之后，有些对象就会被清理掉，有些对象可能还存活着，对于存活着的对象需要将其复制到Survivor 区，然后再清空Eden区中的这些对象。 Survivor区Survivor区分为两块S0和S1，也可以叫做From和To。 在同一个时间点上，S0和S1只能有一个区有数据，另外一个是空的。 接着上面的GC来说，比如一开始只有Eden区和From中有对象，To中是空的。 此时进行一次GC操作，From区中对象的年龄就会+1，我们知道Eden区中所有存活的对象会被复制到To区，From区中还能存活的对象会有两个去处。若对象年龄达到之前设置好的年龄阈值，此时对象会被移动到Old区，此时Eden区和From区没有达到阈值的 对象会被复制到To区。 此时Eden区和From区已经被清空(被GC的对象肯定没了，没有被GC的对象都有了各自的去处)。这时候From和To交换角色，之前的From变成了To，之前的To变成了From。也就是说无论如何都要保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，知道To区被填满，然后会将所有对象复制到老年代中。 Old区一般Old区都是年龄比较大的对象，或者相对超过了某个阈值的对象。如果Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或者等于该年龄的对象就可以直接进入老年代。 在Old区也会有GC的操作，Old区的GC我们称作为Major GC。 默认阈值是15，我们可以通过JVM参数设置这个阈值。 空间分配担保在发生Minor GC之前，虚拟机会先检查老年代中最大可用连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。如果不成立，则虚拟机会查看HandlePromotionFailure设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于将尝试进行一次Minor GC。如果小于或者HandlePromotionFailure设置为不允许，那这时就改为一次Full GC。分配担保解释：新生代使用复制算法完成垃圾收集，为了节约内存Survivor的设置的比较小，当Minor GC后如果还有大量对象存活，超过了一个Survivor的内存空间，这时就需要老年代进行分配担保，把Survivor中无法容纳的对象直接进入老年代。若虚拟机检查老年代中最大可用连续空间大于新生代所有对象总空间那么就能保证不需要发生Full GC，因为老年代的内存空间够用。反之，如果老年代中最大可用连续空间小于新生代所有对象总空间就需要在尝试Minor GC失败后进行Full Gc或者直接Full GC。 调优 命令 解释 -XX:NewSize和-XX:MaxNewSize 用于设置年轻代的大小，建议设为整个堆大小的1/3或者1/4,两个值设为一样大。 -XX:InitialSurvivorRatio 用于设置新生代Eden/Survivor空间的初始比例 -XX:+PrintTenuringDistribution 这个参数用于显示每次Minor GC时Survivor区中各个年龄段的对象的大小。 -XX:NewRatio Old区/Young区的内存比例 －XX:MaxTenuringThreshold 配置一个对象从新生代晋升到老年代的阈值（默认值是15） -XX:+HeapDumpOnOutOfMemoryError 可以让JVM在遇到OOM异常时，输出堆内信息，特别是对相隔数月才出现的OOM异常尤为重要。 问题 如何理解Minor/Major/Full GC？ Minor GC:新生代 Major GC:老年代 Full GC:新生代+老年代 为什么需要Survivor区?只有Eden不行吗? 如果没有Survivor,Eden区每进行一次Minor GC,并且没有年龄限制的话,存活的对象就会被送到老年代。 这样一来，老年代很快被填满,触发Major GC(因为Major GC一般伴随着Minor GC,也可以看做触发了Full GC)。老年代的内存空间远大于新生代,进行一次Full GC消耗的时间比Minor GC长得多。 执行时间长有什么坏处?频发的Full GC消耗的时间很长,会影响大型程序的执行和响应速度。 可能你会说，那就对老年代的空间进行增加或者较少咯。 假如增加老年代空间，更多存活对象才能填满老年代。虽然降低Full GC频率，但是随着老年代空间加大,一旦发生Full GC,执行所需要的时间更长。 假如减少老年代空间，虽然Full GC所需时间减少，但是老年代很快被存活对象填满,Full GC频率增加。 所以Survivor的存在意义,就是减少被送到老年代的对象,进而减少Full GC的发生,Survivor的预筛选保证,只有经历16 次Minor GC还能在新生代中存活的对象,才会被送到老年代。 为什么需要两个Survivor区?最大的好处就是解决了碎片化。也就是说为什么一个Survivor区不行?第一部分中,我们知道了必须设置Survivor区。假设 现在只有一个Survivor区,我们来模拟一下流程:刚刚新建的对象在Eden中,一旦Eden满了,触发一次Minor GC,Eden中的存活对象就会被移动到Survivor区。这样继续循环下去,下一次Eden满了的时候,问题来了,此时进行Minor GC,Eden和Survivor各有一些存活对象,如果此时把Eden区的存活对象硬放到Survivor区,很明显这两部分对象所占有的内存是不连续的,也就导致了内存碎片化。永远有一个Survivor space是空的,另一个非空的Survivor space无碎片。 新生代中Eden:S1:S2为什么是8:1:1? 新生代中的可用内存:复制算法用来担保的内存为9:1 可用内存中Eden:S1区为8:1 即新生代中Eden:S1:S2 = 8:1:1 总结上图可以看出一个对象从出生到最后被回收的整个流程。 参考链接 JVM：Java对象的创建、内存布局 &amp; 访问定位 全过程解析 Java对象内存布局 精美图文带你掌握 JVM 内存布局 JDK官网]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入JVM-初步认识]]></title>
    <url>%2F2020%2F01%2F19%2F%E6%B7%B1%E5%85%A5JVM-%E5%88%9D%E6%AD%A5%E8%AE%A4%E8%AF%86%2F</url>
    <content type="text"><![CDATA[本文讨论以 JDK8 版本展开 JVM 是什么？JDK、JRE、JVM从Reference -&gt; Developer Guides -&gt; 定位到:官网JDK，我们可以得到这么一张图。 JDK（Java Development Kit）是针对Java开发员的产品，是整个Java的核心，包括了Java运行环境JRE、Java工具和Java基础类库。 JRE（Java Runtime Environment）是运行JAVA程序所必须的环境集合，包含JVM标准实现及Java核心类库。 JVM（Java Virtual Machine）是整个java实现跨平台的最核心的部分，能够运行以Java语言写作的软件程序。 JDK中包含JRE，在JDK的安装目录下有一个名为jre的目录，里面有两个文件夹bin和lib，在这里可以认为bin里的就是jvm，lib中则是jvm工作所需要的类库，而jvm和 lib合起来就称为jre。 JVM 如何实现跨平台？JVM（java virtual machine）就是我们常说的java虚拟机，它是整个java实现跨平台的最核心的部分，所有的java程序会首先被编译为.class的类文件，这种类文件可以在虚拟机上执行。也就是说class并不直接与机器的操作系统相对应，而是经过虚拟机间接与操作系统交互，由虚拟机将程序解释给本地系统执行。JVM屏蔽了与具体操作系统平台相关的信息，使得Java程序只需生成在Java虚拟机上运行的目标代码（字节码），就可以在多种平台上不加修改地运行。 编译过程将 .java文件转换为 .class文件，大概经历以下步骤。Person.java -&gt; 词法分析器 -&gt; tokens流 -&gt; 语法分析器 -&gt; 语法树/抽象语法树 -&gt; 语义分析器 -&gt; 注解抽象语法树 -&gt; 字节码生成器 -&gt; Person.class文件我们打开.class文件可以看到一下信息： 123456cafe babe 0000 0034 0027 0a00 0600 18090019 001a 0800 1b0a 001c 001d 0700 1e07001f 0100 046e 616d 6501 0012 4c6a 6176612f 6c61 6e67 2f53 7472 696e 673b 01000361 6765 0100 0149 0100 0761 6464 7265...... 与官网 The class File Format 进行解析可以知道，.class 字节码文件包含： 魔数与class文件版本 常量池 访问标志 类索引、父类索引、接口索引 字段表集合 方法表集合 属性表集合 类加载机制加载借助类加载器完成查找字节流的过程，并据此创建类。 通过一个类的全限定名获取定义此类的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在Java堆中生成一个代表这个类的java.lang.Class 对象，作为对方法区中这些数据的访问入口。 启动类加载器是由C++实现的，其他的类加载器都是java.lang.ClassLoader 的子类。这些类加载器都需要由另一个类加载器（比如启动类加载器），加载至Java虚拟机中才能执行类加载。检查某个类是否已经加载:顺序是自底向上，从Custom ClassLoader到BootStrap ClassLoader逐层检查，只要某个Classloader已加载，就视为已加载此类，保证此类只所有ClassLoader加载一次。Java虚拟机的双亲委派模型： 定义：每当一个类加载器接收到加载请求时，它会先将请求转发给父类加载器。在父类加载器没有找到所请求的类的情况下，该类加载器才会尝试去加载。 优势:Java类随着加载它的类加载器一起具备了一种带有优先级的层次关系。比如，Java 中的 Object 类，它存放在rt.jar之中,无论哪一个类加载器要加载这个类，最终都是委派给处于模型 最顶端的启动类加载器进行加载，因此Object在各种类加载环境中都是同一个类。如果不采用 双亲委派模型，那么由各个类加载器自己取加载的话，那么系统中会存在多种不同的Object 类。 我们还可以自定义类加载器，实现特殊的加载方式。在Java虚拟机中，类的唯一性是由类加载器实例以及类的全名一同确定的。 链接将创建成的类合并至Java虚拟机中，使之能够执行的过程。它可分为验证、准备以及解析三个阶段。 验证验证为了确保被加载类能够满足Java虚拟机的约束条件。 文件格式验证 元数据验证 字节码验证 符号引用验证 准备准备是为被加载类的静态字段分配内存。对于一个方法调用，编译器会生成一个包含目标方法所在类的名字、目标方法的名字、接收参数类型以及返回值类型的符号引用，来指代所要调用的方法。 解析解析阶段的目的，正是将这些符号引用解析成为实际引用。如果符号引用指向一个未被加载的类，或者未被加载类的字段或方法，那么解析将触发这个类的加载（但未必触发这个类的链接以及初始化）。如果某些字节码使用了符号引用，那么在执行这些字节码之前需要完成对这些符号引用的解析。 初始化如果要初始化一个静态字段我们可以在声明时直接赋值，也可以在静态代码块中对其赋值。如果直接赋值的静态字段被 final 所修饰，并且它的类型是基本类型或字符串时，那么该字段便会被 Java 编译器标记成为常量，其初始化由Java虚拟机完成。除此之外的赋值操作以及所有静态代码块中的代码则会被Java编译器置于同一方法中，并把它命名为 clinit。 类加载的最后一步是初始化，便是为标记为常量的字段赋值，以及执行 clinit 方法的过程。Java虚拟机会通过加锁来确保类的 clinit 方法仅被执行一次。 总结加载是指查找字节流，并且据此创建类的过程。加载需要借助类加载器，在 Java 虚拟机中类加载器使用了双亲委派模型，即接收到加载请求时，会先将请求转发给父类加载器。链接，是指将创建成的类合并至 Java 虚拟机中，使之能够执行的过程。链接还分验证、 准备和解析三个阶段。其中，解析阶段为非必须的。初始化，则是为标记为常量值的字段赋值，以及执行 &lt; clinit &gt; 方法的过程。类的初始化仅会被执行一次，这个特性被用来实现单例的延迟初始化。类的初始化何时会被触发呢？JVM规范枚举了下述多种触发情况： 当虚拟机启动时，初始化用户指定的主类； 当遇到⽤以新建⽬标类实例的 new 指令时，初始化 new 指令 的⽬标类； 当遇到调用静态方法的指令时，初始化该静态字段所在的类； 当遇到访问静态字段的指令时，初始化该静态字段所在的类； 子类的初始化会触发父类的初始化； 如果一个接口定义了default方法，那么直接实现或者间接实现该接口的类的初始化，会触发该接口的初始化； 使用反射API对某个类进行反射调用时，初始化这个类； 当初次调用MethodHandle实例时，初始化该MethodHandle指向的方法所在的类。 运行时数据区The Java Virtual Machine defines various run-time data areas that are used during execution of a program. Some of these data areas are created on Java Virtual Machine start-up and are destroyed only when the Java Virtual Machine exits. Other data areas are per thread. Per-thread data areas are created when a thread is created and destroyed when the thread exits.根据官网描述：Java虚拟机定义了在程序执行期间使用的各种运行时数据区域。其中一些数据区域是在Java虚拟机启动时创建的，仅在Java虚拟机退出时才被销毁。其他数据区域是每个线程的。创建线程时创建每个线程的数据区域，并在线程退出时销毁每个数据区域。 如果按照线程是否共享来分类的话，如下图所示： Metaspace (元空间)定义Java8之前叫做永久代（ ≈ 方法区）中用于存放类和方法的元数据以及常量池，比如Class和Method。每当一个类初次被加载的时候，它的元数据都会放到永久代中。永久代是有大小限制的，因此如果加载的类太多，很有可能导致永久代内存溢出，为此我们不得不对虚拟机做调优。Java 8 中 PermGen 为什么被移出 HotSpot JVM 了？（详见：JEP 122: Remove the Permanent Generation）： 由于 PermGen 内存经常会溢出，引发恼人的 java.lang.OutOfMemoryError: PermGen，因此 JVM 的开发者希望这一块内存可以更灵活地被管理，不要再经常出现这样的 OOM。 移除 PermGen 可以促进 HotSpot JVM 与 JRockit VM 的融合，因为 JRockit 没有永久代。 Java7开始字符串常量池移至堆区，Java8开始 PermGen 被元空间代替，其他内容比如类元信息、字段、静态属性、方法、常量等都移动到元空间区。官网定义如下： Java虚拟机具有一个在所有Java虚拟机线程之间共享的方法区域。该方法区域类似于常规语言的编译代码的存储区域，或者类似于操作系统过程中的“文本”段。它存储每个类的结构，例如运行时常量池，字段和方法数据，以及方法和构造函数的代码，包括用于类和实例初始化以及接口初始化的特殊方法。 方法区域是在虚拟机启动时创建的。尽管方法区域在逻辑上是堆的一部分，但是简单的实现可以选择不进行垃圾回收或压缩。该规范没有规定方法区域的位置或用于管理已编译代码的策略。方法区域可以是固定大小的，或者可以根据计算的需要进行扩展，如果不需要更大的方法区域，则可以缩小。方法区域的内存不必是连续的。 元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。默认情况下，元空间的大小仅受本地内存限制。 调优Java虚拟机实现可以为程序员或用户提供对方法区域初始大小的控制，以及在方法区域大小可变的情况下，可以控制最大和最小方法区域大小。 参数 作用 -XX:MetaspaceSize 分配给Metaspace（以字节计）的初始大小 -XX:MaxMetaspaceSize 分配给Metaspace 的最大值，超过此值就会触发Full GC，此值默认没有限制，但应取决于系统内存的大小。JVM会动态地改变此值。 -XX:MinMetaspaceFreeRatio 在GC之后，最小的Metaspace剩余空间容量的百分比，减少为分配空间所导致的垃圾收集。 -XX:MaxMetaspaceFreeRatio 在GC之后，最大的Metaspace剩余空间容量的百分比，减少为释放空间所导致的垃圾收集 异常如果无法提供方法区域中的内存来满足分配请求，则Java虚拟机将抛出一个OutOfMemoryError。 Heap (堆区）定义Java虚拟机具有一个在所有Java虚拟机线程之间共享的堆。堆是运行时数据区（内存区域中最大的一块区域），从中分配所有类实例和数组的内存。堆是在虚拟机启动时创建的。对象的堆存储由自动存储管理系统（称为垃圾收集器）回收的主要区域，对象永远不会显式释放。Java虚拟机不假定特定类型的自动存储管理系统，可以根据实现者的系统要求选择存储管理技术。堆的大小可以是固定的，也可以根据计算的要求进行扩展。如果不需要更大的堆，则可以将其收缩。堆的内存不必是连续的。 调优Java虚拟机实现可以为程序员或用户提供对堆的初始大小的控制，并且可以动态扩展或收缩堆，可以控制最大和最小堆大小。通过设置如下参数，可以设定堆区的初始值和最大值，比如 -Xms256M -Xmx 1024M，其中 -X这个字母代表它是JVM运行时参数，ms是 memory start 的简称，中文意思就是内存初始值，mx 是 memory max 的简称，意思就是最大内存。在通常情况下，服务器在运行过程中，堆空间不断地扩容与回缩，会形成不必要的系统压力 所以在线上生产环境中 JVM的 Xms 和 Xmx 会设置成同样大小，避免在GC 后调整堆大小时带来的额外压力。 异常如果计算需要的堆多于自动存储管理系统可以提供的堆，则Java虚拟机将抛出一个 OutOfMemoryError。 Java Virtual Machine Stacks(虚拟机栈)定义每个Java虚拟机线程都有一个私有Java虚拟机堆栈，与该线程同时创建。除了Native方法以外，Java方法都是通过Java 虚拟机栈来实现调用和执行过程的（需要程序技术器、堆、元空间内数据的配合）。所以Java虚拟机栈是虚拟机执行引擎的核心之一。而Java虚拟机栈中出栈入栈的元素就称为「栈帧」。栈帧(Stack Frame)是用于支持虚拟机进行方法调用和方法执行的数据结构。栈帧存储了方法的局部变量表、操作数栈、动态连接和方法返回地址等信息。每一个方法从调用至执行完成的过程，都对应着一个栈帧在虚拟机栈里从入栈到出栈的过程。 调优该规范允许Java虚拟机堆栈具有固定大小，或根据计算要求动态扩展和收缩。如果Java虚拟机堆栈的大小固定，则在创建每个Java虚拟机堆栈时可以独立选择其大小。Java虚拟机实现可以为程序员或用户提供对Java虚拟机堆栈初始大小的控制，并且在动态扩展或收缩Java虚拟机堆栈的情况下，可以控制最大和最小大小。 -Xss：每个线程的栈容量大小 异常 如果线程中的计算所需的Java虚拟机堆栈超出允许的范围，则Java虚拟机将抛出StackOverflowError。 如果可以动态扩展Java虚拟机堆栈，并尝试进行扩展，但是可以提供足够的内存来实现扩展，或者如果没有足够的内存来为新线程创建初始Java虚拟机堆栈，则Java虚拟机机器抛出一个OutOfMemoryError。 The pc Register(程序计数器)定义程序计数器占用的内存空间很小，由于Java虚拟机的多线程是通过线程轮流切换，并分配处理器执行时间的方式来实现的，在任意时刻，一个处理器只会执行一条线程中的指令。因此，为了线程切换后能够恢复到正确的执行位置，每条线程需要有一个独立的程序计数器(线程私有)。 如果线程正在执行Java方法，则计数器记录的是正在执行的虚拟机字节码指令的地址; 如果正在执行的是Native方法，则这个计数器为空。 Native Method Stacks(本地方法栈)定义Java虚拟机的实现可以使用传统的堆栈（俗称“ C堆栈”）来支持native方法（用Java编程语言以外的语言编写的方法）。解释程序的实现也可以使用诸如C之类的语言来解释Java虚拟机的指令集，以使用 native 本机方法栈。无法加载方法并且自身不依赖于常规堆栈的Java虚拟机实现无需提供本机方法栈。如果提供，通常在创建每个线程时为每个线程分配本机方法堆栈。 异常 如果线程中的计算需要比允许的更大的本机方法堆栈，则Java虚拟机将抛出StackOverflowError。 如果可以动态扩展本机方法堆栈并尝试进行本机方法堆栈扩展，但可以提供足够的内存，或者可以提供足够的内存来为新线程创建初始本机方法堆栈，则Java虚拟机将抛出OutOfMemoryError。 参考 精美图文带你掌握 JVM 内存布局 极客时间-深入拆解Java虚拟机专栏 《深入理解Java虚拟机》 - 周志明 JDK8官网]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[分库分表介绍]]></title>
    <url>%2F2020%2F01%2F17%2F%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[为什么要分库分表对于应用来说，如果数据库性能出现问题，要么是无法获取连接，是因为在高并发的情况下连接数不够了。要么是操作数据变慢，数据库处理数据的效率出了问题。要么 是存储出现问题，比如单机存储的数据量太大了，存储的问题也可能会导致性能的问题。归根结底都是受到了硬件的限制，比如 CPU，内存，磁盘，网络等等。但是我们优化肯定不可能直接从扩展硬件入手，因为带来的收益和成本投入比例太低。 数据库优化方案对比SQL与索引因为 SQL 语句是在我们的应用端编写的，所以第一步，我们可以在程序中对 SQL 语句进行优化，最终的目标是用到索引。这个是容易的也是最常用的优化手段。 表与存储引擎数据是存放在表里面的，表又是以不同的格式存放在存储引擎中的，所以我们可以选用特定的存储引擎，或者对表进行分区，对表结构进行拆分或者冗余处理， 或者对表结构比如字段的定义进行优化。 架构 如果只有一台数据库的服务器，我们可以运行多个实例，做集群的方案，做负载均衡。 基于主从复制实现读写分离，让写的服务都访问 master 服务器，读的请求都访问从服务器，slave 服务器自动 master 主服务器同步数据。 在数据库前面加一层缓存，达到减少数据库的压力，提升访问速度的目的。 为了分散数据库服务的存储压力和访问压力，我们也可以把不同的数据分布到不同的服务节点，这个就是分库分表(scale out)。主从通过数据冗余实现高可用和实现读写分离，分片通过拆分数据分散存储和访问压力。 配置数据库配置的优化，比如连接数，缓冲区大小等等，优化配置的目的都是为了更高效地利用硬件。 操作系统与硬件操作系统和硬件的优化。 总结从上往下，成本收益比慢慢地在增加。所以肯定不是查询一慢就堆硬件，堆硬件叫做向上的扩展(scale up)。 分库分表当我们对原来一个数据库的表做了分库以后，其中一些表的数据还在以一个非常快的速度在增长，这个时候查询也已经出现了非常明显的效率下降。在分库之后，还需要进一步进行分表。当然，我们最开始想到的可能是在一个数据库里面拆分数据，分区或者分表，到后面才是切分到多个数据库中。分表主要是为了减少单张表的大小，解决单表数据量带来的性能问题。分库分表会提升系统的复杂度，如果在近期或者未来一段时间内非必要解决存储和性能的问题，就不要去做超前设计和过度设计。就像我们搭建项目，从快速实现的角度来说，肯定是从单体项目起步的，在业务丰富完善之前，也用不到微服务架构。如果我们创建的表结构合理、字段不是太多，并且索引创建正确的情况下，单张表存储几千万的数据是完全没有问题的，这个还是以应用的实际情况为准。当然我们也会 对未来一段时间的业务发展做一个预判。 分库分表的类型和特点从维度来说分成两种，一种是垂直，一种是水平。 垂直切分:基于表或字段划分，表结构不同。我们有单库的分表，也有多库的分库。 水平切分:基于数据划分，表结构相同，数据不同，也有同库的水平切分和多库的切分。 垂直切分垂直分表有两种，一种是单库的，一种是多库的。 单库垂直分表单库分表，比如:商户信息表，拆分成基本信息表，联系方式表，结算信息表，附件表等等。 多库垂直分表多库垂直分表就是把原来存储在一个库的不同的表，拆分到不同的数据库。比如:消费金融核心系统数据库，有很多客户相关的表，这些客户相关的表，全部单独存放到客户的数据库里面。合同，放款、风控相关的业务表也是一样的。当我们对原来的一张表做了分库的处理，如果某些业务系统的数据还是有一个非常快的增长速度，比如说还款数据库的还款历史表，数据量达到了几个亿，这个时候硬件限制导致的性能问题还是会出现，所以从这个角度来说垂直切分并没有从根本上解决单 库单表数据量过大的问题。在这个时候，我们还需要对我们的数据做一个水平的切分。 水平切分当我们的客户表数量已经到达数千万甚至上亿的时候，单表的存储容量和查询效率 都会出现问题，我们需要进一步对单张表的数据进行水平切分。水平切分的每个数据库 的表结构都是一样的，只是存储的数据不一样，比如每个库存储 1000 万的数据。水平切分也可以分成两种，一种是单库的，一种是多库的。 单库水平分表银行的交易流水表，所有进出的交易都需要登记这张表，因为绝大部分时候客户都是查询当天的交易和一个月以内的交易数据，所以我们根据使用频率把这张表拆分成三 张表: 当天表:只存储当天的数据。 当月表:在夜间运行一个定时任务，前一天的数据，全部迁移到当月表。用的是 insert into select，然后 delete。 历史表:同样是通过定时任务，把登记时间超过 30 天的数据，迁移到 history 历史表(历史表的数据非常大，我们按照月度，每个月建立分区)。 消费金融公司跟线下商户合作，给客户办理了贷款以后，消费金融公司要给商户返 费用，或者叫提成，每天都会产生很多的费用的数据。为了方便管理，我们每个月建立 一张费用表，例如 fee_detail_201901……fee_detail_201912。跟分区一样，这种方式虽然可以一定程度解决单表查询性能的问题，但 是并不能解决单机存储瓶颈的问题。 多库水平分表一般我们说的分库分表都是跨库的分表。比如客户表，我们拆分到多个库存储，表结构是完全一样的。 多案分库分表带来的问题跨库关联查询比如查询在合同信息的时候要关联客户数据，由于是合同数据和客户数据是在不同的数据库，那么我们肯定不能直接使用 join 的这种方式去做关联查询。 字段冗余如我们查询合同库的合同表的时候需要关联客户库的客户表，我们可以直接把一些经常关联查询的客户字段放到合同表，通过这种方式避免跨库关联查询的问题。 数据同步比如商户系统要查询产品系统的产品表，我们干脆在商户系统创建一张产品表，通过 ETL 或者其他方式定时同步产品数据。 全局表比如行名行号信息被很多业务系统用到，如果我们放在核心系统，每个系统都要去关联查询，这个时候我们可以在所有的数据库都存储相同的基础数据。 ER 表我们有些表的数据是存在逻辑的主外键关系的，比如订单表 order_info，存的是汇总的商品数、商品金额;订单明细表 order_detail，是每个商品的价格、个数等等。或者叫做从属关系，父表和子表的关系。他们之间会经常有关联查询的操作，如果父表的数 据和子表的数据分别存储在不同的数据库，跨库关联查询也比较麻烦。所以我们能不能把父表和数据和从属于父表的数据落到一个节点上呢?比如 order_id=1001 的数据在 node1，它所有的明细数据也放到 node1; order_id=1002 的数据在 node2，它所有的明细数据都放到 node2，这样在关联查询的 时候依然是在一个数据库。上面的思路都是通过合理的数据分布避免跨库关联查询，实际上在我们的业务中， 也是尽量不要用跨库关联查询，如果出现了这种情况，就要分析一下业务或者数据拆分 是不是合理。如果还是出现了需要跨库关联的情况，那我们就只能用最后一种办法。 系统层组装在不同的数据库节点把符合条件数据的数据查询出来，然后重新组装，返回给客户端。 分布式事务比如在一个贷款的流程里面，合同系统登记了数据，放款系统也必须生成放款记录，如果两个动作不是同时成功或者同时失败，就会出现数据一致性的问题。如果在一个数 据库里面，我们可以用本地事务来控制，但是在不同的数据库里面就不行了。所以分布式环境里面的事务，我们也需要通过一些方案来解决。分布式系统的基础是 CAP 理论。 C (一致性) Consistency:对某个指定的客户端来说，读操作能返回最新的写操作。对于数据分布在不同节点上的数据来说，如果在某个节点更新了数据，那么在其他节点如果都能读取到这个最新的数据，那么就称为强一致，如果有某个节点没有读取到，那就是分布式不一致。 A (可用性) Availability:非故障的节点在合理的时间内返回合理的响应(不是错误和超时的响应)。可用性的两个关键一个是合理的时间，一个是合理的响应。合理的时间指的是请求不能无限被阻塞，应该在合理的时间给出返回。合理的响应指的是系统应该明确返回结果并且结果是正确的。 P (分区容错性) Partition tolerance:当出现网络分区后，系统能够继续工作。打个比方，这里集群有多台机器，有台机器网络出现了问题，但是这个集群仍然可以工作。 CAP 三者是不能共有的，只能同时满足其中两点。基于 AP，我们又有了 BASE 理论。 基本可用(Basically Available):分布式系统在出现故障时，允许损失部分可用功能，保证核心功能可用。 软状态(Soft state):允许系统中存在中间状态，这个状态不影响系统可用性，这里指的是 CAP 中的不一致。 最终一致(Eventually consistent):最终一致是指经过一段时间后，所有节点数 据都将会达到一致。 分布式事务有几种常见的解决方案: 全局事务(比如 XA 两阶段提交;应用、事务管理器(TM)、资源管理器(DB))， 例如 Atomikos 基于可靠消息服务的分布式事务 柔性事务 TCC(Try-Confirm-Cancel)tcc-transaction 最大努力通知，通过消息中间件向其他系统发送消息(重复投递+定期校对) 排序、翻页、函数计算问题跨节点多库进行查询时，会出现 limit 分页，order by 排序的问题。比如有两个节点，节点 1 存的是奇数 id=1,3,5,7,9……;节点 2 存的是偶数 id=2,4,6,8,10……执行 select * from user_info order by id limit 0,10需要在两个节点上各取出 10 条，然后合并数据，重新排序。 max、min、sum、count 之类的函数在进行计算的时候，也需要先在每个分片上执行相应的函数，然后将各个分片的结果集进行汇总和再次计算，最终将结果返回。 全局主键避重问题MySQL 的数据库里面字段有一个自增的属性，Oracle 也有 Sequence 序列。如果是一个数据库，那么可以保证 ID 是不重复的，但是水平分表以后，每个表都按照自己的规律自增，肯定会出现 ID 重复的问题，这个时候我们就不能用本地自增的方式了。 UUID(Universally Unique Identifier 通用唯一识别码)UUID 标准形式包含 32 个 16 进制数字，分为 5 段，形式为 8-4-4-4-12 的 36 个字符，例如:c4e7956c-03e7-472c-8909-d733803e79a9。xxxxxxxx-xxxx-Mxxx-Nxxx-xxxxxxxxxxxxM 表示 UUID 版本，目前只有五个版本，即只会出现 1，2，3，4，5，数字 N 的 一至三个最高有效位表示 UUID 变体，目前只会出现 8，9，a，b 四种情况。 a. 基于时间和 MAC 地址的 UUID b. 基于第一版却更安全的 DCE UUID c. 基于 MD5 散列算法的 UUID d. 基于随机数的 UUID——用的最多，JDK 里面是 4 e. 基于 SHA1 散列算法的 UUID UUID 是主键是最简单的方案，本地生成，性能高，没有网络耗时。但缺点也很明显，由于 UUID 非常长，会占用大量的存储空间;另外作为主键建立索引和基于索引进行 查询时都会存在性能问题，在 InnoDB 中，UUID 的无序性会引起数据位置频繁变动，导致分页。 数据库把序号维护在数据库的一张表中。这张表记录了全局主键的类型、位数、起始值、当前值。当其他应用需要获得全局 ID 时，先 for update 锁行，取到值+1后并且更新后返回。并发性比较差。 Redis基于 Redis 的 INT 自增的特性，使用批量的方式降低数据库的写压力，每次获取一段区间的 ID 号段，用完之后再去数据库获取，可以大大减轻数据库的压力。 雪花算法 Snowflake(64bit) a. 使用 41bit 作为毫秒数，可以使用 69 年 b. 10bit 作为机器的 ID(5bit 是数据中心，5bit 的机器 ID)，支持 1024 个节点 c. 12bit 作为毫秒内的流水号(每个节点在每毫秒可以产生 4096 个 ID) d. 最后还有一个符号位，永远是 0。 优点:毫秒数在高位，生成的 ID 整体上按时间趋势递增;不依赖第三方系统，稳定性和效率较高，理论上 QPS 约为 409.6w/s(1000*2^12)，并且整个分布式系统内不会产生 ID 碰撞;可根据自身业务灵活分配 bit 位。不足就在于:强依赖机器时钟，如果时钟回拨，则可能导致生成 ID 重复。 多数据源/读写数据源的解决方案当我们对数据做了切分，分布在不同的节点上存储的时候，是不是意味着会产生多个数据源?既然有了多个数据源，那么在我们的项目里面就要配置多个数据源。我们先要分析一下 SQL 执行经过的流程。DAO——Mapper(ORM)——JDBC——代理——数据库服务 客户端 DAO 层第一个就是在我们的客户端的代码，比如 DAO 层，在我们连接到某一个数据源之前，我们先根据配置的分片规则，判断需要连接到哪些节点，再建立连接。Spring 中提供了一个抽象类 AbstractRoutingDataSource，可以实现数据源的动态切换。优势:不需要依赖 ORM 框架，即使替换了 ORM 框架也不受影响。实现简单(不需要解析 SQL 和路由规则)，可以灵活地定制。缺点:不能复用，不能跨语言。 ORM 框架层比如我们用 MyBatis 连接数据库，也可以指定数据源。我们可以基于 MyBatis 插件的拦截机制(拦截 query 和 update 方法)，实现数据源的选择。例如:https://github.com/colddew/shardbatis https://docs.jboss.org/hibernate/stable/shards/reference/en/html_single/ 驱动层不管是 MyBatis 还是 Hibernate，还是 Spring 的 JdbcTemplate，本质上都是对 JDBC 的封装，所以第三层就是驱动层。比如 Sharding-JDBC，就是对 JDBC 的对象进行了封装。JDBC 的核心对象: DataSource:数据源 Connection:数据库连接 Statement:语句对象 ResultSet:结果集 那我们只要对这几个对象进行封装或者拦截或者代理，就可以实现分片的操作。 代理层前面三种都是在客户端实现的，也就是说不同的项目都要做同样的改动，不同的编程语言也有不同的实现，所以我们能不能把这种选择数据源和实现路由的逻辑提取出来， 做成一个公共的服务给所有的客户端使用呢?这个就是第四层代理层。比如 Mycat 和 Sharding-Proxy，都是属于这一层。 数据库服务最后一层就是在数据库服务上实现，也就是服务层，某些特定的数据库或者数据库的特定版本可以实现这个功能。]]></content>
      <categories>
        <category>分库分表</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入MySQL-性能优化]]></title>
    <url>%2F2020%2F01%2F14%2F%E6%B7%B1%E5%85%A5MySQL-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[优化思路我们说到性能调优，大部分时候想要实现的目标是让我们的查询更快。一个查询的 动作又是由很多个环节组成的，每个环节都会消耗时间。我们要减少查询所消耗的时间，就要从每一个环节入手。 连接——配置优化第一个环节是客户端连接到服务端，连接这一块有可能会出现什么样的性能问题? 有可能是服务端连接数不够导致应用程序获取不到连接。比如报了一个 Mysql: error 1040: Too many connections 的错误。我们可以从两个方面来解决连接数不够的问题: 从服务端来说，我们可以增加服务端的可用连接数。 如果有多个应用或者很多请求同时访问数据库，连接数不够的时候，我们可以:修改配置参数增加可用连接数，修改 max_connections 的大小:1show variables like &apos;max_connections&apos;; -- 修改最大连接数，当有多个应用连接的时候 或者及时释放不活动的连接。交互式和非交互式的客户端的默认超时时间都是 28800 秒，8 小时，我们可以把这个值调小。 1show global variables like &apos;wait_timeout&apos;; --及时释放不活动的连接，注意不要释放连接池还在使用的连接 从客户端来说，可以减少从服务端获取的连接数。我们可以引入连接池，实现连接的重用。ORM 层面(MyBatis 自带了一个连接池);或者使用专用的连接池工具(阿里的 Druid、Spring Boot 2.x 版本默认的连接池 Hikari、老牌的 DBCP 和 C3P0)。当客户端改成从连接池获取连接之后，连接池的大小应该怎么设置呢?大家可能会有一个误解，觉得连接池的最大连接数越大越好，这样在高并发的情况下客户端可以获取的连接数更多，不需要排队。实际情况并不是这样。连接池并不是越大越好，只要维护一定数量大小的连接池，其他的客户端排队等待获取连接就可以了。有的时候连接池越大，效率反而越低。Druid 的默认最大连接池大小是 8。Hikari 的默认最大连接池大小是 10。在 Hikari 的 github 文档中，给出了一个 PostgreSQL 数据库建议的设置连接池大小的公式:https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing它的建议是机器核数乘以 2 加 1。也就是说，4 核的机器，连接池维护 9 个连接就够了。这个公式从一定程度上来说对其他数据库也是适用的。这里面还有一个减少连接池大小实现提升并发度和吞吐量的案例。每一个连接，服务端都需要创建一个线程去处理它。连接数越多，服务端创建的线程数就会越多。CPU 是怎么同时执行远远超过它的核数大小的任务的?分配时间片进行上下文切换。 而 CPU 的核数是有限的，频繁的上下文切换会造成比较大的性能开销。 缓存——架构优化缓存在应用系统的并发数非常大的情况下，如果没有缓存，会造成两个问题:一方面是会给数据库带来很大的压力。另一方面，从应用的层面来说，操作数据的速度也会受到 影响。我们可以用第三方的缓存服务来解决这个问题，例如 Redis。运行独立的缓存服务，属于架构层面的优化。 主从复制如果单台数据库服务满足不了访问需求，那我们可以做数据库的集群方案。集群的话必然会面临一个问题，就是不同的节点之间数据一致性的问题。如果同时读写多台数据库节点，怎么让所有的节点数据保持一致?这个时候我们需要用到复制技术(replication)，被复制的节点称为 master，复制的节点称为 slave。slave 本身也可以作为其他节点的数据来源，这个叫做级联复制。主从复制是怎么实现的呢?更新语句会记录 binlog，它是一种逻辑日志。有了这个 binlog，从服务器会获取主服务器的 binlog 文件，然后解析里面的 SQL 语句，在从服务器上面执行一遍，保持主从的数据一致。这里面涉及到三个线程： 连接到 master 获取 binlog，并且解析 binlog 写入中继日志，这个线程叫做 I/O 线程。 Master 节点上有一个 log dump 线程，是用来发送 binlog 给 slave 的。 从库的 SQL 线程，是用来读取 relay log，把数据写入到数据库的。 做了主从复制的方案之后，我们只把数据写入 master 节点，而读的请求可以分担到 slave 节点。我们把这种方案叫做读写分离。 单线程读写分离可以一定程度低减轻数据库服务器的访问压力，但是需要特别注意主从数 据一致性的问题。如果我们在 master 写入了，马上到 slave 查询，而这个时候 slave 的数据还没有同步过来，怎么办?所以，基于主从复制的原理，我们需要弄明白，主从复制到底慢在哪里?在早期的 MySQL 中，slave 的 SQL 线程是单线程。master 可以支持 SQL 语句的并行执行，配置了多少的最大连接数就是最多同时多少个 SQL 并行执行。而 slave 的 SQL 却只能单线程排队执行，在主库并发量很大的情况下，同步数据肯定会出现延迟。为什么从库上的 SQL Thread 不能并行执行呢?举个例子，主库执行了多条 SQL 语句，首先用户发表了一条评论，然后修改了内容，最后把这条评论删除了。这三条语句在从库上的执行顺序肯定是不能颠倒的。 123insert into user_comments (10000009,&apos;nice&apos;);update user_comments set content =&apos;very good&apos; where id =10000009; delete from user_comments where id =10000009; 异步与全同步首先我们需要知道，在主从复制的过程中，MySQL 默认是异步复制的。也就是说，对于主节点来说，写入 binlog，事务结束就返回给客户端了。对于 slave 来说，接收到 binlog 就完事儿了，master 不关心 slave 的数据有没有写入成功。如果要减少延迟，是不是可以等待全部从库的事务执行完毕，才返回给客户端呢? 这样的方式叫做全同步复制。从库写完数据，主库才返会给客户端。这种方式虽然可以保证在读之前数据已经同步成功了，但是带来的副作用大家应该能想到，事务执行的时间会变长，它会导致 master 节点性能下降。 半同步复制介于异步复制和全同步复制之间，还有一种半同步复制的方式。主库在执行完客户端提交的事务后不是立刻返回给客户端，而是等待至少一个从库接收到 binlog 并写到 relay log 中才返回给客户端。master 不会等待很长的时间，但是返回给客户端的时候，数据就即将写入成功了，因为它只剩最后一步了:就是读取 relay log，写入从库。如果我们要在数据库里面用半同步复制，必须安装一个插件，这个是谷歌的一位工程师贡献的。这个插件在 mysql 的插件目录下已经有提供: 1cd /usr/lib64/mysql/plugin/ 主库和从库是不同的插件，安装之后需要启用: 12345678-- 主库执行INSTALL PLUGIN rpl_semi_sync_master SONAME &apos;semisync_master.so&apos;;set global rpl_semi_sync_master_enabled=1; show variables like &apos;%semi_sync%&apos;;-- 从库执行INSTALL PLUGIN rpl_semi_sync_slave SONAME &apos;semisync_slave.so&apos;; set global rpl_semi_sync_slave_enabled=1;show global variables like &apos;%semi%&apos;; 相对于异步复制，半同步复制提高了数据的安全性，同时它也造成了一定程度的延迟，它需要等待一个 slave 写入中继日志，这里多了一个网络交互的过程，所以，半同步复制最好在低延时的网络中使用。 多库并行复制怎么实现并行复制呢?设想一下，如果 3 条语句是在三个数据库执行，操作各自的数据库，是不是肯定不会产生并发的问题呢?执行的顺序也没有要求。所以如果是操作三个数据库，这三个数据库的从库的 SQL 线程可以并发执行。这是 MySQL 5.6 版本里面支持的多库并行复制。在大部分的情况下，我们都是单库多表的情况。在一个数据库里面怎么实现并行复制呢?我们知道数据库本身就是支持多个事务同时操作的，为什么这些事务在主库上面可以并行执行，却不会出现问题呢?因为他们本身就是互相不干扰的，比如这些事务是操作不同的表，或者操作不同的 行，不存在资源的竞争和数据的干扰。那在主库上并行执行的事务，在从库上肯定也是 可以并行执行，是不是?比如在 master 上有三个事务同时分别操作三张表，这三个事务 是不是在 slave 上面也可以并行执行呢? 异步复制之 GTID 复制所以，我们可以把那些在主库上并行执行的事务，分为一个组，并且给他们编号，这一个组的事务在从库上面也可以并行执行。这个编号我们把它叫做 GTID(Global Transaction Identifiers)，这种主从复制的方式，我们把它叫做基于 GTID 的复制。如果我们要使用 GTID 复制，我们可以通过修改配置参数打开它，默认是关闭的: 1show global variables like &apos;gtid_mode&apos;; 无论是优化 master 和 slave 的连接方式，还是让从库可以并行执行 SQL，都是从数据库的层面去解决主从复制延迟的问题。 分库分表我们在做了主从复制之后，如果单个 master 节点或者单张表存储的数据过大的时候，比如一张表有上亿的数据，单表的查询性能还是会下降，我们要进一步对单台数据库节点的数据分型拆分，这个就是分库分表。垂直分库，减少并发压力。水平分表，解决存储瓶颈。垂直分库的做法，把一个数据库按照业务拆分成不同的数据库:水平分库分表的做法，把单张表的数据按照一定的规则分布到多个数据库。通过主从或者分库分表可以减少单个数据库节点的访问压力和存储压力，达到提升数据库性能的目的，但是如果 master 节点挂了，怎么办?所以，高可用(High Available)也是高性能的基础。 高可用方案主从复制传统的 HAProxy + keepalived 的方案，基于主从复制。 NDB Cluster基于 NDB 集群存储引擎的 MySQL Cluster。(https://dev.mysql.com/doc/mysql-cluster-excerpt/5.7/en/mysql-cluster-overview.html) Galera一种多主同步复制的集群方案。(https://galeracluster.com/) MHA/MMMMMM(Master-Master replication manager for MySQL)，一种多主的高可用 架构，是一个日本人开发的，像美团这样的公司早期也有大量使用 MMM。(https://tech.meituan.com/2017/06/29/database-availability-architecture.html) MMM 和 MHA(MySQL Master High Available)都是对外提供一个虚拟 IP，并且监控主节点和从节点，当主节点发生故障的时候，需要把一个从节点提升为主节点，并且把从节点里面比主节点缺少的数据补上，把 VIP 指向新的主节点。 MGRMySQL 5.7.17 版本推出的 InnoDB Cluster，也叫 MySQL Group Replicatioin (MGR)，这个套件里面包括了 mysql shell 和 mysql-route。(https://dev.mysql.com/doc/refman/5.7/en/group-replication.htmlhttps://dev.mysql.com/doc/refman/5.7/en/mysql-cluster.html) 总结高可用 HA 方案需要解决的问题都是当一个 master 节点宕机的时候，如何提升一个数据最新的 slave 成为 master。如果同时运行多个 master，又必须要解决 master 之间数据复制，以及对于客户端来说连接路由的问题。 优化器——SQL 语句分析与优化优化器就是对我们的 SQL 语句进行分析，生成执行计划。第一步，我们要把 SQL 执行情况记录下来。 慢查询日志 slow query log因为开启慢查询日志是有代价的(跟 bin log、optimizer-trace 一样)，所以它默认是关闭的: 1show variables like &apos;slow_query%&apos;; 除了这个开关，还有一个参数控制执行超过多长时间的 SQL 才记录到慢日志，默认是 10 秒。 1show variables like &apos;%slow_query%&apos;; 可以直接动态修改参数(重启后失效)。 1234set @@global.slow_query_log=1; -- 1 开启，0 关闭，重启后失效set @@global.long_query_time=3; -- mysql 默认的慢查询时间是 10 秒，另开一个窗口后才会查到最新值 ​show variables like &apos;%long_query%&apos;;show variables like &apos;%slow_query%&apos;; 或者修改配置文件 my.cnf。以下配置定义了慢查询日志的开关、慢查询的时间、日志文件的存放路径。 123slow_query_log = ONlong_query_time=2slow_query_log_file =/var/lib/mysql/localhost-slow.log 慢日志分析123show global status like &apos;slow_queries&apos;; -- 查看有多少慢查询 show variables like &apos;%slow_query%&apos;; -- 获取慢日志目录cat /var/lib/mysql/ localhost-slow.log 有了慢查询日志，怎么去分析统计呢?比如 SQL 语句的出现的慢查询次数最多，平均每次执行了多久?MySQL 提供了 mysqldumpslow 的工具，在 MySQL 的 bin 目录下。 1mysqldumpslow --help 查询用时最多的 20 条慢 SQL: 1mysqldumpslow -s t -t 20 -g &apos;select&apos; /var/lib/mysql/localhost-slow.log SHOW PROFILESHOW PROFILE 是谷歌高级架构师 Jeremy Cole 贡献给 MySQL 社区的，可以查看 SQL 语句执行的时候使用的资源，比如 CPU、IO 的消耗情况。在 SQL 中输入 help profile 可以得到详细的帮助信息。查看是否开启 12select @@profiling; set @@profiling=1; 查看 profile 统计 1show profiles; 其他系统命令1show processlist; 这是很重要的一个命令，用于显示用户运行线程。可以根据 id 号 kill 线程。 列 含义 Id 线程的唯一标志，可以根据它 kill 线程 User 启动这个线程的用户，普通用户只能看到自己的线程 Host 哪个 IP 端口发起的连接 db 操作的数据库 Command 线程的命令(https://dev.mysql.com/doc/refman/5.7/en/thread-commands.html) Time 操作持续时间，单位秒 State 线程状态，比如查询可能有 copying to tmp table，Sorting result，Sending data https://dev.mysql.com/doc/refman/5.7/en/general-thread-states.html Info SQL 语句的前 100 个字符，如果要查看完整的 SQL 语句，用 SHOW FULL PROCESSLIST 1show status SHOW STATUS 用于查看 MySQL 服务器运行状态(重启后会清空)，有 session 和 global 两种作用域，格式:参数-值。可以用 like 带通配符过滤。 1SHOW GLOBAL STATUS LIKE &apos;com_select&apos;; -- 查看 select 次数 1show engine show engine 用来显示存储引擎的当前运行信息，包括事务持有的表锁、行锁信息;事务的锁等待情况;线程信号量等待;文件 IO 请求;buffer pool 统计信息。 1show engine innodb status; 如果需要将监控信息输出到错误信息 error log 中(15 秒钟一次)，可以开启输出。 1234show variables like &apos;innodb_status_output%&apos;; -- 开启输出:SET GLOBAL innodb_status_output=ON;SET GLOBAL innodb_status_output_locks=ON; EXPLAIN 执行计划id 值根据 id 不同的先大后小，id 相同的从上往下。例如子查询只有拿到内层的结果之后才能进行外层的查询，所有优先执行子查询。当 id 值相同时，表的查询顺序是从上往下顺序执行。因为 MySQL 要把查询的结果包括中间结果和最终结果都保存到内存，所以 MySQL 会优先选择中间结果数据量比较小的顺序进行查询。 select type 查询类型 SIMPLE–简单查询，不包含子查询，不包含关联查询 union。 PRIMARY–根据主键值进行查询 SUBQUERY–子查询中所有的内层查询都是 SUBQUERY 类型的。 DERIVED–衍生查询，表示在得到最终查询结果之前会用到临时表。 UNION–用到了 UNION 查询。 UNION RESULT–主要是显示哪些表之间存在 UNION 查询。&lt;union2,3&gt;代表 id=2 和 id=3 的查询 存在 UNION。 type 连接类型在常用的链接类型中:system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; all（其 他 : fulltext 、 ref_or_null 、 index_merger 、unique_subquery、index_subquery）以上访问类型除了 all，都能用到索引。 const：主键索引或者唯一索引，只能查到一条数据的 SQL。 system ： const 的一种特例，只有一行满足条件（例如:只有一条数据的系统表）。 eq_ref：通常出现在多表的 join 查询，表示对于前表的每一个结果，都只能匹配到后表的一行结果。一般是唯一性索引的查询(UNIQUE 或 PRIMARY KEY)。eq_ref 是除 const 之外最好的访问类型。 ref：查询用到了非唯一性索引或者关联操作使用了索引的最左前缀。 range：索引范围扫描，例如 betweenand 或 &lt;或 &gt; 或 &gt;= 或 &lt;= 或 in 这些。 index：Full Index Scan，查询全部索引中的数据。 all：Full Table Scan，如果没有索引或者没有用到索引就代表全表扫描。 NULL：不用访问表或者索引就能得到结果。 一般来说，需要保证查询至少达到 range 级别，最好能达到 ref。all(全表扫描)和 index(查询全部索引)都是需要优化的。 possible_key、key可能用到的索引和实际用到的索引。如果是 NULL 就代表没有用到索引。possible_key 可以有一个或者多个，可能用到索引不代表一定用到索引。另外 possible_key 为空，key有可能有值(比如：覆盖索引的情况)。 key_len索引的长度(使用的字节数)。跟索引字段的类型、长度有关。 rowsMySQL 认为扫描多少行才能返回请求的数据，是一个预估值。一般来说行数越少越好。 filtered这个字段表示存储引擎返回的数据在 server 层过滤后，剩下多少满足查询的记录数量的比例，它是一个百分比。 ref使用哪个列或者常数和索引一起从表中筛选数据。 Extra执行计划给出的额外的信息说明。 using index：用到了覆盖索引，不需要回表。 using where：使用了 where 过滤，表示存储引擎返回的记录并不是所有的都满足查询条件，需要在 server 层进行过滤(跟是否使用索引没有关系)。 Using index condition：索引条件下推。 using filesort：没有使用索引字段来排序，用到了额外的字段进行排序(跟磁盘或文件没有关系)。 using temporary：用到了临时表，例如 distinct 非索引列、group by 非索引列、使用 join 的时候，group 任意列。 总结模拟优化器执行 SQL 查询语句的过程，来知道 MySQL 是怎么处理一条 SQL 语句的。 通过这种方式我们可以分析语句或者表的性能瓶颈。分析出问题之后，就是对 SQL 语句的具体优化。 SQL与索引优化当我们的 SQL 语句比较复杂，有多个关联和子查询的时候，就要分析 SQL 语句有没有改写的方法。 12345-- 大偏移量的 limitselect * from user_innodb limit 900000,10;-- 改成先过滤 ID，再 limitSELECT * FROM user_innodb WHERE id &gt;= 900000 LIMIT 10; 对于具体的 SQL 语句的优化，MySQL 官网也提供了很多建议，这个是我们在分析 具体的 SQL 语句的时候需要注意的。 存储引擎存储引擎的选择为不同的业务表选择不同的存储引擎，例如:查询插入操作多的业务表，用 MyISAM。临时数据用 Memeroy。常规的并发大更新多的表用 InnoDB。 分区或者分表分区不推荐，一般根据业务划分使用分表就好：交易历史表:在年底为下一年度建立 12 个分区，每个月一个分区。渠道交易表:分成当日表;当月表;历史表，历史表再做分区。 字段定义原则:使用可以正确存储数据的最小数据类型。为每一列选择合适的字段类型，比如 INT 有 8 种类型，不同的类型的最大存储范围是不一样的所以要选择合适长度的类型。 字符类型变长情况下，varchar 更节省空间，但是对于 varchar 字段，需要一个字节来记录长度。固定长度的用 char，不要用 varchar。 非空非空字段尽量定义成 NOT NULL，提供默认值，或者使用特殊值、空串代替 null。NULL 类型的存储、优化、使用都会存在问题。 大文件存储不要用数据库存储图片(比如 base64 编码)或者大文件;把文件放在 OSS 上，数据库只需要存储 URI(相对路径)。 表拆分将不常用的字段拆分出去，避免列数过多和数据量过大。比如在业务系统中，要记录所有接收和发送的消息，这个消息是 XML 格式的，用 blob 或者 text 存储，用来追踪和判断重复，可以建立一张表专门用来存储报文。 优化体系除了对于代码、SQL 语句、表定义、架构、配置优化之外，业务层面的优化也不能忽视。举几个例子: 在某一年的双十一，为什么会做一个充值到余额宝和余额有奖金的活动(充 300 送 50)?因为使用余额或者余额宝付款是记录本地或者内部数据库，而使用银行卡付款，需 要调用接口，操作内部数据库肯定更快。 在去年的双十一，为什么在凌晨禁止查询今天之外的账单? 这是一种降级措施，用来保证当前最核心的业务。 最近几年的双十一，为什么提前一个多星期就已经有双十一当天的价格了? 预售分流。 在应用层面同样有很多其他的方案来优化，达到尽量减轻数据库的压力的目的，比如限流，或者引入 MQ 削峰等等。为什么同样用 MySQL，有的公司可以扛住百万千万级别的并发，而有的公司几百个并发都扛不住，关键在于怎么用。所以用数据库慢，不代表数据库本身慢，有的时候 还要往上层去优化。如果关系型数据库解决不了的问题，我们可能需要用到搜索引擎或者大数据的方案了，并不是所有的数据都要放到关系型数据库存储。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入MySQL-事务与锁]]></title>
    <url>%2F2020%2F01%2F13%2F%E6%B7%B1%E5%85%A5MySQL-%E4%BA%8B%E5%8A%A1%E4%B8%8E%E9%94%81%2F</url>
    <content type="text"><![CDATA[什么是数据库事务？事务的典型场景在项目里面，什么地方会开启事务，或者配置了事务?无论是在方法上加注解，还是配置切面。比如下单，会操作订单表，资金表，物流表等等，这个时候我们需要让这些操作都在一个事务里面完成。当一个业务流程涉及多个表的操作的时候，我们希望它们要么是全部成功的，要么都不成功，这个时候我们会启用事务。事务就是解决一批次操作要么全部成功，要么全部失败。 事务的定义维基百科的定义:事务是数据库管理系统(DBMS)执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。第一个，它是数据库最小的工作单元，是不可以再分的。第二个，它可能包含了一个或者一系列的 DML 语句，包括 insert delete update。(单条 DDL(create drop)和 DCL(grant revoke)也会有事务) 哪些存储引擎支持事务 InnoDB支持事务，也是它成为默认的存储引擎的一个重要原因。 NDB 事务的四大特性事务的四大特性:ACID。 原子性（Atomicity）也就是我们刚才说的不可再分，也就意味着我们对数据库的一系列的操作，要么都是成功，要么都是失败，不可能出现部分成功或者部分失败的情况。以转账的场景为例，一个账户的余额减少，对应一个账户的增加，这两个一定是同时成功或者同时失败的。原子性，在 InnoDB 里面是通过 undo log 来实现的，它记录了数据修改之前的值(逻辑日志)，一旦发生异常，就可以用 undo log 来实现回滚操作。 一致性（consistent）指的是数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。比如主键必须是唯一的，字段长度符合要求。除了数据库自身的完整性约束，还有一个是用户自定义的完整性（用户自定义的完整性通常要在代码中控制）。 隔离性（Isolation）我们有了事务的定义以后，在数据库里面会有很多的事务同时去操作我们的同一张表或者同一行数据，必然会产生一些并发或者干扰的操作，那么我们对隔离性的定义，就是这些很多个的事务，对表或者行的并发操作，应该是透明的，互相不干扰的。通过这种方式，我们最终也是保证业务数据的一致性。 持久性（Durable）事务的持久性是什么意思呢?我们对数据库的任意的操作，增删改，只要事务提交成功，那么结果就是永久性的，不可能因为我们系统宕机或者重启了数据库的服务器，它又恢复到原来的状态了。这个就是事务的持久性。持久性是通过 redo log 和 double write 双写缓冲来实现的，我们操作数据的时候，会先写到内存的 buffer pool 里面，同时记录 redo log，如果在刷盘之前出现异常，在重启后就可以读取 redo log 的内容，写入到磁盘，保证数据的持久性。当然，恢复成功的前提是数据页本身没有被破坏，是完整的，这个通过双写缓冲 (double write)保证。 原子性，隔离性，持久性，最后都是为了实现一致性。 数据库什么时候会出现事务无论是我们在 Navicat 的这种工具里面去操作，还是在我们的 Java 代码里面通过 API 去操作，还是加上@Transactional 的注解或者 AOP 配置，其实最终都是发送一个指令到数据库去执行，Java 的 JDBC 只不过是把这些命令封装起来了。我们先来看一下我们的操作环境。版本(5.7)，存储引擎(InnnoDB)，事务隔离 级别(RR)。 123select version();show variables like &apos;%engine%&apos;;show global variables like &quot;tx_isolation&quot;; 执行这样一条更新语句的时候，它有事务吗? 1update student set sname = &apos;猫老公 111&apos; where id=1; 实际上，它自动开启了一个事务，并且提交了，所以最终写入了磁盘。这个是开启事务的第一种方式，自动开启和自动提交。InnoDB 里面有一个 autocommit 的参数(分成两个级别， session 级别和 global级别)。 1show variables like &apos;autocommit&apos;; 它的默认值是 ON。autocommit 这个参数是什么意思呢?是否自动提交。如果它的值是 true/on 的话，我们在操作数据的时候，会自动开启一个事务和自动提交事务。否则，如果我们把 autocommit 设置成 false/off，那么数据库的事务就需要我们手动地去开启和手动地去结束。手动开启事务也有几种方式，一种是用 begin;一种是用 start transaction。我们结束也有两种方式，第一种就是提交一个事务， commit;还有一种就是 rollback，回滚的时候，事务也会结束。还有一种情况，客户端的连接断开的时候，事务也会结束。 事务并发会带来什么问题? 脏读：即为事务1第二次读取时，读到了事务2未提交的数据。若事务2回滚，则事务1第二次读取时，读到了脏数据。 不可重复读：与脏读逻辑类似。主要在于事务2在事务1第二次读取时，提交了数据。导致事务1前后两次读取的数据不一致。 事务1第二次查询时，读到了事务2提交的插入数据。 不可重复读是修改或者删除，幻读是插入。无论是脏读，还是不可重复读，还是幻读，它们都是数据库的读一致性的问题，都是在一个事务里面前后两次读取出现了不一致的情况。 隔离级别 Read Uncommitted(未提交读)，一个事务可以读取到其他事务未提交的数据会出现脏读，所以叫做 RU，它没有解决任何的问题。 Read Committed(已提交读)，也就是一个事务只能读取到其他事务已提交的数据，不能读取到其他事务未提交的数据，它解决了脏读的问题，但是会出现不可重复读的问题。 Repeatable Read (可重复读)，它解决了不可重复读的问题，也就是在同一个事务里面多次读取同样的数据结果是一样的，但是在这个级别下，没有 定义解决幻读的问题。 Serializable(串行化)，在这个隔离级别里面，所有的事务都是串行执行的，也就是对数据的操作需要排队，已经不存在事务的并发操作了，所以它解决了所有的问题。 这个是 SQL92 的标准，但是不同的数据库厂商或者存储引擎的实现有一定的差异，比如 Oracle 里面就只有两种 RC(已提交读)和 Serializable(串行化)。那么 InnoDB 的实现又是怎么样的呢? InnoDB 对隔离级别的支持在 MySQL InnoDB 里面，不需要使用串行化的隔离级别去解决所有问题。InnoDB 支持的四个隔离级别和 SQL92 定义的基本一致，隔离级别越高，事务的并发度就越低。唯一的区别就在于，InnoDB 在 RR 的级别就解决了幻读的问题。这个也是 InnoDB 默认使用 RR 作为事务隔离级别的原因，既保证了数据的一致性，又支持较高的并发度。 两大实现方案那么大家想一下，如果要解决读一致性的问题，保证一个事务中前后两次读取数据结果一致，实现事务隔离，应该怎么做?我们有哪一些方法呢?你的思路是什么样的呢? LBCC第一种，我既然要保证前后两次读取数据一致，那么我读取数据的时候，锁定我要操作的数据，不允许其他的事务修改就行了。这种方案我们叫做基于锁的并发控制 Lock Based Concurrency Control(LBCC)。如果仅仅是基于锁来实现事务隔离，一个事务读取的时候不允许其他时候修改，那就意味着不支持并发的读写操作，而我们的大多数应用都是读多写少的，这样会极大地影响操作数据的效率。 MVCC所以我们还有另一种解决方案，如果要让一个事务前后两次读取的数据保持一致，那么我们可以在修改数据的时候给它建立一个备份或者叫快照，后面再来读取这个快照 就行了。这种方案我们叫做多版本的并发控制 Multi Version Concurrency Control (MVCC)。MVCC 的核心思想是: 我可以查到在我这个事务开始之前已经存在的数据，即使它在后面被修改或者删除了。在我这个事务之后新增的数据，我是查不到的。问题:这个快照什么时候创建?读取数据的时候，怎么保证能读取到这个快照而不是最新的数据?这个怎么实现呢?InnoDB 为每行记录都实现了两个隐藏字段: DB_TRX_ID，6 字节:插入或更新行的最后一个事务的事务 ID，事务编号是自动递增的(我们把它理解为创建版本号，在数据新增或者修改为新数据的时候，记录当前事务 ID)。 DB_ROLL_PTR，7 字节:回滚指针(我们把它理解为删除版本号，数据被删除或记录为旧数据的时候，记录当前事务 ID)。 MVCC 的查找规则:只能查找创建时间小于等于当前事务 ID 的数据和删除时间大于当前事务 ID 的行(或未删除)，也就是不能查到在我的事务开始之后做的更新、删除、插入操作的数据。 我们把这两个事务 ID 理解为版本号。在 InnoDB 中，MVCC 是通过 Undo log 实现的。Oracle、Postgres 等等其他数据库都有 MVCC 的实现。需要注意，在 InnoDB 中，MVCC 和锁是协同使用的，这两种方案并不是互斥的。 InnoDB 锁的基本类型官网把锁分成了 8 类。所以我们把前面的两个行级别的锁(Shared and Exclusive Locks)和两个表级别的锁(Intention Locks)称为锁的基本模式。后面三个 Record Locks、Gap Locks、Next-Key Locks，我们把它们叫做锁的算法，也就是分别在什么情况下锁定什么范围。 锁的粒度表锁，顾名思义，是锁住一张表;行锁就是锁住表里面的一行数据。 锁定粒度，表锁肯定是大于行锁的。 加锁效率，表锁肯定是大于行锁的。 冲突概率，表锁肯定是大于行锁的。 并发效率，表锁肯定是小于行锁的。 共享锁第一个行级别的锁就是我们在官网看到的 Shared Locks (共享锁)，我们获取了一行数据的读锁以后，可以用来读取数据，所以它也叫做读锁。注意不要在加上了读锁 以后去写数据，不然的话可能会出现死锁的情况。而且多个事务可以共享一把读锁。那怎么给一行数据加上读锁呢?我们可以用 select …… lock in share mode; 的方式手工加上一把读锁。释放锁有两种方式，只要事务结束，锁就会自动释放，包括提交事务和结束事务。 排它锁第二个行级别的锁叫做 Exclusive Locks(排它锁)，它是用来操作数据的，所以又叫做写锁。只要一个事务获取了一行数据的排它锁，其他的事务就不能再获取这一行数据的共享锁和排它锁。排它锁的加锁方式有两种 自动加排他锁。我们在操作数据的时候，包括增删改，都会默认加上一个排它锁。 手工加锁，我们用一个 FOR UPDATE 给一行数据加上一个排它锁，这个无论是在我们的代码里面还是操作数据的工具里面，都比较常用。 释放锁的方式跟前面是一样的。 意向锁意向锁是由数据库自己维护的，当我们给一行数据加上共享锁之前，数据库会自动在这张表上面加一个意向共享锁。当我们给一行数据加上排他锁之前，数据库会自动在这张表上面加一个意向排他锁。反过来说： 如果一张表上面至少有一个意向共享锁，说明有其他的事务给其中的某些数据行加上了共享锁。 如果一张表上面至少有一个意向排他锁，说明有其他的事务给其中的某些数据行加上了排他锁。 那么这两个表级别的锁存在的意义是什么呢?第一个，我们有了表级别的锁，在 InnoDB 里面就可以支持更多粒度的锁。它的第二个作用，如果说没有意向锁的话，当我们准备给一张表加上表锁的时候，我们首先要做什么?是不是必须先要去判断有没其他的事务锁定了其中了某些行?如果有的话，肯定不能加上表锁。那么这个时候我们就要去扫描整张表才能确定能不能成功加上一个表锁，如果数据量特别大，比如有上千万的数据的时候，加表锁的效率是不是很低?但是我们引入了意向锁之后就不一样了。我只要判断这张表上面有没有意向锁，如果有，就直接返回失败。如果没有，就可以加锁成功。所以 InnoDB 里面的表锁，我们 可以把它理解成一个标志。就像火车上厕所有没有人使用的灯，是用来提高加锁的效率的。 总结锁的作用是什么?它跟 Java 里面的锁是一样的， 是为了解决资源竞争的问题，Java 里面的资源是对象，数据库的资源就是数据表或者数据行。所以锁是用来解决事务对数据的并发访问的问题的。 行锁的原理InnoDB 的行锁，就是通过锁住索引来实现的。 为什么锁表？为什么表里面没有索引的时候，锁住一行数据会导致锁表？或者说，如果锁住的是索引，一张表没有索引怎么办？ 如果我们定义了主键(PRIMARY KEY)，那么 InnoDB 会选择主键作为聚集索引。 如果没有显式定义主键，则 InnoDB 会选择第一个不包含有 NULL 值的唯一索引作为主键索引。 如果也没有这样的唯一索引，则 InnoDB 会选择内置 6 字节长的 ROWID 作为隐藏的聚集索引，它会随着行记录的写入而主键递增。 为什么锁表，是因为查询没有使用索引，会进行全表扫描，然后把每一个隐藏的聚集索引都锁住了。 为什么整行数据被锁住?为什么通过唯一索引给数据行加锁，主键索引也会被锁住?大家还记得在 InnoDB 里面，当我们使用辅助索引的时候，它是怎么检索数据的吗? 辅助索引的叶子节点存储的是什么内容?在辅助索引里面，索引存储的是二级索引和主键的值。比如 name=4，存储的是 name 的索引和主键 id 的值 4。而主键索引里面除了索引之外，还存储了完整的数据。所以我们通过辅助索引锁定一行数据的时候，它跟我们检索数据的步骤是一样的，会通过主键值找到主键索引，然后也锁定。 锁的算法在官网上，还有 3 种锁，我 们把它理解为锁的算法。我们也来看下 InnoDB 在什么时候分别锁住什么范围。我们先来看一下我们测试用的表，t2，这张表有一个主键索引。我们插入了 4 行数据，主键值分别是 1、4、7、10。为了让大家真正理解这三种行锁算法的区别，我们需要了解一下三种范围的概念。因为我们用主键索引加锁，我们这里的划分标准就是主键索引的值。这些数据库里面存在的主键值，我们把它叫做 Record，记录，那么这里我们就有 4 个 Record。根据主键，这些存在的 Record 隔开的数据不存在的区间，我们把它叫做 Gap(间隙)，它是一个左开右开的区间。间隙(Gap)连同它左边的记录(Record)，我们把它叫做临键的区间，它是一个左开右闭的区间。t2 的主键索引，它是整型的可以排序，所以才有这种区间。如果我的主键索引不是整型，是字符怎么办呢?字符可以排序吗? 用 ASCII 码来排序。 记录锁当我们对于唯一性的索引(包括唯一索引和主键索引)使用等值查询，精准匹配到一条记录的时候，这个时候使用的就是记录锁。比如whereid=1 4 7 10 。这个演示我们在前面已经看过了。我们使用不同的 key 去加锁，不会冲突，它只锁住这个 record。 间隙锁第二种情况，当我们查询的记录不存在，没有命中任何一个 record，无论是用等值查询还是范围查询的时候，它使用的都是间隙锁。比如：where id &gt;4 and id &lt;7，where id = 6。 Transaction 1 Transaction 2 begin; INSERT INTO t2 (id, name) VALUES (5, ‘5’); // BLOCKED INSERT INTO t2 (id, name) VALUES (6, ‘6’); // BLOCKED select * from t2 where id =6 for update; // OK select * from t2 where id &gt;20 for update; INSERT INTO t2 (id, name) VALUES (11, ‘11’); // BLOCKED 注意，间隙锁主要是阻塞插入 insert。相同的间隙锁之间不冲突。Gap Lock 只在 RR 中存在。如果要关闭间隙锁，就是把事务隔离级别设置成 RC，并且把innodb_locks_unsafe_for_binlog 设置为 ON。这种情况下除了外键约束和唯一性检查会加间隙锁，其他情况都不会用间隙锁。 临键锁当我们使用了范围查询，不仅仅命中了 Record 记录，还包含了 Gap 间隙，在这种情况下我们使用的就是临键锁，它是 MySQL 里面默认的行锁算法，相当于记录锁加上间隙锁。比如我们使用&gt;5 &lt;9， 它包含了记录不存在的区间，也包含了一个 Record 7。临键锁，锁住最后一个 key 的下一个左开右闭的区间。 12select * from t2 where id &gt;5 and id &lt;=7 for update; -- 锁住(4,7]和(7,10] select * from t2 where id &gt;8 and id &lt;=10 for update; -- 锁住 (7,10]，(10,+∞) 隔离级别的实现为什么 InnoDB 的 RR 级别能够解决幻读的问题，就是用临键锁实现的。 RU 隔离级别:不加锁。 Serializable 所有的 select 语句都会被隐式的转化为 select … in share mode，会和 update、delete 互斥。 RR 隔离级别下，普通的 select 使用快照读(snapshot read)，底层使用 MVCC 来实现。加锁的 select(select … in share mode / select … for update)以及更新操作 update, delete 等语句使用当前读(current read)，底层使用记录锁、或者间隙锁、 临键锁。 RC 隔离级别下，普通的 select 都是快照读，使用 MVCC 实现。加锁的 select 都使用记录锁，因为没有 Gap Lock 。除了两种特殊情况——外键约束检查(foreign-key constraint checking)以及重复键检查(duplicate-key checking)时会使用间隙锁封锁区间。所以 RC 会出现幻读的问题。事务隔离级别怎么选?RU 和 Serializable 肯定不能用。为什么有些公司要用 RC，或者说网上有些文章推荐有 RC?RC 和 RR 主要有几个区别: RR 的间隙锁会导致锁定范围的扩大。 条件列未使用到索引，RR 锁表，RC 锁行。 RC 的“半一致性”(semi-consistent)读可以增加 update 操作的并发性。 在 RC 中，一个 update 语句，如果读到一行已经加锁的记录，此时 InnoDB 返回记录最近提交的版本，由 MySQL 上层判断此版本是否满足 update 的 where 条件。若满足(需要更新)，则 MySQL 会重新发起一次读操作，此时会读取行的最新版本(并加锁)。实际上，如果能够正确地使用锁(避免不使用索引去加锁)，只锁定需要的数据，用默认的 RR 级别就可以了。排它锁有互斥的特性。一个事务或者说一个线程持有锁的时候，会阻止其他的线程获取锁，这个时候会造成阻塞等待，如果循环等待，会有可能造成死锁。 死锁锁的释放与阻塞如果一个事务一直未释放锁，其他事务会被阻塞多久?会不会永远等待下去?如果是，在并发访问比较高的情况下，如果大量事务因无法立即获得所需的锁而挂起，会占用大量计算机资源，造成严重性能问题，甚至拖跨数据库。MySQL 有一个参数来控制获取锁的等待时间，默认是 50 秒。 1show VARIABLES like &apos;innodb_lock_wait_timeout&apos;; 对于死锁，是无论等多久都不能获取到锁的，这种情况，也需要等待 50 秒钟吗?那不是白白浪费了 50 秒钟的时间吗? 死锁的发生和检测 Session 1 Session 2 begin; select * from t2 where id =1 for update; begin; delete from t2 where id =4 ; update t2 set name= ‘4d’ where id =4 ; delete from t2 where id =1 ; 在第一个事务中，检测到了死锁，马上退出了，第二个事务获得了锁，不需要等待 50 秒。 [Err] 1213 - Deadlock found when trying to get lock; try restarting transaction 为什么可以直接检测到呢?是因为死锁的发生需要满足一定的条件，所以在发生死锁时，InnoDB 一般都能通过算法(wait-for graph)自动检测到。那么死锁需要满足什么条件?死锁的产生条件: 同一时刻只能有一个事务持有这把锁 其他的事务需要在这个事务释放锁之后才能获取锁，而不可以强行剥夺 当多个事务形成等待环路的时候，即发生死锁。 如果锁一直没有释放，就有可能造成大量阻塞或者发生死锁，造成系统吞吐量下降，这时候就要查看是哪些事务持有了锁。 查看锁信息SHOW STATUS 命令中，包括了一些行锁的信息: 1show status like &apos;innodb_row_lock_%&apos;; Innodb_row_lock_current_waits:当前正在等待锁定的数量; Innodb_row_lock_time :从系统启动到现在锁定的总时间长度，单位 ms; Innodb_row_lock_time_avg :每次等待所花平均时间; Innodb_row_lock_time_max:从系统启动到现在等待最长的一次所花的时间; Innodb_row_lock_waits :从系统启动到现在总共等待的次数。 SHOW 命令是一个概要信息。InnoDB 还提供了三张表来分析事务与锁的情况: 123select * from information_schema.INNODB_TRX; -- 当前运行的所有事务 ，还有具体的语句select * from information_schema.INNODB_LOCKS; -- 当前出现的锁select * from information_schema.INNODB_LOCK_WAITS; -- 锁等待的对应关系 找出持有锁的事务之后呢?如果一个事务长时间持有锁不释放，可以 kill 事务对应的线程 ID，也就是 INNODB_TRX 表中的 trx_mysql_thread_id，例如执行 kill 4，kill 7，kill 8。当然，死锁的问题不能每次都靠 kill 线程来解决，这是治标不治本的行为。我们应该尽量在应用端，也就是在编码的过程中避免。 死锁的避免 在程序中，操作多张表时，尽量以相同的顺序来访问(避免形成等待环路); 批量操作单张表数据的时候，先对数据进行排序(避免形成等待环路); 申请足够级别的锁，如果要操作数据，就申请排它锁; 尽量使用索引访问数据，避免没有 where 条件的操作，避免锁表; 如果可以，大事务化成小事务; 使用等值查询而不是范围查询查询数据，命中记录，避免间隙锁对并发的影响。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入MySQL-索引剖析]]></title>
    <url>%2F2020%2F01%2F11%2F%E6%B7%B1%E5%85%A5MySQL-%E7%B4%A2%E5%BC%95%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[索引索引是什么?索引图解首先数据是以文件的形式存放在磁盘上面的，每一行数据都有它的磁盘地址。如果没有索引的话，要从 500 万行数据里面检索一条数据，只能依次遍历这张表的全部数据，直到找到这条数据。但是有了索引之后，只需要在索引里面去检索这条数据就行了，因为它是一种特殊的专门用来快速检索的数据结构，我们找到数据存放的磁盘地址以后，就可以拿到数据 了。就像我们从一本 500 页的书里面去找特定的一小节的内容，肯定不可能从第一页开 始翻。那么这本书有专门的目录，它可能只有几页的内容，它是按页码来组织的，可以根据拼音或者偏旁部首来查找，只要确定内容对应的页码，就能很快地找到我们想要的内容。 索引类型第一个是索引的名称，第二个是索引的列，比如我们是要对 id 创建索引还是对 name 创建索引。后面两个很重要，一个叫索引类型。在 InnoDB 里面，索引类型有三种，普通索引、唯一索引(主键索引是特殊的唯一 索引)、全文索引。 普通(Normal):也叫非唯一索引，是最普通的索引，没有任何的限制。 唯一(Unique):唯一索引要求键值不能重复。另外需要注意的是，主键索引是一 种特殊的唯一索引，它还多了一个限制条件，要求键值不能为空。主键索引用 primay key 创建。 全文(Fulltext):针对比较大的数据，比如我们存放的是消息内容，有几 KB 的数 据的这种情况，如果要解决 like 查询效率低的问题，可以创建全文索引。只有文本类型的字段才可以创建全文索引，比如 char、varchar、text。 索引存储模型推演二分查找有序数组的等值查询和比较查询效率非常高，但是更新数据的时候会出现一个问题，可能要挪动大量的数据(改变 index)，所以只适合存储静态的数据。为了支持频繁的修改，比如插入数据，我们需要采用链表。链表的话，如果是单链表，它的查找效率还是不够高。所以，有没有可以使用二分查找的链表呢?为了解决这个问题，BST(Binary Search Tree)也就是我们所说的二叉查找树诞生了。 二叉查找树二叉查找树的特点是什么?左子树所有的节点都小于父节点，右子树所有的节点都大于父节点。投影到平面以后，就是一个有序的线性表。二叉查找树既能够实现快速查找，又能够实现快速插入。但是二叉查找树有一个问题: 就是它的查找耗时是和这棵树的深度相关的，在最坏的情况下时间复杂度会退化成O(n)。造成它倾斜的原因是什么呢?因为左右子树深度差太大，这棵树的左子树根本没有节点——也就是它不够平衡。所以，我们有没有左右子树深度相差不是那么大，更加平衡的树呢? 这个就是平衡二叉树，叫做 Balanced binary search trees，或者 AVL 树(AVL 是发明这个数据结构的人的名字)。 平衡二叉树平衡二叉树的定义:左右子树深度差绝对值不能超过 1。是什么意思呢?比如左子树的深度是 2，右子树的深度只能是 1 或者 3。这个时候我们再按顺序插入 1、2、3、4、5、6，一定是这样，不会变成一棵“斜树”。当我们插入了 1、2 之后，如果按照二叉查找树的定义，3 肯定是要在 2 的右边的，这个时候根节点 1 的右节点深度会变成 2，但是左节点的深度是 0，因为它没有子节点，所以就会违反平衡二叉树的定义。那应该怎么办呢?因为它是右节点下面接一个右节点，右-右型，所以这个时候我们要把 2 提上去，这个操作叫做左旋。同样的，如果我们插入 7、6、5，这个时候会变成左左型，就会发生右旋操作，把 6 提上去。所以为了保持平衡，AVL 树在插入和更新数据的时候执行了一系列的计算和调整的操作。平衡的问题我们解决了，那么平衡二叉树作为索引怎么查询数据?在平衡二叉树中，一个节点，它的大小是一个固定的单位，作为索引应该存储什么内容? 第一个是索引的键值。比如我们在 id 上面创建了一个索引，我在用 where id =1 的条件查询的时候就会找到索引里面的 id 的这个键值。 第二个是数据的磁盘地址，因为索引的作用就是去查找数据的存放的地址。 第三个，因为是二叉树，它必须还要有左子节点和右子节点的引用，这样我们才能找到下一个节点。比如大于 26 的时候，走右边，到下一个树的节点，继续判断。 如果是这样存储数据的话，我们来看一下会有什么问题。在分析用 AVL 树存储索引数据之前，我们先来学习一下 InnoDB 的逻辑存储结构。 InnoDB 逻辑存储结构MySQL 的存储结构分为 5 级:表空间、段、簇、页、行。 表空间 Table Space上节课讲磁盘结构的时候讲过了，表空间可以看做是 InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。分为:系统表空间、独占表空间、通用表空间、临时表空间、Undo 表空间。 段 Segment表空间是由各个段组成的，常见的段有数据段、索引段、回滚段等，段是一个逻辑的概念。一个 ibd 文件(独立表空间文件)里面会由很多个段组成。创建一个索引会创建两个段，一个是索引段:leaf node segment，一个是数据段: non-leaf node segment。索引段管理非叶子节点的数据。数据段管理叶子节点的数据。也就是说，一个表的段数，就是索引的个数乘以 2。 簇 Extent一个段(Segment)又由很多的簇(也可以叫区)组成，每个区的大小是 1MB(64个连续的页)。每一个段至少会有一个簇，一个段所管理的空间大小是无限的，可以一直扩展下去，但是扩展的最小单位就是簇。 页 Page为了高效管理物理空间，对簇进一步细分，就得到了页。簇是由连续的页(Page) 组成的空间，一个簇中有 64 个连续的页。(1MB/16KB=64)。这些页面在物理上和 逻辑上都是连续的。跟大多数数据库一样，InnoDB 也有页的概念(也可以称为块)，每个页默认 16KB。页是 InnoDB 存储引擎磁盘管理的最小单位，通过 innodb_page_size 设置。一个表空间最多拥有 2^32 个页，默认情况下一个页的大小为 16KB，也就是说一个表空间最多存储 64TB 的数据。注意，文件系统中，也有页的概念。操作系统和内存打交道，最小的单位是页 Page。文件系统的内存页通常是 4K。 1SHOW VARIABLES LIKE &apos;innodb_page_size&apos;; 假设一行数据大小是 1K，那么一个数据页可以放 16 行这样的数据。举例:一个页放 3 行数据。往表中插入数据时，如果一个页面已经写完，产生一个新的页面。如果一个簇的 所有的页面都被用完，会从当前页面所在段新分配一个簇。如果数据不是连续的，往已经写满的页中插入数据，会导致叶页面分裂: 行 RowInnoDB 存储引擎是面向行的(row-oriented)，也就是说数据的存放按行进行存放。 AVL 树用于存储索引数据首先，索引的数据，是放在硬盘上的。查看数据和索引的大小: 12345selectCONCAT(ROUND(SUM(DATA_LENGTH/1024/1024),2),&apos;MB&apos;) AS data_len,CONCAT(ROUND(SUM(INDEX_LENGTH/1024/1024),2),&apos;MB&apos;) as index_len from information_schema.TABLESwhere table_schema=&apos;test&apos; and table_name=&apos;user_innodb&apos;; 当我们用树的结构来存储索引的时候，访问一个节点就要跟磁盘之间发生一次 IO。InnoDB 操作磁盘的最小的单位是一页(或者叫一个磁盘块)，大小是 16K(16384 字节)。那么，一个树的节点就是 16K 的大小。如果我们一个节点只存一个键值+数据+引用，例如整形的字段，可能只用了十几个 或者几十个字节，它远远达不到 16K 的容量，所以访问一个树节点，进行一次 IO 的时候，浪费了大量的空间。所以如果每个节点存储的数据太少，从索引中找到我们需要的数据，就要访问更多的节点，意味着跟磁盘交互次数就会过多。如果是机械硬盘时代，每次从磁盘读取数据需要 10ms 左右的寻址时间，交互次数越多，消耗的时间就越多。比如上面这张图，我们一张表里面有 6 条数据，当我们查询 id=37 的时候，要查询两个子节点，就需要跟磁盘交互 3 次，如果我们有几百万的数据呢?这个时间更加难以估计。所以我们的解决方案是什么呢? 第一个就是让每个节点存储更多的数据。 第二个，节点上的关键字的数量越多，我们的指针数也越多，也就是意味着可以有更多的分叉(我们把它叫做“路数”)。 因为分叉数越多，树的深度就会减少(根节点是 0)。 这样，我们的树是不是从原来的高瘦高瘦的样子，变成了矮胖矮胖的样子? 这个时候，我们的树就不再是二叉了，而是多叉，或者叫做多路。 多路平衡查找树(B Tree)(分裂、合并)这个就是我们的多路平衡查找树，叫做 B Tree(B 代表平衡)。跟 AVL 树一样，B 树在枝节点和叶子节点存储键值、数据地址、节点引用。 它有一个特点:分叉数(路数)永远比关键字数多1。比如我们画的这棵树，每个节点存储两个关键字，那么就会有三个指针指向三个子节点。比如我们要在这张表里面查找 15。因为 15 小于 17，走左边。因为 15 大于 12，走右边。在磁盘块 7 里面就找到了 15，只用了 3 次 IO。那 B Tree 又是怎么实现一个节点存储多个关键字，还保持平衡的呢?跟 AVL 树有什么区别?比如 Max Degree(路数)是 3 的时候，我们插入数据 1、2、3，在插入 3 的时候， 本来应该在第一个磁盘块，但是如果一个节点有三个关键字的时候，意味着有 4 个指针， 子节点会变成 4 路，所以这个时候必须进行分裂。把中间的数据 2 提上去，把 1 和 3 变 成 2 的子节点。如果删除节点，会有相反的合并的操作。注意这里是分裂和合并，跟 AVL 树的左旋和右旋是不一样的。我们继续插入 4 和 5，B Tree 又会出现分裂和合并的操作。从这个里面我们也能看到，在更新索引的时候会有大量的索引的结构的调整，所以解释了为什么我们不要在频繁更新的列上建索引，或者为什么不要更新主键。节点的分裂和合并，其实就是 InnoDB 页的分裂和合并。 B+树(加强版多路平衡查找树)B Tree 的效率已经很高了，为什么 MySQL 还要对 B Tree 进行改良，最终使用了 B+Tree 呢?总体上来说，这个 B 树的改良版本解决的问题比 B Tree 更全面。 我们来看一下 InnoDB 里面的 B+树的存储结构:MySQL 中的 B+Tree 有几个特点: 它的关键字的数量是跟路数相等的; B+Tree 的根节点和枝节点中都不会存储数据，只有叶子节点才存储数据。搜索到关键字不会直接返回，会到最后一层的叶子节点。比如我们搜索 id=28，虽然在第一层直接命中了，但是全部的数据在叶子节点上面，所以我还要继续往下搜索，一直到叶子节点。举个例子:假设一条记录是 1K，一个叶子节点(一页)可以存储 16 条记录。非叶子节点可以存储多少个指针?假设索引字段是 bigint 类型，长度为 8 字节。指针大小在 InnoDB 源码中设置为 6 字节，这样一共 14 字节。非叶子节点(一页)可以存储 16384/14=1170 个这样的单元(键值+指针)，代表有 1170 个指针。树深度为 2 的时候，有 1170^2 个叶子节点，可以存储的数据为 1170117016=21902400。在查找数据时一次页的查找代表一次 IO，也就是说，一张 2000 万左右的表，查询数据最多需要访问 3 次磁盘。所以在 InnoDB 中 B+ 树深度一般为 1-3 层，它就能满足千万级的数据存储。 B+Tree 的每个叶子节点增加了一个指向相邻叶子节点的指针，它的最后一个数据会指向下一个叶子节点的第一个数据，形成了一个有序链表的结构。 它是根据左闭右开的区间 [ )来检索数据。 我们来看一下 B+Tree 的数据搜寻过程: 比如我们要查找 28，在根节点就找到了键值，但是因为它不是页子节点，所以会继续往下搜寻，28 是[28,66)的左闭右开的区间的临界值，所以会走中间的子节点，然后继续搜索，它又是[28,34)的左闭右开的区间的临界值，所以会走左边的子节点，最后在叶子节点上找到了需要的数据。 第二个，如果是范围查询，比如要查询从 22 到 60 的数据，当找到 22 之后，只需要顺着节点和指针顺序遍历就可以一次性访问到所有的数据节点，这样就极大地提高了区间查询效率(不需要返回上层父节点重复遍历查找)。 总结一下，InnoDB 中的 B+Tree 的特点: 它是 B Tree 的变种，B Tree 能解决的问题，它都能解决。B Tree 解决的两大问题是什么?(每个节点存储更多关键字;路数更多) 扫库、扫表能力更强(如果我们要对表进行全表扫描，只需要遍历叶子节点就可以了，不需要遍历整棵 B+Tree 拿到所有的数据) B+Tree 的磁盘读写能力相对于 B Tree 来说更强(根节点和枝节点不保存数据区，所以一个节点可以保存更多的关键字，一次磁盘加载的关键字更多) 排序能力更强(因为叶子节点上有下一个数据区的指针，数据形成了链表) 效率更加稳定(B+Tree 永远是在叶子节点拿到数据，所以 IO 次数是稳定的)为什么不用红黑树?红黑树也是 BST 树，但是不是严格平衡的。 必须满足 5 个约束: 节点分为红色或者黑色。 根节点必须是黑色的。 叶子节点都是黑色的 NULL 节点。 红色节点的两个子节点都是黑色(不允许两个相邻的红色节点)。 从任意节点出发，到其每个叶子节点的路径中包含相同数量的黑色节点。 基于以上规则，可以推导出:从根节点到叶子节点的最长路径(红黑相间的路径)不大于最短路径(全部是黑色节点)的 2 倍。为什么不用红黑树?1、只有两路;2、不够平衡。红黑树一般只放在内存里面用。例如 Java 的 TreeMap。 索引方式:真的是用的 B+Tree 吗?索引方式有两种，Hash 和 B Tree。HASH:以 KV 的形式检索数据，也就是说，它会根据索引字段生成哈希码和指针，指针指向数据。 它的时间复杂度是 O(1)，查询速度比较快。因为哈希索引里面的数据不是按顺序存储的，所以不能用于排序。 我们在查询数据的时候要根据键值计算哈希码，所以它只能支持等值查询 (= IN)，不支持范围查询(&gt; &lt; &gt;= &lt;= between and)。 如果字段重复值很多的时候，会出现大量的哈希冲突(采用拉链法解 决)，效率会降低。 InnoDB 内部使用哈希索引来实现自适应哈希索引特性。这句话的意思是 InnoDB 只支持显式创建 B+Tree 索引，对于一些热点数据页， InnoDB 会自动建立自适应 Hash 索引，也就是在 B+Tree 索引基础上建立 Hash 索引， 这个过程对于客户端是不可控制的，隐式的。buffer pool 里面有一块区域是 Adaptive Hash Index 自适应哈希 索引，就是这个。这个开关默认是 ON: 1show variables like &apos;innodb_adaptive_hash_index&apos;; 因为 B Tree 和 B+Tree 的特性，它们广泛地用在文件系统和数据库中，例如 Windows 的 HPFS 文件系统，Oracel、MySQL、SQLServer 数据库。 B+Tree 落地形式MySQL 架构MySQL 是一个支持插件式存储引擎的数据库。在 MySQL 里面，每个表在创建的时候都可以指定它所使用的存储引擎。这里我们主要关注一下最常用的两个存储引擎，MyISAM 和 InnoDB 的索引的实现。 MySQL 数据存储文件首先，MySQL 的数据都是文件的形式存放在磁盘中的，我们可以找到这个数据目录的地址。在 MySQL 中有这么一个参数，我们来看一下: 1show VARIABLES LIKE &apos;datadir&apos;; 每个数据库有一个目录，我们新建了一个叫做 test 的数据库，那么这里就有一个 test 的文件夹。这个数据库里面我们又建了 5 张表:archive、innodb、memory、myisam、csv。每张 InnoDB 的表有两个文件(.frm 和.ibd)，MyISAM 的表 有三个文件(.frm、.MYD、.MYI)。有一个是相同的文件，.frm。 .frm 是 MySQL 里面表结构定义的文件，不管你建表的时候选用任何一个存储引擎都会生成。 MyISAM在 MyISAM 里面，另外有两个文件:一个是.MYD 文件，D 代表 Data，是 MyISAM 的数据文件，存放数据记录，比如我们的 user_myisam 表的所有的表数据。一个是.MYI 文件，I 代表 Index，是 MyISAM 的索引文件，存放索引，比如我们在 id 字段上面创建了一个主键索引，那么主键索引就是在这个索引文件里面。也就是说，在 MyISAM 里面，索引和数据是两个独立的文件。那我们怎么根据索引找到数据呢?MyISAM 的 B+Tree 里面，叶子节点存储的是数据文件对应的磁盘地址。所以从索引文件.MYI 中找到键值后，会到数据文件.MYD 中获取相应的数据记录。这里是主键索引，如果是辅助索引，有什么不一样呢?在 MyISAM 里面，辅助索引也在这个.MYI 文件里面。辅助索引跟主键索引存储和检索数据的方式是没有任何区别的，一样是在索引文件里面找到磁盘地址，然后到数据文件里面获取数据。 InnoDBInnoDB 只有一个文件(.ibd 文件)，那索引放在哪里呢?在 InnoDB 里面，它是以主键为索引来组织数据的存储的，所以索引文件和数据文件是同一个文件，都在.ibd 文件里面。在 InnoDB 的主键索引的叶子节点上，它直接存储了我们的数据。什么叫做聚集索引(聚簇索引)?就是索引键值的逻辑顺序跟表数据行的物理存储顺序是一致的。(比如字典的目录 是按拼音排序的，内容也是按拼音排序的，按拼音排序的这种目录就叫聚集索引)。在 InnoDB 里面，它组织数据的方式叫做叫做(聚集)索引组织表(clustered index organize table)，所以主键索引是聚集索引，非主键都是非聚集索引。如果 InnoDB 里面主键是这样存储的，那主键之外的索引，比如我们在 name 字段 上面建的普通索引，又是怎么存储和检索数据的呢?InnoDB 中，主键索引和辅助索引是有一个主次之分的。辅助索引存储的是辅助索引和主键值。如果使用辅助索引查询，会根据主键值在主 键索引中查询，最终取得数据。为什么在辅助索引里面存储的是主键值而不是主键的磁盘地址呢?如果主键的数据类型比较大，是不是比存地址更消耗空间呢?我们前面说到 B Tree 是怎么实现一个节点存储多个关键字，还保持平衡的呢?是因为有分叉和合并的操作，这个时候键值的地址会发生变化，所以在辅助索引里面不能存储地址。如果一张表没有主键怎么办? 如果我们定义了主键(PRIMARY KEY)，那么 InnoDB 会选择主键作为聚集索引。 如果没有显式定义主键，则 InnoDB 会选择第一个不包含有 NULL 值的唯一索引作为主键索引。 如果也没有这样的唯一索引，则 InnoDB 会选择内置 6 字节长的 ROWID 作为隐藏的聚集索引，它会随着行记录的写入而主键递增。索引使用原则我们容易有以一个误区，就是在经常使用的查询条件上都建立索引，索引越多越好，那到底是不是这样呢?列的离散度第一个叫做列的离散度，我们先来看一下列的离散度的公式:count(distinct(column_name))/count(*)，列的全部不同值和所有数据行的比例。 数据行数相同的情况下，分子越大，列的离散度就越高。简单来说，如果列的重复值越多，离散度就越低，重复值越少，离散度就越高。了解了离散度的概念之后，我们再来思考一个问题，我们在 name 上面建立索引和 在 gender 上面建立索引有什么区别。当我们用在 gender 上建立的索引去检索数据的时候，由于重复值太多，需要扫描的 行数就更多。例如，我们现在在 gender 列上面创建一个索引，然后看一下执行计划。12ALTER TABLE user_innodb DROP INDEX idx_user_gender;ALTER TABLE user_innodb ADD INDEX idx_user_gender (gender); -- 耗时比较久 EXPLAIN SELECT * FROM `user_innodb` WHERE gender = 0; 而 name 的离散度更高，比如“测试”的这名字，只需要扫描一行。 1ALTER TABLE user_innodb DROP INDEX idx_user_name; ALTER TABLE user_innodb ADD INDEX idx_user_name (name); EXPLAIN SELECT * FROM `user_innodb` WHERE name = &apos;测试&apos;; 查看表上的索引，Cardinality [kɑ:dɪ’nælɪtɪ] 代表基数，代表预估的不重复的值的数量。索引的基数与表总行数越接近，列的离散度就越高。 1show indexes from user_innodb; 如果在 B+Tree 里面的重复值太多，MySQL 的优化器发现走索引跟使用全表扫描差不了多少的时候，就算建了索引，也不一定会走索引。总结：建立索引，要使用离散度(选择度)更高的字段。 联合索引最左匹配前面我们说的都是针对单列创建的索引，但有的时候我们的多条件查询的时候，也会建立联合索引。单列索引可以看成是特殊的联合索引。比如我们在 user 表上面，给 name 和 phone 建立了一个联合索引。 12ALTER TABLE user_innodb DROP INDEX comidx_name_phone;ALTER TABLE user_innodb add INDEX comidx_name_phone (name,phone); 联合索引在 B+Tree 中是复合的数据结构，它是按照从左到右的顺序来建立搜索树的(name 在左边，phone 在右边)。name 是有序的，phone 是无序的。当 name 相等的时候， phone 才是有序的。我们使用 where name= ‘测试’ and phone = ‘136xx ‘去查询数据的时候， B+Tree 会优先比较 name 来确定下一步应该搜索的方向，往左还是往右。如果 name 相同的时候再比较 phone。但是如果查询条件没有 name，就不知道第一步应该查哪个节点，因为建立搜索树的时候 name 是第一个比较因子，所以用不到索引。 什么时候用到联合索引我们在建立联合索引的时候，一定要把最常用的列放在最左边。 使用两个字段，可以用到联合索引: 1EXPLAIN SELECT * FROM user_innodb WHERE name= &apos;权亮&apos; AND phone = &apos;15204661800&apos;; 使用左边的 name 字段，可以用到联合索引: 1EXPLAIN SELECT * FROM user_innodb WHERE name= &apos;权亮&apos; 使用右边的 phone 字段，无法使用索引，全表扫描: 1EXPLAIN SELECT * FROM user_innodb WHERE phone = &apos;15204661800&apos; 覆盖索引非主键索引，我们先通过索引找到主键索引的键值，再通过主键值查出索引里面没有的数据，它比基于主键索引的查询多扫描了一棵索引树，这个过程就叫回表。在辅助索引里面，不管是单列索引还是联合索引，如果 select 的数据列只用从索引中就能够取得，不必从数据区中读取，这时候使用的索引就叫做覆盖索引，这样就避免了回表。 123- 创建联合索引ALTER TABLE user_innodb DROP INDEX comixd_name_phone;ALTER TABLE user_innodb add INDEX `comixd_name_phone` (`name`,`phone`); 这三个查询语句都用到了覆盖索引: 1EXPLAIN SELECT name,phone FROM user_innodb WHERE name= &apos;青山&apos; AND phone = &apos; 13666666666&apos;; EXPLAIN SELECT name FROM user_innodb WHERE name= &apos;青山&apos; AND phone = &apos; 13666666666&apos;; EXPLAIN SELECT phone FROM user_innodb WHERE name= &apos;青山&apos; AND phone = &apos; 13666666666&apos;; Extra 里面值为“Using index”代表使用了覆盖索引。select * ，用不到覆盖索引。很明显，因为覆盖索引减少了 IO 次数，减少了数据的访问量，可以大大地提升查询效率。 索引条件下推(ICP)再来看这么一张表，在 last_name 和 first_name 上面创建联合索引。 12345678910111213141516CREATE TABLE `employees` (`emp_no` int(11) NOT NULL, `birth_date` date NULL, `first_name` varchar(14) NOT NULL,`last_name` varchar(16) NOT NULL, `gender` enum(&apos;M&apos;,&apos;F&apos;) NOT NULL, `hire_date` date NULL,PRIMARY KEY (`emp_no`)) ENGINE=InnoDB DEFAULT CHARSET=latin1; alter table employees add index idx_lastname_firstname(last_name,first_name); INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;698&apos;, &apos;liu&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;d99&apos;, &apos;zheng&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;e08&apos;, &apos;huang&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;59d&apos;, &apos;lu&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;0dc&apos;, &apos;yu&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;989&apos;, &apos;wang&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;e38&apos;, &apos;wang&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;0zi&apos;, &apos;wang&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;dc9&apos;, &apos;xie&apos;, &apos;F&apos;, NULL);INSERT INTO `employees` (`emp_no`, `birth_date`, `first_name`, `last_name`, NULL, &apos;5ba&apos;, &apos;zhou&apos;, &apos;F&apos;, NULL); 关闭 ICP: 1set optimizer_switch=&apos;index_condition_pushdown=off&apos;; 查看参数: 1show variables like &apos;optimizer_switch&apos;; 现在我们要查询所有姓 wang，并且名字最后一个字是 zi 的员工，比如王胖子，王瘦子。查询的 SQL: 1select * from employees where last_name=&apos;wang&apos; and first_name LIKE &apos;%zi&apos; ; 这条 SQL 有两种执行方式: 根据联合索引查出所有姓 wang 的二级索引数据，然后回表，到主键索引上查询全部符合条件的数据(3 条数据)。然后返回给 Server 层，在 Server 层过滤出名字以 zi 结尾的员工。 根据联合索引查出所有姓 wang 的二级索引数据(3 个索引)，然后从二级索引中筛选出 first_name 以 zi 结尾的索引(1 个索引)，然后再回表，到主键索引上查询全部符合条件的数据(1 条数据)，返回给 Server 层。很明显，第二种方式到主键索引上查询的数据更少。注意，索引的比较是在存储引擎进行的，数据记录的比较，是在 Server 层进行的。而当 first_name 的条件不能用于索引过滤时，Server 层不会把 first_name 的条件传递 给存储引擎，所以读取了两条没有必要的记录。这时候，如果满足 last_name=’wang’的记录有 100000 条，就会有 99999 条没有 必要读取的记录。1explain select * from employees where last_name=&apos;wang&apos; and first_name LIKE &apos;%zi&apos; ; Using Where 代表从存储引擎取回的数据不全部满足条件，需要在 Server 层过滤。先用 last_name 条件进行索引范围扫描，读取数据表记录，然后进行比较，检查是否符合 first_name LIKE ‘%zi’ 的条件。此时 3 条中只有 1 条符合条件。开启 ICP: 1set optimizer_switch=&apos;index_condition_pushdown=on&apos;; 此时的执行计划，Using index condition:把 first_name LIKE ‘%zi’下推给存储引擎后，只会从数据表读取所需的 1 条记录。索引条件下推(Index Condition Pushdown)，5.6 以后完善的功能。只适用于二级索引。ICP 的目标是减少访问表的完整行的读数量从而减少 I/O 操作。 索引的创建与使用因为索引对于改善查询性能的作用是巨大的，所以我们的目标是尽量使用索引。 前缀索引当字段值比较长的时候，建立索引会消耗很多的空间，搜索起来也会很慢。我们可以通过截取字段的前面一部分内容建立索引，这个就叫前缀索引。创建一张商户表，因为地址字段比较长，在地址字段上建立前缀索引: 12create table shop(address varchar(120) not null); alter table shop add key (address(12)); 问题是，截取多少呢?截取得多了，达不到节省索引存储空间的目的，截取得少了， 重复内容太多，字段的散列度(选择性)会降低。怎么计算不同的长度的选择性呢?先看一下字段在全部数据中的选择度: 1select count(distinct address) / count(*) from shop; 通过不同长度去计算，与全表的选择性对比: 12select count(distinct left(address,10))/count(*) as sub10, count(distinct left(address,11))/count(*) as sub11, count(distinct left(address,12))/count(*) as sub12, count(distinct left(address,13))/count(*) as sub13from shop; 根据散列度选择合适的长度建立前缀索引。 索引的创建 在用于 where 判断 order 排序和 join 的(on)字段上创建索引 索引的个数不要过多——浪费空间，更新变慢。 区分度低的字段，例如性别，不要建索引——离散度太低，导致扫描行数过多。 频繁更新的值，不要作为主键或者索引——页分裂 组合索引把散列性高(区分度高)的值放在前面。 创建复合索引，而不是修改单列索引。 思考一下： 过长的字段，怎么建立索引? 为什么不建议用无序的值(例如身份证、UUID )作为索引? 什么时候用不到索引? 索引列上使用函数(replace\SUBSTR\CONCAT\sum count avg)、表达式、 计算(+ - * /): 1explain SELECT * FROM `t2` where id+1 = 4; 字符串不加引号，出现隐式转换 12explain SELECT * FROM `user_innodb` where name = 136; explain SELECT * FROM `user_innodb` where name = &apos;136&apos;; like 条件中前面带% 12explain select *from user_innodb where name like &apos;wang%&apos;; explain select *from user_innodb where name like &apos;%wang&apos;; 过滤的开销太大，所以无法使用索引。这个时候可以用全文索引。 负向查询–NOT LIKE 不能1explain select *from employees where last_name not like &apos;wang&apos; != (&lt;&gt;)和 NOT IN 在某些情况下可以: 12explain select *from employees where emp_no not in (1);explain select *from employees where emp_no &lt;&gt; 1; 总结注意一个 SQL 语句是否使用索引，跟数据库版本、数据量、数据选择度都有关系。其实，用不用索引，最终都是优化器说了算。优化器是基于什么的优化器?基于 cost 开销(Cost Base Optimizer)，它不是基于规则(Rule-Based Optimizer)，也不是基于语义。怎么样开销小就怎么来。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入MySQL-SQL执行流程]]></title>
    <url>%2F2020%2F01%2F06%2F%E6%B7%B1%E5%85%A5MySQL-SQL%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一条查询 SQL 语句是如何执行的？我们的程序或者工具要操作数据库，第一步要跟数据库建立连接。 建立连接首先，MySQL 必须要运行一个服务，监听默认的 3306 端口。在我们开发系统跟第三方对接的时候，必须要弄清楚的有两件事。 第一个就是通信协议，比如我们是用 HTTP 还是 WebService 还是 TCP？ 第二个是消息格式，比如我们用 XML 格式，还是 JSON 格式，还是定长格式？报文头长度多少，包含什么内容，每个字段的详细含义。 MySQL 是支持多种通信协议的，可以使用同步/异步的方式，支持长连接/短连接。这里我们拆分来看。 通信类型：同步或者异步同步通信的特点： 同步通信依赖于被调用方，受限于被调用方的性能。也就是说，应用操作数据库，线程会阻塞，等待数据库的返回。 一般只能做到一对一，很难做到一对多的通信。 异步跟同步相反： 异步可以避免应用阻塞等待，但是不能节省 SQL 执行的时间。 如果异步存在并发，每一个 SQL 的执行都要单独建立一个连接，避免数据混乱。但是这样会给服务端带来巨大的压力（一个连接就会创建一个线程，线程间切换会占用大量 CPU 资源）。另外异步通信还带来了编码的复杂度，所以一般不建议使用。如果要异步，必须使用连接池，排队从连接池获取连接而不是创建新连接。 一般来说我们连接数据库都是同步连接。 连接方式：长连接或者短连接MySQL 既支持短连接，也支持长连接。短连接就是操作完毕以后，马上 close 掉。长连接可以保持打开，减少服务端创建和释放连接的消耗，后面的程序访问的时候还可以使用这个连接。一般我们会在连接池中使用长连接。 保持长连接会消耗内存。长时间不活动的连接，MySQL 服务器会断开。 12show global variables like &apos;wait_timeout&apos;; -- 非交互式超时时间，如 JDBC 程序show global variables like &apos;interactive_timeout&apos;; -- 交互式超时时间，如数据库工具 默认都是 28800 秒，8 小时。查看服务端当前连接数 1show global status like &apos;Thread%&apos;; Threads_cached：缓存中的线程连接数。 Threads_connected：当前打开的连接数。 Threads_created：为处理连接创建的线程数。 Threads_running：非睡眠状态的连接数，通常指并发连接数。 每产生一个连接或者一个会话，在服务端就会创建一个线程来处理。反过来，如果要杀死会话，就是 Kill 线程。查看 SQL 的执行状态。 1SHOW PROCESSLIST; 状态 含义 Sleep 线程正在等待客户端，以向它发送一个新语句 Query 线程正在执行查询或往客户端发送数据 Locked 该查询被其它查询锁定 Copying to tmp table on disk 临时结果集合大于 tmp_table_size。线程把临时表从存储器内部格式改变为磁盘模式，以节约存储器 Sending data 线程正在为 SELECT 语句处理行，同时正在向客户端发送数据 Sorting for group 线程正在进行分类，以满足 GROUP BY 要求 Sorting for order 线程正在进行分类，以满足 ORDER BY 要求 MySQL 服务允许的最大连接数是多少呢？在 5.7 版本中默认是 151 个，最大可以设置成 16384（2^14）。 1show variables like &apos;max_connections&apos;; show 的参数说明： 级别：会话 session 级别（默认）；全局 global 级别 动态修改：set，重启后失效；永久生效，修改配置文件/etc/my.cnf1set global max_connections = 1000; 通信协议第一种是 Unix Socket。 比如我们在 Linux 服务器上，如果没有指定-h 参数，它就用 socket 方式登录（省略了-S /var/lib/mysql/mysql.sock）。它不用通过网络协议，也可以连接到 MySQL 的服务器，它需要用到服务器上的一个物理文件（/var/lib/mysql/mysql.sock）。 第二种方式，TCP/IP 协议。 1mysql -h127.0.0.1 -uroot -p123456 我们的编程语言的连接模块都是用 TCP 协议连接到 MySQL 服务器的，比如 mysql-connector-java-x.x.xx.jar。 另外还有命名管道（Named Pipes）和内存共享（Share Memory）的方式，这两种通信方式只能在 Windows 上面使用，一般用得比较少。 通信方式MySQL 使用了半双工的通信方式 要么是客户端向服务端发送数据，要么是服务端向客户端发送数据，这两个动作不能同时发生。所以客户端发送 SQL 语句给服务端的时候，（在一次连接里面）数据是不能分成小块发送的，不管你的 SQL 语句有多大，都是一次性发送。 比如我们用 MyBatis 动态 SQL 生成了一个批量插入的语句，插入 10 万条数据，values 后面跟了一长串的内容，或者 where 条件 in 里面的值太多，会出现问题。 这个时候我们必须要调整 MySQL 服务器配置 max_allowed_packet 参数的值（默认是 4M），把它调大，否则就会报错。 另一方面，对于服务端来说，也是一次性发送所有的数据，不能因为你已经取到了想要的数据就中断操作，这个时候会对网络和内存产生大量消耗。 所以，我们一定要在程序里面避免不带 limit 的这种操作，比如一次把所有满足条件的数据全部查出来，一定要先 count 一下。如果数据量的话，可以分批查询 查询缓存MySQL 内部自带了一个缓存模块。 缓存的作用我们应该很清楚了，把数据以 KV 的形式放到内存里面，可以加快数据的读取速度，也可以减少服务器处理的时间。但是 MySQL 的缓存我们好像比较陌生，从来没有去配置过，也不知道它什么时候生效？ MySQL 的缓存默认是关闭的 1show variables like &apos;query_cache%&apos;; 默认关闭的意思就是不推荐使用，为什么 MySQL 不推荐使用它自带的缓存呢？主要是因为 MySQL 自带的缓存的应用场景有限 第一个是它要求 SQL 语句必须一模一样，中间多一个空格，字母大小写不同都被认为是不同的的 SQL。 第二个是表里面任何一条数据发生变化的时候，这张表所有缓存都会失效，所以对于有大量数据更新的应用，也不适合。 所以缓存这一块，我们还是交给 ORM 框架（比如 MyBatis 默认开启了一级缓存），或者独立的缓存服务，比如 Redis 来处理更合适。（在 MySQL 8.0 中，查询缓存已经被移除了。） 语法解析和预处理为什么我的一条SQL能被识别，MySQL是怎么知道我的语法有错误并给出提示的呢？这个就是 MySQL 的 Parser 解析器和 Preprocessor 预处理模块。 词法解析词法分析就是把一个完整的 SQL 语句打碎成一个个的单词。 1select name from user where id = 1; 它会打碎成 8 个符号，每个符号是什么类型，从哪里开始到哪里结束。 语法解析第二步就是语法分析，语法分析会对 SQL 做一些语法检查，比如单引号有没有闭合，然后根据 MySQL 定义的语法规则，根据 SQL 语句生成一个数据结构。这个数据结构我们把它叫做解析树（select_lex）。任何数据库的中间件，比如 Mycat，Sharding-JDBC（用到了 Druid Parser），都必须要有词法和语法分析功能，在市面上也有很多的开源的词法解析的工具（比如 LEX，Yacc）。 预处理器如果我写了一个词法和语法都正确的 SQL，但是表名或者字段不存在，会在哪里报错？是在数据库的执行层还是解析器？解析器可以分析语法，但是它怎么知道数据库里面有什么表，表里面有什么字段呢？实际上还是在解析的时候报错，解析 SQL 的环节里面有个预处理器。它会检查生成的解析树，解决解析器无法解析的语义。比如，它会检查表和列名是否存在，检查名字和别名，保证没有歧义。(预处理之后得到一个新的解析树) 查询优化与查询执行计划什么是优化器？得到解析树之后，是不是执行 SQL 语句了呢？这里我们有一个问题，一条 SQL 语句是不是只有一种执行方式？或者说数据库最终执行的 SQL 是不是就是我们发送的 SQL？这个答案是否定的。一条 SQL 语句是可以有很多种执行方式的，最终返回相同的结果，他们是等价的。但是如果有这么多种执行方式，这些执行方式怎么得到的？最终选择哪一种去执行？根据什么判断标准去选择？这个就是 MySQL 的查询优化器的模块（Optimizer）。查询优化器的目的就是根据解析树生成不同的执行计划（Execution Plan），然后选择一种最优的执行计划，MySQL 里面使用的是基于开销（cost）的优化器，那种执行计划开销最小，就用哪种。 12//查看查询的开销show status like &apos;Last_query_cost&apos;; 优化器可以做什么？MySQL 的优化器能处理哪些优化类型呢？ 当我们对多张表进行关联查询的时候，以哪个表的数据作为基准表。 有多个索引可以使用的时候，选择哪个索引。 实际上，对于每一种数据库来说，优化器的模块都是必不可少的，他们通过复杂的算法实现尽可能优化查询效率的目标。但是优化器也不是万能的，并不是再垃圾的 SQL 语句都能自动优化，也不是每次都能选择到最优的执行计划，大家在编写 SQL 语句的时候还是要注意。 优化器是怎么得到执行计划的？首先我们要启用优化器的追踪（默认是关闭的）： 12SHOW VARIABLES LIKE &apos;optimizer_trace&apos;;set optimizer_trace=&apos;enabled=on&apos;; 注意开启这开关是会消耗性能的，因为它要把优化分析的结果写到表里面，所以不要轻易开启，或者查看完之后关闭它（注意：参数分为 session 和 global 级别）。接着我们执行一个 SQL 语句，优化器会生成执行计划： 1select t.tcid from teacher t,teacher_contact tc where t.tcid = tc.tcid; 这个时候优化器分析的过程已经记录到系统表里面了，我们可以查询： 1select * from information_schema.optimizer_trace\G 它是一个 JSON 类型的数据，主要分成三部分，准备阶段、优化阶段和执行阶段。expanded_query 是优化后的 SQL 语句。considered_execution_plans 里面列出了所有的执行计划。分析完记得关掉: 12set optimizer_trace=&quot;enabled=off&quot;;SHOW VARIABLES LIKE &apos;optimizer_trace&apos;; 优化器得到的结果优化器最终会把解析树变成一个查询执行计划，查询执行计划是一个数据结构。当然，这个执行计划是不是一定是最优的执行计划呢？不一定，因为 MySQL 也有可能覆盖不到所有的执行计划。我们怎么查看 MySQL 的执行计划呢？比如多张表关联查询，先查询哪张表？在执行查询的时候可能用到哪些索引，实际上用到了什么索引。MySQL 提供了一个执行计划的工具。我们在 SQL 语句前面加上 EXPLAIN，就可以看到执行计划的信息。 1EXPLAIN select name from user where id=1; 注意 Explain 的结果也不一定最终执行的方式。 存储引擎得到执行计划以后，SQL 语句是不是终于可以执行了？ 从逻辑的角度来说，我们的数据是放在哪里的，或者说放在一个什么结构里面？ 执行计划在哪里执行？是谁去执行？存储引擎基本介绍我们先回答第一个问题：在关系型数据库里面，数据是放在什么结构里面的？（放在表 Table 里面的）我们可以把这个表理解成 Excel 电子表格的形式。所以我们的表在存储数据的同时，还要组织数据的存储结构，这个存储结构就是由我们的存储引擎决定的，所以我们也可以把存储引擎叫做表类型。查看存储引擎比如我们数据库里面已经存在的表，我们怎么查看它们的存储引擎呢？1show table status from `test`; 或者通过 DDL 建表语句来查看。在 MySQL 里面，我们创建的每一张表都可以指定它的存储引擎，而不是一个数据库只能使用一个存储引擎。存储引擎的使用是以表为单位的。而且，创建表之后还可以修改存储引擎。我们说一张表使用的存储引擎决定我们存储数据的结构，那在服务器上它们是怎么存储的呢？我们先要找到数据库存放数据的路径： 1show variables like &apos;datadir&apos;; 默认情况下，每个数据库有一个自己文件夹，以 test 数据库为例。任何一个存储引擎都有一个 frm 文件，这个是表结构定义文件。不同的存储引擎存放数据的方式不一样，产生的文件也不一样，innodb 是 1 个，memory 没有，myisam 是两个。 存储引擎比较MyISAM 和 InnoDB 是我们用得最多的两个存储引擎，在 MySQL 5.5 版本之前，默认的存储引擎是 MyISAM，它是 MySQL 自带的。我们创建表的时候不指定存储引擎，它就会使用 MyISAM 作为存储引擎。MyISAM 的前身是 ISAM（Indexed Sequential Access Method：利用索引，顺序存取数据的方法）。5.5 版本之后默认的存储引擎改成了 InnoDB，它是第三方公司为 MySQL 开发的。为什么要改呢？最主要的原因还是 InnoDB 支持事务，支持行级别的锁，对于业务一致性要求高的场景来说更适合。我们可以用这个命令查看数据库对存储引擎的支持情况： 1show engines ; 其中有存储引擎的描述和对事务、XA 协议和 Savepoints 的支持。 XA 协议用来实现分布式事务（分为本地资源管理器，事务管理器）。 Savepoints 用来实现子事务（嵌套事务）。创建了一个 Savepoints 之后，事务就可以回滚到这个点，不会影响到创建 Savepoints 之前的操作。 MyISAM应用范围比较小。表级锁定限制了读/写的性能，因此在 Web 和数据仓库配置中，它通常用于只读或以读为主的工作。特点： 支持表级别的锁（插入和更新会锁表）。 不支持事务。 拥有较高的插入（insert）和查询（select）速度。 存储了表的行数（count 速度更快）。（怎么快速向数据库插入 100 万条数据？我们有一种先用 MyISAM 插入数据，然后修改存储引擎为 InnoDB 的操作。） 适合：只读之类的数据分析的项目。 InnoDBmysql 5.7 中的默认存储引擎。InnoDB 是一个事务安全（与 ACID 兼容）的 MySQL存储引擎，它具有提交、回滚和崩溃恢复功能来保护用户数据。InnoDB 行级锁（不升级为更粗粒度的锁）和 Oracle 风格的一致非锁读提高了多用户并发性和性能。InnoDB 将用户数据存储在聚集索引中，以减少基于主键的常见查询的 I/O。为了保持数据完整性，InnoDB 还支持外键引用完整性约束。特点： 支持事务，支持外键，因此数据的完整性、一致性更高。 支持行级别的锁和表级别的锁。 支持读写并发，写不阻塞读（MVCC）。 特殊的索引存放方式，可以减少 IO，提升查询效率。 适合：经常更新的表，存在并发读写或者有事务处理的业务系统。 Memory将所有数据存储在 RAM 中，以便在需要快速查找非关键数据的环境中快速访问。这个引擎以前被称为堆引擎。其使用案例正在减少；InnoDB 及其缓冲池内存区域提供了一种通用、持久的方法来将大部分或所有数据保存在内存中，而 ndbcluster 为大型分布式数据集提供了快速的键值查找。特点： 把数据放在内存里面，读写的速度很快，但是数据库重启或者崩溃，数据会全部消失。只适合做临时表。 CSV它的表实际上是带有逗号分隔值的文本文件。csv表允许以csv格式导入或转储数据，以便与读写相同格式的脚本和应用程序交换数据。因为 csv 表没有索引，所以通常在正常操作期间将数据保存在 innodb 表中，并且只在导入或导出阶段使用 csv 表。特点： 不允许空行，不支持索引。格式通用，可以直接编辑，适合在不同数据库之间导入导出。 Archive这些紧凑的未索引的表用于存储和检索大量很少引用的历史、存档或安全审计信息。特点： 不支持索引，不支持 update delete。 总结这是 MySQL 里面常见的一些存储引擎，我们看到了，不同的存储引擎提供的特性都不一样，它们有不同的存储机制、索引方式、锁定水平等功能。我们在不同的业务场景中对数据操作的要求不同，就可以选择不同的存储引擎来满足我们的需求，这个就是 MySQL 支持这么多存储引擎的原因。 如何选择存储引擎？ 如果对数据一致性要求比较高，需要事务支持，可以选择 InnoDB。 如果数据查询多更新少，对查询性能要求比较高，可以选择 MyISAM。 如果需要一个用于查询的临时表，可以选择 Memory。 如果所有的存储引擎都不能满足你的需求，并且技术能力足够，可以根据官网内部手册用 C 语言开发一个存储引擎：https://dev.mysql.com/doc/internals/en/custom-engine.html 执行引擎，返回结果OK，存储引擎分析完了，它是我们存储数据的形式，继续第二个问题，是谁使用执行计划去操作存储引擎呢？这就是我们的执行引擎，它利用存储引擎提供的相应的 API 来完成操作。为什么我们修改了表的存储引擎，操作方式不需要做任何改变？因为不同功能的存储引擎实现的 API 是相同的。最后把数据返回给客户端，即使没有结果也要返回。 MySQL 体系结构总结基于上面分析的流程，我们一起来梳理一下 MySQL 的内部模块。 模块详解 Connector：用来支持各种语言和 SQL 的交互，比如 PHP，Python，Java 的JDBC； Management Serveices &amp; Utilities：系统管理和控制工具，包括备份恢复. MySQL 复制. 集群等等； Connection Pool：连接池，管理需要缓冲的资源，包括用户密码权限线程等等； SQL Interface：用来接收用户的 SQL 命令，返回用户需要的查询结果 Parser：用来解析 SQL 语句； Optimizer：查询优化器； Cache and Buffer：查询缓存，除了行记录的缓存之外，还有表缓存，Key 缓存，权限缓存等等； Pluggable Storage Engines：插件式存储引擎，它提供 API 给服务层使用，跟具体的文件打交道。架构分层总体上，我们可以把 MySQL 分成三层，跟客户端对接的连接层，真正执行操作的服务层，和跟硬件打交道的存储引擎层（参考 MyBatis：接口、核心、基础）。连接层我们的客户端要连接到 MySQL 服务器 3306 端口，必须要跟服务端建立连接，那么管理所有的连接，验证客户端的身份和权限，这些功能就在连接层完成。服务层连接层会把 SQL 语句交给服务层，这里面又包含一系列的流程： 比如查询缓存的判断、根据 SQL 调用相应的接口，对我们的 SQL 语句进行词法和语法的解析（比如关键字怎么识别，别名怎么识别，语法有没有错误等等）。 然后就是优化器，MySQL 底层会根据一定的规则对我们的 SQL 语句进行优化，最后再交给执行器去执行。 存储引擎存储引擎就是我们的数据真正存放的地方，在 MySQL 里面支持不同的存储引擎。再往下就是内存或者磁盘。 一条更新 SQL 是如何执行的？在数据库里面，我们说的 update 操作其实包括了更新、插入和删除。如果大家有看过 MyBatis 的源码，应该知道 Executor 里面也只有 doQuery()和 doUpdate()的方法，没有 doDelete()和 doInsert()。更新流程和查询流程有什么不同呢？基本流程也是一致的，也就是说，它也要经过解析器、优化器的处理，最后交给执行器。区别就在于拿到符合条件的数据之后的操作。 缓冲池首先，InnnoDB 的数据都是放在磁盘上的，InnoDB 操作数据有一个最小的逻辑单位，叫做页（索引页和数据页）。我们对于数据的操作，不是每次都直接操作磁盘，因为磁盘的速度太慢了。InnoDB 使用了一种缓冲池的技术，也就是把磁盘读到的页放到一块内存区域里面。这个内存区域就叫 Buffer Pool。下一次读取相同的页，先判断是不是在缓冲池里面，如果是，就直接读取，不用再次访问磁盘。修改数据的时候，先修改缓冲池里面的页。内存的数据页和磁盘数据不一致的时候，我们把它叫做脏页。InnoDB 里面有专门的后台线程把 Buffer Pool 的数据写入到磁盘，每隔一段时间就一次性地把多个修改写入磁盘，这个动作就叫做刷脏。Buffer Pool 是 InnoDB 里面非常重要的一个结构，它的内部又分成几块区域。这里我们趁机到官网来认识一下 InnoDB 的内存结构和磁盘结构。 InnoDB 内存结构和磁盘结构 内存结构Buffer Pool 主要分为 3 个部分： Buffer Pool、Change Buffer、Adaptive Hash Index，另外还有一个（redo）log buffer。 Buffer PoolBuffer Pool 缓存的是页面信息，包括数据页、索引页。查看服务器状态，里面有很多跟 Buffer Pool 相关的信息： 1SHOW STATUS LIKE &apos;%innodb_buffer_pool%&apos;; Buffer Pool 默认大小是 128M（134217728 字节），可以调整。查看参数（系统变量）： 1SHOW VARIABLES like &apos;%innodb_buffer_pool%&apos;; 内存的缓冲池写满了怎么办？（Redis 设置的内存满了怎么办？）InnoDB 用 LRU算法来管理缓冲池（链表实现，不是传统的 LRU，分成了 young 和 old），经过淘汰的数据就是热点数据。 Change Buffer内存缓冲区对于提升读写性能有很大的作用。思考一个问题：当需要更新一个数据页时，如果数据页在 Buffer Pool 中存在，那么就直接更新好了。否则的话就需要从磁盘加载到内存，再对内存的数据页进行操作。也就是说，如果没有命中缓冲池，至少要产生一次磁盘 IO，有没有优化的方式呢？如果这个数据页不是唯一索引，不存在数据重复的情况，也就不需要从磁盘加载索引页判断数据是不是重复（唯一性检查）。这种情况下可以先把修改记录在内存的缓冲池中，从而提升更新语句（Insert、Delete、Update）的执行速度。这一块区域就是 Change Buffer。5.5 之前叫 Insert Buffer 插入缓冲，现在也能支持 delete 和 update。最后把 Change Buffer 记录到数据页的操作叫做 merge。什么时候发生 merge？有几种情况：在访问这个数据页的时候，或者通过后台线程、或者数据库 shut down、redo log 写满时触发。如果数据库大部分索引都是非唯一索引，并且业务是写多读少，不会在写数据后立刻读取，就可以使用 Change Buffer（写缓冲）。写多读少的业务，调大这个值： 1SHOW VARIABLES LIKE &apos;innodb_change_buffer_max_size&apos;; 代表 Change Buffer 占 Buffer Pool 的比例，默认 25%。 Adaptive Hash Index索引应该是放在磁盘的，为什么要专门把一种哈希的索引放到内存？下章详解 （redo）Log Buffer思考一个问题：如果 Buffer Pool 里面的脏页还没有刷入磁盘时，数据库宕机或者重启，这些数据丢失。如果写操作写到一半，甚至可能会破坏数据文件导致数据库不可用。为了避免这个问题，InnoDB 把所有对页面的修改操作专门写入一个日志文件，并且在数据库启动时从这个文件进行恢复操作（实现 crash-safe）——用它来实现事务的持久性。这个文件就是磁盘的 redo log（叫做重做日志），对应于/var/lib/mysql/目录下的 ib_logfile0 和 ib_logfile1，每个48M。这 种 日 志 和 磁 盘 配 合 的 整 个 过 程 ， 其 实 就 是 MySQL 里 的 WAL 技 术（Write-Ahead Logging），它的关键点就是先写日志，再写磁盘。 1show variables like &apos;innodb_log%&apos;; 值 含义 innodb_log_file_size 指定每个文件的大小，默认48M innodb_log_files_in_group 指定文件的数量，默认为2 innodb_log_group_home_dir 指定文件所在路径，相对或绝对。如果不指定，则为datadir 路径。 同样是写磁盘，为什么不直接写到 db file 里面去？为什么先写日志再写磁盘？我们先来了解一下随机 I/O 和顺序 I/O 的概念。磁盘的最小组成单元是扇区，通常是 512 个字节。操作系统和内存打交道，最小的单位是页 Page。操作系统和磁盘打交道，读写磁盘，最小的单位是块 Block。如果我们所需要的数据是随机分散在不同页的不同扇区中，那么找到相应的数据需要等到磁臂旋转到指定的页，然后盘片寻找到对应的扇区，才能找到我们所需要的一块数据，一次进行此过程直到找完所有数据，这个就是随机 IO，读取数据速度较慢。假设我们已经找到了第一块数据，并且其他所需的数据就在这一块数据后边，那么就不需要重新寻址，可以依次拿到我们所需的数据，这个就叫顺序 IO。刷盘是随机 I/O，而记录日志是顺序 I/O，顺序 I/O 效率更高。因此先把修改写入日志，可以延迟刷盘时机，进而提升系统吞吐。当然 redo log 也不是每一次都直接写入磁盘，在 Buffer Pool 里面有一块内存区域（Log Buffer）专门用来保存即将要写入日志文件的数据，默认 16M，它一样可以节省磁盘 IO。 1SHOW VARIABLES LIKE &apos;innodb_log_buffer_size&apos;; 需要注意：redo log 的内容主要是用于崩溃恢复。磁盘的数据文件，数据来自 buffer pool。redo log 写入磁盘，不是写入数据文件。那么，Log Buffer 什么时候写入 log file？在我们写入数据到磁盘的时候，操作系统本身是有缓存的。flush 就是把操作系统缓冲区写入到磁盘。log buffer 写入磁盘的时机，由一个参数控制，默认是 1。 1SHOW VARIABLES LIKE &apos;innodb_flush_log_at_trx_commit&apos;; 值 含义 0（延迟写） log buffer 将每秒一次地写入 log file 中，并且 log file 的 flush 操作同时进行。该模式下，在事务提交的时候，不会主动触发写入磁盘的操作。 1（默认，实时写，实时刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file，并且刷到磁盘中去。 2（实时写，延迟刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file。但是 flush 操作并不会同时进行。该模式下，MySQL 会每秒执行一次 flush 操作。 这是内存结构的第 4 块内容，redo log，它又分成内存和磁盘两部分。redo log 有什么特点？ redo log 是 InnoDB 存储引擎实现的，并不是所有存储引擎都有。 不是记录数据页更新之后的状态，而是记录这个页做了什么改动，属于物理日志。 redo log 的大小是固定的，前面的内容会被覆盖。 check point 是当前要覆盖的位置。如果 write pos 跟 check point 重叠，说明 redo log 已经写满，这时候需要同步 redo log 到磁盘中。这是 MySQL 的内存结构，总结一下，分为：Buffer pool、change buffer、Adaptive Hash Index、 log buffer。磁盘结构里面主要是各种各样的表空间，叫做 Table space。 磁盘结构表空间可以看做是 InnoDB 存储引擎逻辑结构的最高层，所有的数据都存放在表空间中。InnoDB 的表空间分为 5 大类。 系统表空间在默认情况下 InnoDB 存储引擎有一个共享表空间（对应文件/var/lib/mysql/ibdata1），也叫系统表空间。InnoDB 系统表空间包含 InnoDB 数据字典和双写缓冲区，Change Buffer 和 Undo Logs），如果没有指定 file-per-table，也包含用户创建的表和索引数据。 undo 在后面介绍，因为有独立的表空间。 数据字典：由内部系统表组成，存储表和索引的元数据（定义信息）。 双写缓冲（InnoDB 的一大特性）：InnoDB 的页和操作系统的页大小不一致，InnoDB 页大小一般为 16K，操作系统页大小为 4K，InnoDB 的页写入到磁盘时，一个页需要分 4 次写。如果存储引擎正在写入页的数据到磁盘时发生了宕机，可能出现页只写了一部分的情况，比如只写了 4K，就宕机了，这种情况叫做部分写失效（partial page write），可能会导致数据丢失。1show variables like &apos;innodb_doublewrite&apos;; 我们不是有 redo log 吗？但是有个问题，如果这个页本身已经损坏了，用它来做崩溃恢复是没有意义的。所以在对于应用 redo log 之前，需要一个页的副本。如果出现了写入失效，就用页的副本来还原这个页，然后再应用 redo log。这个页的副本就是 double write，InnoDB 的双写技术。通过它实现了数据页的可靠性。跟 redo log 一样，double write 由两部分组成，一部分是内存的 double write，一个部分是磁盘上的 double write。因为 double write 是顺序写入的，不会带来很大的开销。在默认情况下，所有的表共享一个系统表空间，这个文件会越来越大，而且它的空间不会收缩。 独占表空间我们可以让每张表独占一个表空间。这个开关通过 innodb_file_per_table 设置，默认开启。 1SHOW VARIABLES LIKE &apos;innodb_file_per_table&apos;; 开启后，则每张表会开辟一个表空间，这个文件就是数据目录下的 ibd 文件（例如/var/lib/mysql/gupao/user_innodb.ibd），存放表的索引和数据。但是其他类的数据，如回滚（undo）信息，插入缓冲索引页、系统事务信息，二次写缓冲（Double write buffer）等还是存放在原来的共享表空间内。 通用表空间通用表空间也是一种共享的表空间，跟 ibdata1 类似。可以创建一个通用的表空间，用来存储不同数据库的表，数据路径和文件可以自定义。语法： 1create tablespace ts2673 add datafile &apos;/var/lib/mysql/ts2673.ibd&apos; file_block_size=16K engine=innodb; 在创建表的时候可以指定表空间，用 ALTER 修改表空间可以转移表空间。 1create table t2673(id integer) tablespace ts2673; 不同表空间的数据是可以移动的，删除表空间需要先删除里面的所有表。 临时表空间存储临时表的数据，包括用户创建的临时表，和磁盘的内部临时表。对应数据目录下的 ibtmp1 文件。当数据服务器正常关闭时，该表空间被删除，下次重新产生。 undo log tablespaceundo log（撤销日志或回滚日志）记录了事务发生之前的数据状态（不包括 select）。如果修改数据时出现异常，可以用 undo log 来实现回滚操作（保持原子性）。在执行 undo 的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，属于逻辑格式的日志。redo Log 和 undo Log 与事务密切相关，统称为事务日志。undo Log 的数据默认在系统表空间 ibdata1 文件中，因为共享表空间不会自动收缩，也可以单独创建一个 undo 表空间。 1show global variables like &apos;%undo%&apos;; 有了这些日志之后，我们来总结一下一个更新操作的流程，这是一个简化的过程。name原值为test 1update user set name = &apos;test1&apos; where id=1; 事务开始，从内存或磁盘取到这条数据，返回给 Server 的执行器； 执行器修改这一行数据的值为 test1； 记录 name=test 到 undo log； 记录 name=test1 到 redo log； 调用存储引擎接口，在内存（Buffer Pool）中修改 name=test1； 事务提交。 内存和磁盘之间，工作着很多后台线程。 后台线程后台线程的主要作用是负责刷新内存池中的数据和把修改的数据页刷新到磁盘。后台线程分为：master thread，IO thread，purge thread，page cleaner thread。 master thread 负责刷新缓存数据到磁盘并协调调度其它后台进程。 IO thread 分为 insert buffer、log、read、write 进程。分别用来处理 insert buffer、重做日志、读写请求的 IO 回调。 purge thread 用来回收 undo 页。 page cleaner thread 用来刷新脏页。 除了 InnoDB 架构中的日志文件，MySQL 的 Server 层也有一个日志文件，叫做 binlog，它可以被所有的存储引擎使用。 Binlogbinlog 以事件的形式记录了所有的 DDL 和 DML 语句（因为它记录的是操作而不是数据值，属于逻辑日志），可以用来做主从复制和数据恢复。跟 redo log 不一样，它的文件内容是可以追加的，没有固定大小限制。在开启了 binlog 功能的情况下，我们可以把 binlog 导出成 SQL 语句，把所有的操作重放一遍，来实现数据的恢复。binlog 的另一个功能就是用来实现主从复制，它的原理就是从服务器读取主服务器的 binlog，然后执行一遍。有了这两个日志之后，我们来看一下一条更新语句是怎么执行的：例如一条语句：update teacher set name=’彭于晏’ where id=1; 先查询到这条数据，如果有缓存，也会用到缓存。 把 name 改成彭于晏，然后调用引擎的 API 接口，写入这一行数据到内存，同时记录 redo log。这时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，可以随时提交。 执行器收到通知后记录 binlog，然后调用存储引擎接口，设置 redo log为 commit状态。 更新完成。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解Redis-实战篇]]></title>
    <url>%2F2019%2F12%2F01%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Redis-%E5%AE%9E%E6%88%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Redis客户端客户端通信原理客户端和服务器通过 TCP 连接来进行数据交互， 服务器默认的端口号为 6379 。 客户端和服务器发送的命令或数据一律以 \r\n (CRLF 回车+换行)结尾。如果使用 wireshark 对 jedis 抓包:环境:Jedis 连接到虚拟机 202，运行 main，对 VMnet8 抓包。过滤条件:ip.dst==192.168.8.202 and tcp.port in {6379}对 set test 111 抓包，可以看到实际发出的数据包是: 1*3\r\n$3\r\nSET\r\n$4\r\ntest\r\n$3\r\n111\r\n 对 get test 抓包，可以看到实际发出的数据包是: 1*2\r\n$3\r\nGET\r\n$4\r\ntest\r\n 客户端跟 Redis 之间 使用一种特殊的编码格式(在 AOF 文件里面我们看到了)，叫 做 Redis Serialization Protocol (Redis 序列化协议)。特点:容易实现、解析快、可读 性强。客户端发给服务端的消息需要经过编码，服务端收到之后会按约定进行解码，反之亦然。基于此，我们可以自己实现一个 Redis 客户端。 建立 Socket 连接 OutputStream 写入数据(发送到服务端) InputStream 读取数据(从服务端接口) 基于这种协议，我们可以用 Java 实现所有的 Redis 操作命令。当然，我们不需要这 么做，因为已经有很多比较成熟的 Java 客户端，实现了完整的功能和高级特性，并且提供了良好的性能。官网展示的客户端官网推荐的 Java 客户端有 3 个 Jedis，Redisson 和 Luttuce。 客户端 描述 Jedis A blazingly small and sane redis java client lettuce Advanced Redis client for thread-safe sync, async, and reactive usage. Supports Cluster, Sentinel, Pipelining, and codecs. Redisson distributed and scalable Java data structures on top of Redis server Spring 连接 Redis 用的是什么?RedisConnectionFactory 接口支持多种实现，例如 : JedisConnectionFactory 、 JredisConnectionFactory、LettuceConnectionFactory、SrpConnectionFactory。 Jedishttps://github.com/xetorthio/jedis 特点Jedis 是我们最熟悉和最常用的客户端。轻量，简洁，便于集成和改造。Jedis 多个线程使用一个连接的时候线程不安全。可以使用连接池，为每个请求创建不同的连接，基于 Apache common pool 实现。跟数据库一样，可以设置最大连接数等参数。Jedis 中有多种连接池的子类（JedisPool、JedisSentinelPool、ShardedJedisPool）。Jedis 有4种工作模式:单节点、分片、哨兵、集群。Jedis 有3种请求模式:Client、Pipeline、事务。Client 模式就是客户端发送一个命令，阻塞等待服务端执行，然后读取 返回结果。Pipeline 模式是一次性发送多个命令，最后一次取回所有的返回结果，这种模式通过减少网络的往返时间和 io 读写次数，大幅度提高通信性能。第三种是事务模式。Transaction 模式即开启 Redis 的事务管理，事务模式开启后，所有的命令(除了 exec，discard，multi 和 watch)到达服务端以后不会立即执行，会进入一个等待队列。 Sentinel 获取连接原理问题:Jedis 连接 Sentinel 的时候，我们配置的是全部哨兵的地址。Sentinel 是如 何返回可用的 master 地址的呢?在构造方法中: 1pool = new JedisSentinelPool(masterName, sentinels); 调用了: 1HostAndPort master = initSentinels(sentinels, masterName); 查看: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859private HostAndPort initSentinels(Set&lt;String&gt; sentinels, final String masterName) &#123; HostAndPort master = null; boolean sentinelAvailable = false; log.info(&quot;Trying to find master from available Sentinels...&quot;); // 有多个 sentinels,遍历这些个 sentinels for (String sentinel : sentinels) &#123; // host:port 表示的 sentinel 地址转化为一个 HostAndPort 对象。 final HostAndPort hap = HostAndPort.parseString(sentinel); log.fine(&quot;Connecting to Sentinel &quot; + hap); Jedis jedis = null; try &#123; // 连接到 sentinel jedis = new Jedis(hap.getHost(), hap.getPort()); // 根据 masterName 得到 master 的地址，返回一个 list，host= list[0], port =// list[1] List&lt;String&gt; masterAddr = jedis.sentinelGetMasterAddrByName(masterName); sentinelAvailable = true; if (masterAddr == null || masterAddr.size() != 2) &#123; log.warning(&quot;Can not get master addr, master name: &quot; + masterName + &quot;. Sentinel: &quot; + hap + &quot;.&quot;); continue; &#125; // 如果在任何一个 sentinel 中找到了 master，不再遍历 sentinels master = toHostAndPort(masterAddr); log.fine(&quot;Found Redis master at &quot; + master); break; &#125; catch (JedisException e) &#123; // resolves #1036, it should handle JedisException there&apos;s another chance // of raising JedisDataException log.warning(&quot;Cannot get master address from sentinel running @ &quot; + hap + &quot;. Reason: &quot; + e + &quot;. Trying next one.&quot;); &#125; finally &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125; &#125; // 到这里，如果 master 为 null，则说明有两种情况，一种是所有的 sentinels 节点都 down 掉了，一种是 master 节点没有被存活的 sentinels 监控到 if (master == null) &#123; if (sentinelAvailable) &#123; // can connect to sentinel, but master name seems to not // monitored throw new JedisException(&quot;Can connect to sentinel, but &quot; + masterName + &quot; seems to be not monitored...&quot;); &#125; else &#123; throw new JedisConnectionException(&quot;All sentinels down, cannot determine where is &quot; + masterName + &quot; master is running...&quot;); &#125; &#125; // 如果走到这里，说明找到了 master 的地址 log.info(&quot;Redis master running at &quot; + master + &quot;, starting Sentinel listeners...&quot;); // 启动对每个 sentinels 的监听为每个 sentinel 都启动了一个监听者 MasterListener。MasterListener 本身是一个线程，它会去订阅 sentinel 上关于 master 节点地址改变的消息。 for (String sentinel : sentinels) &#123; final HostAndPort hap = HostAndPort.parseString(sentinel); MasterListener masterListener = new MasterListener(masterName, hap.getHost(), hap.getPort()); // whether MasterListener threads are alive or not, process can be stopped masterListener.setDaemon(true); masterListeners.add(masterListener); masterListener.start(); &#125; return master;&#125; Cluster 获取连接原理问题:使用 Jedis 连接 Cluster 的时候，我们只需要连接到任意一个或者多个 redis group 中的实例地址，那我们是怎么获取到需要操作的 Redis Master 实例的?关键问题:在于如何存储 slot 和 Redis 连接池的关系。 程序启动初始化集群环境，读取配置文件中的节点配置，无论是主从，无论多少个，只拿第一个，获取 redis 连接实例(后面有个 break)。 12345678910111213141516171819// redis.clients.jedis.JedisClusterConnectionHandler#initializeSlotsCacheprivate void initializeSlotsCache(Set&lt;HostAndPort&gt; startNodes, GenericObjectPoolConfig poolConfig, String password) &#123; for (HostAndPort hostAndPort : startNodes) &#123; // 获取一个 Jedis 实例 Jedis jedis = new Jedis(hostAndPort.getHost(), hostAndPort.getPort()); if (password != null) &#123; jedis.auth(password); &#125; try &#123; // 获取 Redis 节点和 Slot 虚拟槽 cache.discoverClusterNodesAndSlots(jedis); // 直接跳出循环 break; &#125; catch (JedisConnectionException e) &#123; // try next nodes &#125; finally &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125; &#125;&#125; 用获取的 redis 连接实例执行 clusterSlots ()方法，实际执行 redis 服务端 cluster slots 命令，获取虚拟槽信息。该集合的基本信息为[long, long, List, List], 第一，二个元素是该节点负责槽点的起 始位置，第三个元素是主节点信息，第四个元素为主节点对应的从节点信息。该 list 的 基本信息为[string,int,string],第一个为 host 信息，第二个为 port 信息，第三个为唯一 id。 获取有关节点的槽点信息后，调用 getAssignedSlotArray(slotinfo)来获取所有的槽点值。 再获取主节点的地址信息，调用 generateHostAndPort(hostInfo)方法，生成一个 ostAndPort 对象。 再根据节点地址信息来设置节点对应的 JedisPool，即设置 Map&lt;String,JedisPool&gt; nodes 的值。接下来判断若此时节点信息为主节点信息时，则调用 assignSlotsToNodes 方法，设置每个槽点值对应的连接池，即设置 Map&lt;Integer, JedisPool&gt; slots 的值。 12345678910111213141516171819202122232425262728293031323334353637383940// redis.clients.jedis.JedisClusterInfoCache#discoverClusterNodesAndSlotspublic void discoverClusterNodesAndSlots(Jedis jedis) &#123; w.lock();​ try &#123; reset(); // 获取节点集合 List&lt;Object&gt; slots = jedis.clusterSlots(); // 遍历3个master节点 for (Object slotInfoObj : slots) &#123; // slotInfo 槽开始，槽结束，主，从 // &#123;[0,5460,7291,7294],[5461,10922,7292,7295],[10923,16383,7293,7296]&#125; List&lt;Object&gt; slotInfo = (List&lt;Object&gt;) slotInfoObj; // 如果&lt;=2，代表没有分配 slot if (slotInfo.size() &lt;= MASTER_NODE_INDEX) &#123; continue; &#125; // 获取分配到当前 master 节点的数据槽，例如 7291 节点的&#123;0,1,2,3......5460&#125; List&lt;Integer&gt; slotNums = getAssignedSlotArray(slotInfo); ​ // hostInfos int size = slotInfo.size();// size 是 4，槽最小最大，主，从 // 第 3 位和第 4 位是主从端口的信息 for (int i = MASTER_NODE_INDEX; i &lt; size; i++) &#123; List&lt;Object&gt; hostInfos = (List&lt;Object&gt;) slotInfo.get(i); if (hostInfos.size() &lt;= 0) &#123; continue; &#125; // 根据 IP 端口生成 HostAndPort 实例 HostAndPort targetNode = generateHostAndPort(hostInfos); // 据HostAndPort解析出ip:port的key值，再根据key从缓存中查询对应的jedisPool实例。如果没有jedisPool实例，就创建 JedisPool 实例，最后放入缓存中。nodeKey 和 nodePool 的关系 setupNodeIfNotExist(targetNode); // 把 slot 和 jedisPool 缓存起来(16384 个)，key 是 slot 下标，value 是连接池 if (i == MASTER_NODE_INDEX) &#123; assignSlotsToNode(slotNums, targetNode); &#125; &#125; &#125; &#125; finally &#123; w.unlock(); &#125;&#125; 从集群环境存取值: 把 key 作为参数，执行 CRC16 算法，获取 key 对应的 slot 值。 通过该 slot 值，去 slots 的 map 集合中获取 jedisPool 实例。 通过 jedisPool 实例获取 jedis 实例，最终完成 redis 数据存取工作。 pipeline通过 Lua 脚本 set 2 万个 key 用了好几分钟，这个速度太慢了，完全没有把 Redis 10 万的 QPS 利用起来。但是单个命令的执行到底慢在哪里? 慢在哪里?Redis 使用的是客户端/服务器(C/S)模型和请求/响应协议的 TCP 服务器。这意味着通常情况下一个请求会遵循以下步骤: 客户端向服务端发送一个查询请求，并监听 Socket 返回，通常是以阻塞模式，等待服务端响应。 服务端处理命令，并将结果返回给客户端。 Redis 客户端与 Redis 服务器之间使用 TCP 协议进行连接，一个客户端可以通过一个 socket 连接发起多个请求命令。每个请求命令发出后 client 通常会阻塞并等待 redis 服务器处理，redis 处理完请求命令后会将结果通过响应报文返回给 client，因此当执行多条命令的时候都需要等待上一条命令执行完毕才能执行。Redis 本身提供了一些批量操作命令，比如 mget，mset，可以减少通信的时间，但是大部分命令是不支持 multi 操作的，例如 hash 就没有。由于通信会有网络延迟，假如 client 和 server 之间的包传输时间需要 10 毫秒，一次交互就是 20 毫秒(RTT:Round Trip Time)。这样的话，client 1 秒钟也只能也只能发送 50 个命令。这显然没有充分利用 Redis 的处理能力。另外一个，Redis 服务端执行 I/O 的次数过多。 Pipeline 管道https://redis.io/topics/pipelining那我们能不能像数据库的 batch 操作一样，把一组命令组装在一起发送给 Redis 服务端执行，然后一次性获得返回结果呢?这个就是 Pipeline 的作用。Pipeline 通过一个队列把所有的命令缓存起来，然后把多个命令在一次连接中发送给服务器。要实现 Pipeline，既要服务端的支持，也要客户端的支持。对于服务端来说，需要能够处理客户端通过一个 TCP 连接发来的多个命令，并且逐个地执行命令一起返回 。对于客户端来说，要把多个命令缓存起来，达到一定的条件就发送出去，最后才处理 Redis 的应答(这里也要注意对客户端内存的消耗)。jedis-pipeline 的 client-buffer 限制:8192bytes，客户端堆积的命令超过 8192bytes 时，会发送给服务端。源码:redis.clients.util.RedisOutputStream.java 123public RedisOutputStream(final OutputStream out) &#123; this(out, 8192);&#125; pipeline 对于命令条数没有限制，但是命令可能会受限于 TCP 包大小。如果 Jedis 发送了一组命令，而发送请求还没有结束，Redis 响应的结果会放在接收缓冲区。如果接收缓冲区满了，jedis 会通知 redis win=0，此时 redis 不会再发送结果给 jedis 端，转而把响应结果保存在 Redis 服务端的输出缓冲区中。输出缓冲区的配置:redis.conf 12# client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;client-output-buffer-limit normal 0 0 0 client-output-buffer-limit replica 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 配置 作用 class 客户端类型，分为三种。a)normal:普通客户端;b)slave:slave 客户端，用于复制;c) pubsub:发布订阅客户端 hard limit 如果客户端使用的输出缓冲区大于，客户端会被立即关闭，0 代表不限制 soft limit soft seconds 如果客户端使用的输出缓冲区超过了并且持续了秒，客户端会被立即 关闭 每个客户端使用的输出缓冲区的大小可以用 client list 命令查看 12redis&gt; client listid=5 addr=192.168.8.1:10859 fd=8 name= age=5 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=5 qbuf-free=32763 obl=16380 oll=227 omem=4654408 events=rw cmd=set obl : 输出缓冲区的长度(字节为单位， 0 表示没有分配输出缓冲区) oll : 输出列表包含的对象数量(当输出缓冲区没有剩余空间时，命令回复会以字符串对象的形式被入队到这个 队列里) omem : 输出缓冲区和输出列表占用的内存总量 使用场景如果某些操作需要马上得到 Redis 操作是否成功的结果，这种场景就不适合。有些场景，例如批量写入数据，对于结果的实时性和成功性要求不高，就可以用 Pipeline。 Jedis 实现分布式锁原文地址:https://redis.io/topics/distlock中文地址:http://redis.cn/topics/distlock.html分布式锁的基本特性或者要求: 互斥性:只有一个客户端能够持有锁。 不会产生死锁:即使持有锁的客户端崩溃，也能保证后续其他客户端可以获取锁。 只有持有这把锁的客户端才能解锁。 12345678public static boolean tryGetDistributedLock(Jedis jedis, String lockKey, String requestId, int expireTime) &#123; // set 支持多个参数 NX(not exist) XX(exist) EX(seconds) PX(million seconds) String result = jedis.set(lockKey, requestId, SET_IF_NOT_EXIST, SET_WITH_EXPIRE_TIME, expireTime); if (LOCK_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 参数解读: lockKey 是 Redis key 的名称，也就是谁添加成功这个 key 代表谁获取锁成功。 requestId 是客户端的 ID(设置成 value)，如果我们要保证只有加锁的客户端才能释放锁，就必须获得客户端的 ID(保证第 3 点)。 SET_IF_NOT_EXIST 是我们的命令里面加上 NX(保证第 1 点)。 SET_WITH_EXPIRE_TIME，PX 代表以毫秒为单位设置 key 的过期时间(保证第 2 点)。expireTime 是自动释放锁的时间，比如 5000 代表 5 秒。 释放锁，直接删除 key 来释放锁可以吗?就像这样: 123public static void wrongReleaseLock1(Jedis jedis, String lockKey) &#123; jedis.del(lockKey);&#125; 没有对客户端 requestId 进行判断，可能会释放其他客户端持有的锁。 先判断后删除呢? 1234567public static void wrongReleaseLock2(Jedis jedis, String lockKey, String requestId) &#123; // 判断加锁与解锁是不是同一个客户端 if (requestId.equals(jedis.get(lockKey))) &#123; // 若在此时，这把锁突然不是这个客户端的，则会误解锁 jedis.del(lockKey); &#125;&#125; 如果在释放锁的时候，这把锁已经不属于这个客户端(例如已经过期，并且被别的客户端获取锁成功了)，那就会出现释放了其他客户端的锁的情况。所以我们把判断客户端是否相等和删除 key 的操作放在 Lua 脚本里面执行。 12345678public static boolean releaseDistributedLock(Jedis jedis, String lockKey, String requestId) &#123; String script = &quot;if redis.call(&apos;get&apos;, KEYS[1]) == ARGV[1] then return redis.call(&apos;del&apos;, KEYS[1]) else return 0 end&quot;; Object result = jedis.eval(script, Collections.singletonList(lockKey), Collections.singletonList(requestId)); if (RELEASE_SUCCESS.equals(result)) &#123; return true; &#125; return false;&#125; 这个是 Jedis 里面分布式锁的实现。 Luttecehttps://lettuce.io/ 特点与 Jedis 相比，Lettuce 则完全克服了其线程不安全的缺点:Lettuce 是一个可伸缩的线程安全的 Redis 客户端，支持同步、异步和响应式模式(Reactive)。多个线程可以共享一个连接实例，而不必担心多线程并发问题。它基于 Netty 框架构建，支持 Redis 的高级功能，如 Pipeline、发布订阅/事务、 Sentinel、集群、支持连接池。Lettuce 是 Spring Boot 2.x 默认的客户端，替换了 Jedis。集成之后我们不需要单独使用它，直接调用 Spring 的 RedisTemplate 操作，连接和创建和关闭也不需要我们操心。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; Redissonhttps://redisson.org/https://github.com/redisson/redisson/wiki/目录 本质Redisson 是一个在 Redis 的基础上实现的 Java 驻内存数据网格(In-Memory Data Grid)，提供了分布式和可扩展的 Java 数据结构。 特点 基于 Netty 实现，采用非阻塞 IO，性能高 支持异步请求 支持连接池、pipeline、LUA Scripting、Redis Sentinel、Redis Cluster 不支持事务，官方建议以 LUA Scripting 代替事务 主从、哨兵、集群都支持。Spring 也可以配置和注入 RedissonClient。 实现分布式锁在 Redisson 里面提供了更加简单的分布式锁的实现。加锁： 123456789public static void main(String[] args) throws InterruptedException &#123; RLock rLock = redissonClient.getLock(&quot;updateAccount&quot;); // 最多等待 100 秒、上锁 10s 以后自动解锁 if (rLock.tryLock(100, 10, TimeUnit.SECONDS)) &#123; System.out.println(&quot;获取锁成功&quot;); &#125; // do something rLock.unlock();&#125; 在获得 RLock 之后，只需要一个 tryLock 方法，里面有 3 个参数: watiTime:获取锁的最大等待时间，超过这个时间不再尝试获取锁 leaseTime:如果没有调用 unlock，超过了这个时间会自动释放锁 TimeUnit:释放时间的单位 Redisson 的分布式锁是怎么实现的呢?在加锁的时候，在 Redis 写入了一个 HASH，key 是锁名称，field 是线程名称，value 是 1(表示锁的重入次数)。源码路径：tryLock()——tryAcquire()——tryAcquireAsync()——tryLockInnerAsync()最终也是调用了一段 Lua 脚本。 123456789101112131415161718192021// KEYS[1] 锁名称 updateAccount // ARGV[1] key 过期时间 10000ms // ARGV[2] 线程名称// 锁名称不存在if (redis.call(&apos;exists&apos;, KEYS[1]) == 0) then // 创建一个 hash，key=锁名称，field=线程名，value=1 redis.call(&apos;hset&apos;, KEYS[1], ARGV[2], 1); // 设置 hash 的过期时间 redis.call(&apos;pexpire&apos;, KEYS[1], ARGV[1]); return nil;end;// 锁名称存在，判断是否当前线程持有的锁if (redis.call(&apos;hexists&apos;, KEYS[1], ARGV[2]) == 1) then // 如果是，value+1，代表重入次数+1 redis.call(&apos;hincrby&apos;, KEYS[1], ARGV[2], 1); // 重新获得锁，需要重新设置 Key 的过期时间 redis.call(&apos;pexpire&apos;, KEYS[1], ARGV[1]); return nil;end;// 锁存在，但是不是当前线程持有，返回过期时间(毫秒) return redis.call(&apos;pttl&apos;, KEYS[1]); 释放锁，源码:unlock——unlockInnerAsync 1234567891011121314151617181920212223242526272829303132// KEYS[1] 锁的名称 updateAccount// KEYS[2] 频道名称 redisson_lock__channel:&#123;updateAccount&#125; // ARGV[1] 释放锁的消息 0// ARGV[2] 锁释放时间 10000// ARGV[3] 线程名称// 锁不存在(过期或者已经释放了)if (redis.call(&apos;exists&apos;, KEYS[1]) == 0) then // 发布锁已经释放的消息 redis.call(&apos;publish&apos;, KEYS[2], ARGV[1]); return 1;end;// 锁存在，但是不是当前线程加的锁if (redis.call(&apos;hexists&apos;, KEYS[1], ARGV[3]) == 0) then return nil; end;// 锁存在，是当前线程加的锁// 重入次数-1local counter = redis.call(&apos;hincrby&apos;, KEYS[1], ARGV[3], -1);// -1 后大于 0，说明这个线程持有这把锁还有其他的任务需要执行 if (counter &gt; 0) then // 重新设置锁的过期时间 redis.call(&apos;pexpire&apos;, KEYS[1], ARGV[2]); return 0;else // -1 之后等于 0，现在可以删除锁了 redis.call(&apos;del&apos;, KEYS[1]); // 删除之后发布释放锁的消息 redis.call(&apos;publish&apos;, KEYS[2], ARGV[1]); return 1;end;// 其他情况返回 nil return nil; 这个是 Redisson 里面分布式锁的实现，我们在调用的时候非常简单。Redisson 跟 Jedis 定位不同，它不是一个单纯的 Redis 客户端，而是基于 Redis 实现的分布式的服务，如果有需要用到一些分布式的数据结构，比如我们还可以基于 Redisson 的分布式队列实现分布式事务，就可以引入 Redisson 的依赖实现。 数据一致性缓存使用场景针对读多写少的高并发场景，我们可以使用缓存来提升查询速度。当我们使用 Redis 作为缓存的时候，一般流程是这样的:1、如果数据在 Redis 存在，应用就可以直接从 Redis 拿到数据，不用访问数据库。2、如果 Redis 里面没有，先到数据库查询，然后写入到 Redis，再返回给应用。 一致性问题的定义因为这些数据是很少修改的，所以在绝大部分的情况下可以命中缓存。但是，一旦 被缓存的数据发生变化的时候，我们既要操作数据库的数据，也要操作 Redis 的数据， 所以问题来了。现在我们有两种选择: 先操作 Redis 的数据再操作数据库的数据 先操作数据库的数据再操作 Redis 的数据 首先需要明确的是，不管选择哪一种方案， 我们肯定是希望两个操作要么都成功，要么都一个都不成功。不然就会发生 Redis 跟数据库的数据不一致的问题。但是，Redis 的数据和数据库的数据是不可能通过事务达到统一的，我们只能根据相应的场景和所需要付出的代价来采取一些措施降低数据不一致的问题出现的概率，在数据一致性和性能之间取得一个权衡。对于数据库的实时性一致性要求不是特别高的场合，比如 T+1 的报表，可以采用定时任务查询数据库数据同步到 Redis 的方案。由于我们是以数据库的数据为准的，所以给缓存设置一个过期时间，是保证最终一致性的解决方案。 方案选择Redis:删除还是更新?这里我们先要补充一点，当存储的数据发生变化，Redis 的数据也要更新的时候，我们有两种方案，一种就是直接更新，调用 set;还有一种是直接删除缓存，让应用在下次查询的时候重新写入。这两种方案怎么选择呢?这里我们主要考虑更新缓存的代价。更新缓存之前，是不是要经过其他表的查询、接口调用、计算才能得到最新的数据， 而不是直接从数据库拿到的值。如果是的话，建议直接删除缓存，这种方案更加简单， 而且避免了数据库的数据和缓存不一致的情况。在一般情况下，我们也推荐使用删除的方案。 先更新数据库，再删除缓存正常情况: 更新数据库，成功。 删除缓存，成功。 异常情况: 更新数据库失败，程序捕获异常，不会走到下一步，所以数据不会出现不一致。 更新数据库成功，删除缓存失败。数据库是新数据，缓存是旧数据，发生了不一致的情况。 这种问题怎么解决呢?我们可以提供一个重试的机制。比如:如果删除缓存失败，我们捕获这个异常，把需要删除的 key 发送到消息队列。 让后自己创建一个消费者消费，尝试再次删除这个 key。这种方式有个缺点，会对业务代码造成入侵。所以我们又有了第二种方案(异步更新缓存):因为更新数据库时会往 binlog 写入日志，所以我们可以通过一个服务来监听 binlog 的变化(比如阿里的 canal)，然后在客户端完成删除 key 的操作。如果删除失败的话，再发送到消息队列。总之，对于后删除缓存失败的情况，我们的做法是不断地重试删除，直到成功。无论是重试还是异步删除，都是最终一致性的思想。 先删除缓存，再更新数据库正常情况: 删除缓存，成功。 更新数据库，成功。 异常情况: 删除缓存，程序捕获异常，不会走到下一步，所以数据不会出现不一致。 删除缓存成功，更新数据库失败。 因为以数据库的数据为准，所以不存在数据不一致的情况。 看起来好像没问题，但是如果有程序并发操作的情况下: 线程 A 需要更新数据，首先删除了 Redis 缓存 线程 B 查询数据，发现缓存不存在，到数据库查询旧值，写入 Redis，返回 线程 A 更新了数据库 这个时候，Redis 是旧的值，数据库是新的值，发生了数据不一致的情况。那问题就变成了:能不能让对同一条数据的访问串行化呢?代码肯定保证不了，因为有多个线程，即使做了任务队列也可能有多个服务实例。数据库也保证不了，因为会有多个数据库的连接。只有一个数据库只提供一个连接的情况下，才能保证读写的操作是串行的，或者我们把所有的读写请求放到同一个内存队列当中，但是这种情况吞吐量 太低了。所以我们有一种延时双删的策略，在写入数据之后，再删除一次缓存。 删除缓存 更新数据库 休眠 500ms(这个时间，依据读取数据的耗时而定) 再次删除缓存 高并发问题在 Redis 存储的所有数据中，有一部分是被频繁访问的。有两种情况可能会导致热点问题的产生，一个是用户集中访问的数据，比如抢购的商品，明星结婚和明星出轨的微博。还有一种就是在数据进行分片的情况下，负载不均衡，超过了单个服务器的承受能力。热点问题可能引起缓存服务的不可用，最终造成压力堆积到数据库。出于存储和流量优化的角度，我们必须要找到这些热点数据。 热点数据发现除了自动的缓存淘汰机制之外，怎么找出那些访问频率高的 key 呢?或者说，我们可以在哪里记录 key 被访问的情况呢? 客户端第一个当然是在客户端了，比如我们可不可以在所有调用了 get、set 方法的地方，加上 key 的计数。但是这样的话，每一个地方都要修改，重复的代码也多。如果我们用的是 Jedis 的客户端，我们可以在 Jedis 的 Connection 类的 sendCommand()里面，用 一个 HashMap 进行 key 的计数。但是这种方式有几个问题: 不知道要存多少个 key，可能会发生内存泄露的问题。 会对客户端的代码造成入侵。 只能统计当前客户端的热点 key。 代理层第二种方式就是在代理端实现，比如 TwemProxy 或者 Codis，但是不是所有的项目都使用了代理的架构。 服务端第三种就是在服务端统计，Redis 有一个 monitor 的命令，可以监控到所有 Redis 执行的命令。 123456jedis.monitor(new JedisMonitor() &#123; @Override public void onCommand(String command) &#123; System.out.println(&quot;#monitor: &quot; + command); &#125;&#125;); Facebook 的开源项目 redis-faina 就是基于这个原理实现的。 它是一个 python 脚本，可以分析 monitor 的数据。 1redis-cli -p 6379 monitor | head -n 100000 | ./redis-faina.py 这种方法也会有两个问题: monitor 命令在高并发的场景下，会影响性能，所以 不适合长时间使用。 只能统计一个 Redis 节点的热点 key。 机器层面还有一种方法就是机器层面的，通过对 TCP 协议进行抓包，也有一些开源的方案，比如 ELK 的 packetbeat 插件。当我们发现了热点 key 之后，我们来看下热点数据在高并发的场景下可能会出现的问题，以及怎么去解决。 缓存雪崩什么是缓存雪崩缓存雪崩就是 Redis 的大量热点数据同时过期(失效)，因为设置了相同的过期时间，刚好这个时候 Redis 请求的并发量又很大，就会导致所有的请求落到数据库。 缓存雪崩的解决方案 加互斥锁或者使用队列，针对同一个 key 只允许一个线程到数据库查询 缓存定时预先更新，避免同时失效 通过加随机数，使 key 在不同的时间过期 缓存永不过期 缓存穿透缓存穿透何时发生我们已经知道了 Redis 使用的场景了。在缓存存在和缓存不存在的情况下的什么情况我们都了解了。还有一种情况，数据在数据库和 Redis 里面都不存在，可能是一次条件错误的查询。在这种情况下，因为数据库值不存在，所以肯定不会写入 Redis，那么下一次查询相同的 key 的时候，肯定还是会再到数据库查一次。那么这种循环查询数据库中不存在的值，并且每次使用的是相同的 key 的情况，我们有没有什么办法避免应用到数据库查询呢? 缓存空数据 缓存特殊字符串，比如&amp;&amp; 我们可以在数据库缓存一个空字符串，或者缓存一个特殊的字符串，那么在应用里 面拿到这个特殊字符串的候，就知道数据库没有值了，也没有必要再到数据库查询了。但是这里需要设置一个过期时间，不然的话数据库已经新增了这一条记录，应用也还是拿不到值。这个是应用重复查询同一个不存在的值的情况，如果应用每一次查询的不存在的值是不一样的呢?即使你每次都缓存特殊字符串也没用，因为它的值不一样，比如我们的用户系统登录的场景，如果是恶意的请求，它每次都生成了一个符合 ID 规则的账号，但是这个账号在我们的数据库是不存在的，那 Redis 就完全失去了作用。这种因为每次查询的值都不存在导致的 Redis 失效的情况，我们就把它叫做缓存穿透。这个问题我们应该怎么去解决呢? 经典面试题其实它也是一个通用的问题，关键就在于我们怎么知道请求的 key 在我们的数据库里面是否存在，如果数据量特别大的话，我们怎么去快速判断。这也是一个非常经典的面试题:如何在海量元素中(例如 10 亿无序、不定长、不重复)快速判断一个元素是否存在?如果是缓存穿透的这个问题，我们要避免到数据库查询不存的数据，肯定要把这 10 亿放在别的地方。这些数据在 Redis 里面也是没有的，为了加快检索速度，我们要把数据放到内存里面来判断，问题来了:如果我们直接把这些元素的值放到基本的数据结构(List、Map、Tree)里面，比如 一个元素 1 字节的字段，10 亿的数据大概需要 900G 的内存空间，这个对于普通的服务器来说是承受不了的。所以，我们存储这几十亿个元素，不能直接存值，我们应该找到一种最简单的最节省空间的数据结构，用来标记这个元素有没有出现。这个东西我们就把它叫做位图，他是一个有序的数组，只有两个值，0 和 1。0 代表不存在，1 代表存在。对于这个映射方法，我们有几个基本的要求: 因为我们的值长度是不固定的，我希望不同长度的输入，可以得到固定长度的输出。 转换成下标的时候，我希望他在我的这个有序数组里面是分布均匀的，不然的话 全部挤到一对去了，我也没法判断到底哪个元素存了，哪个元素没存。 这个就是哈希函数，比如 MD5、SHA-1 等等这些都是常见的哈希算法。 哈希碰撞如果发生了哈希碰撞，这个时候对于我们的容器存值肯定是有影响的，我们可以通过哪些方式去降低哈希碰撞的概率呢?第一种就是扩大维数组的长度或者说位图容量。因为我们的函数是分布均匀的，所以，位图容量越大，在同一个位置发生哈希碰撞的概率就越小。是不是位图容量越大越好呢?不管存多少个元素，都创建一个几万亿大小的位图， 可以吗？当然不行，因为越大的位图容量，意味着越多的内存消耗，所以我们要创建一个合适大小的位图容量。第二种就是进行多次哈希计算。如果两个元素经过一次哈希计算，得到的相同下标的概率比较高，我可以不可以计 算多次呢? 原来我只用一个哈希函数，现在我对于每一个要存储的元素都用多个哈希函数计算，这样每次计算出来的下标都相同的概率就小得多了。同样的，我们能不能引入很多个哈希函数呢?比如都计算 100 次，都可以吗？当然也会有问题，第一个就是它会填满位图的更多空间，第二个是计算是需要消耗时间的。所以总的来说，我们既要节省空间，又要很高的计算效率，就必须在位图容量和函数个数之间找到一个最佳的平衡。 布隆过滤器关于布隆过滤器可以参考我的另一篇博客：什么是布隆过滤器？]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[什么是布隆过滤器？]]></title>
    <url>%2F2019%2F12%2F01%2F%E4%BB%80%E4%B9%88%E6%98%AF%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[概念布隆过滤器（英语：Bloom Filter）是1970年由一个叫布隆的小伙子提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 原理布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。Bloom Filter跟单哈希函数Bit-Map不同之处在于：Bloom Filter使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。 集合里面有3个元素，要把它存到布隆过滤器里面去，应该怎么做?首先是a元素，这里我们用3次计算。b、c元素也一样。 元素已经存进去之后，现在我要来判断一个元素在这个容器里面是否存在，就要使用同样的三个函数进行计算。 比如d元素，我用第一个函数h1()计算，发现这个位置上是1，没问题。第二个位置也是1，第三个位置也是1。 如果经过三次计算得到的下标位置值都是1，这种情况下，能不能确定d元素一定 在这个容器里面呢? 实际上是不能的。比如这张图里面，这三个位置分别是把a,b,c 存进去的时候置成1的，所以即使d元素之前没有存进去，也会得到三个1，判断返回 true。 我们再来看另一个元素，e元素。我们要判断它在容器里面是否存在，一样地要用这三个函数去计算。第一个位置是1，第二个位置是1，第三个位置是0。 e 元素是不是一定不在这个容器里面呢? 可以确定一定不存在。如果说当时已经把e元素存到布隆过滤器里面去了，那么这三个位置肯定都是1，不可能出现0。 ss### 特点从容器的角度来说: 如果布隆过滤器判断元素在集合中存在，不一定存在 如果布隆过滤器判断不存在，一定不存在 从元素的角度来说: 如果元素实际存在，布隆过滤器一定判断存在 如果元素实际不存在，布隆过滤器可能判断存在 缺点bloom filter之所以能做到在时间和空间上的效率比较高，是因为牺牲了判断的准确率、删除的便利性 存在误判，可能要查到的元素并没有在容器中，但是hash之后得到的k个位置上值都是1。因为哈希碰撞不可避免，所以它会存在一定的误判率。这种把本来不存在布隆过滤器中的元素误判为存在的情况，我们把它叫做假阳性(False Positive Probability，FPP)。 删除困难。一个放入容器的元素映射到bit数组的k个位置上是1，删除的时候不能简单的直接置为0，可能会影响其他元素的判断。可以采用Counting Bloom Filter 实现 在使用bloom filter时，绕不过的两点是预估数据量n以及期望的误判率fpp 在实现bloom filter时，绕不过的两点就是hash函数的选取以及bit数组的大小 对于一个确定的场景，我们预估要存的数据量为n，期望的误判率为fpp，然后需要计算我们需要的Bit数组的大小m，以及hash函数的个数k，并选择hash函数 Bit数组大小选择根据预估数据量n以及误判率fpp，bit数组大小的m的计算方式：位图的容量是基于元素个数和误判率计算出来的。 1long numBits = optimalNumOfBits(expectedInsertions, fpp); 哈希函数选择由预估数据量n以及bit数组长度m，可以得到一个hash函数的个数k：哈希函数的选择对性能的影响应该是很大的，一个好的哈希函数要能近似等概率的将字符串映射到各个Bit。选择k个不同的哈希函数比较麻烦，一种简单的方法是选择一个哈希函数，然后送入k个不同的参数。哈希函数个数k、位数组大小m、加入的字符串数量n的关系可以参考Bloom Filters - the math，Bloom_filter-wikipedia根据位数组的大小，我们进一步计算出了哈希函数的个数。 1int numHashFunctions = optimalNumOfHashFunctions(expectedInsertions, numBits); 空间存储 100 万个元素只占用了 0.87M 的内存，生成了 5 个哈希函数。https://hur.st/bloomfilter/?n=1000000&amp;p=0.03&amp;m=&amp;k= 代码谷歌的 Guava 里面就提供了一个现成的布隆过滤器。 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;23.0&lt;/version&gt;&lt;/dependency&gt; 简单使用布隆过滤器: 123456789101112131415161718192021222324import com.google.common.hash.BloomFilter;import com.google.common.hash.Funnels;public class BloomFilterTest &#123; private static int total = 1000000; private static BloomFilter&lt;Integer&gt; bf = BloomFilter.create(Funnels.integerFunnel(), total); //可以设置你允许的误差率，误差范围：0&lt;fpp&lt;1 //private static BloomFilter&lt;Integer&gt; bf = BloomFilter.create(Funnels.integerFunnel(), total, 0.01); public static void main(String[] args) &#123; // 初始化1000000条数据到过滤器中 for (int i = 0; i &lt; total; i++) &#123; bf.put(i); &#125; // 获取随机数匹配在过滤器中存在 int i = (int)(1+Math.random()*(10000)); if (bf.mightContain(i)) &#123; System.out.println(&quot;BloomFilter 判定存在&quot;); &#125; &#125;&#125; 使用场景 如何在海量元素中快速判断一个元素是否存在（Redis 缓存穿透） 爬虫过滤已抓到的url 垃圾邮件过滤]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL多表关联查询优化]]></title>
    <url>%2F2019%2F11%2F28%2FMySQL%E5%A4%9A%E8%A1%A8%E5%85%B3%E8%81%94%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[背景最近在对运营报表导出进行优化，总结了一些多表关联查询优化的点记录一下。 避免临时表通过 Explain 分析 SQL 语句，尽量不要使用到临时表。GROUP BY （Explain具体详解，可以看这篇博客） 最容易造成使用临时表，GROUP BY 与临时表的关系 : 1. 如果GROUP BY 的列没有索引,产生临时表. 2. 如果GROUP BY时,SELECT的列不止GROUP BY列一个,并且GROUP BY的列不是主键 ,产生临时表. 3. 如果GROUP BY的列有索引,ORDER BY的列没索引.产生临时表. 4. 如果GROUP BY的列和ORDER BY的列不一样,即使都有索引也会产生临时表. 5. 如果GROUP BY或ORDER BY的列不是来自JOIN语句第一个表.会产生临时表. 6. 如果DISTINCT 和 ORDER BY的列没有索引,产生临时表.如果业务需求没法更改，也不需要强制去掉临时表。 缩小数据范围接下来进行优化第二步，将临时表缩小到最小范围。SQL 执行过程大体如下： 执行FROM语句 执行ON过滤 添加外部行 执行where条件过滤 执行group by分组语句 执行having select列表 执行distinct去重复数据 执行order by字句 执行limit字句 当两个表进行Join操作时，主表的Where限制可以写在最后，但从表分区限制条件不要写在Where条件中，建议写在ON条件或者子查询中。主表的分区限制条件可以写在Where条件中（最好先用子查询过滤）。示例如下： 123select * from A join (select * from B where dt=20150301)B on B.id=A.id where A.dt=20150301； select * from A join B on B.id=A.id where B.dt=20150301； --不允许 select * from (select * from A where dt=20150301)A join (select * from B where dt=20150301)B on B.id=A.id； 第二个语句会先Join，后进行分区裁剪，数据量变大，性能下降。在实际使用过程中，应该尽量避免第二种用法。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 单机安装Redis Cluster（3主3从）]]></title>
    <url>%2F2019%2F11%2F26%2FCentOS%207%20%E5%8D%95%E6%9C%BA%E5%AE%89%E8%A3%85Redis%20Cluster%EF%BC%883%E4%B8%BB3%E4%BB%8E%EF%BC%89%2F</url>
    <content type="text"><![CDATA[首先，本篇要基于单实例的安装，你的机器上已经有一个Redis。（还没安装的同学请自行百度） 安装过程为了节省机器，我们直接把6个Redis实例安装在同一台机器上（3主3从），只是使用不同的端口号。机器IP 192.168.8.207 更新：新版的cluster已经不需要通过ruby脚本创建，删掉了ruby相关依赖的安装 1234cd /usr/local/soft/redis-5.0.5mkdir redis-clustercd redis-clustermkdir 7291 7292 7293 7294 7295 7296 复制redis配置文件到7291目录 1cp /usr/local/soft/redis-5.0.5/redis.conf /usr/local/soft/redis-5.0.5/redis-cluster/7291 修改7291的redis.conf配置文件，内容： 1234567port 7291dir /usr/local/soft/redis-5.0.5/redis-cluster/7291/cluster-enabled yescluster-config-file nodes-7291.confcluster-node-timeout 5000appendonly yespidfile /var/run/redis_7291.pid 把7291下的redis.conf复制到其他5个目录。 123456cd /usr/local/soft/redis-5.0.5/redis-cluster/7291cp redis.conf ../7292cp redis.conf ../7293cp redis.conf ../7294cp redis.conf ../7295cp redis.conf ../7296 批量替换内容 123456cd /usr/local/soft/redis-5.0.5/redis-clustersed -i &apos;s/7291/7292/g&apos; 7292/redis.confsed -i &apos;s/7291/7293/g&apos; 7293/redis.confsed -i &apos;s/7291/7294/g&apos; 7294/redis.confsed -i &apos;s/7291/7295/g&apos; 7295/redis.confsed -i &apos;s/7291/7296/g&apos; 7296/redis.conf 启动6个Redis节点 1234567cd /usr/local/soft/redis-5.0.5/./src/redis-server redis-cluster/7291/redis.conf./src/redis-server redis-cluster/7292/redis.conf./src/redis-server redis-cluster/7293/redis.conf./src/redis-server redis-cluster/7294/redis.conf./src/redis-server redis-cluster/7295/redis.conf./src/redis-server redis-cluster/7296/redis.conf 是否启动了6个进程 1ps -ef|grep redis 创建集群旧版本中的redis-trib.rb已经废弃了，直接用–cluster命令注意用绝对IP，不要用127.0.0.1 12cd /usr/local/soft/redis-5.0.5/src/redis-cli --cluster create 192.168.8.207:7291 192.168.8.207:7292 192.168.8.207:7293 192.168.8.207:7294 192.168.8.207:7295 192.168.8.207:7296 --cluster-replicas 1 Redis会给出一个预计的方案，对6个节点分配3主3从，如果认为没有问题，输入yes确认 12345678910111213141516171819202122&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 127.0.0.1:7295 to 127.0.0.1:7291Adding replica 127.0.0.1:7296 to 127.0.0.1:7292Adding replica 127.0.0.1:7294 to 127.0.0.1:7293&gt;&gt;&gt; Trying to optimize slaves allocation for anti-affinity[WARNING] Some slaves are in the same host as their masterM: dfdc9c0589219f727e4fd0ad8dafaf7e0cfb4f1c 127.0.0.1:7291 slots:[0-5460] (5461 slots) masterM: 8c878b45905bba3d7366c89ec51bd0cd7ce959f8 127.0.0.1:7292 slots:[5461-10922] (5462 slots) masterM: aeeb7d7076d9b25a7805ac6f508497b43887e599 127.0.0.1:7293 slots:[10923-16383] (5461 slots) masterS: ebc479e609ff8f6ca9283947530919c559a08f80 127.0.0.1:7294 replicates aeeb7d7076d9b25a7805ac6f508497b43887e599S: 49385ed6e58469ef900ec48e5912e5f7b7505f6e 127.0.0.1:7295 replicates dfdc9c0589219f727e4fd0ad8dafaf7e0cfb4f1cS: 8d6227aefc4830065624ff6c1dd795d2d5ad094a 127.0.0.1:7296 replicates 8c878b45905bba3d7366c89ec51bd0cd7ce959f8Can I set the above configuration? (type &apos;yes&apos; to accept): 注意看slot的分布： 1237291 [0-5460] (5461个槽) 7292 [5461-10922] (5462个槽) 7293 [10923-16383] (5461个槽) 集群创建完成 12345678910111213141516171819202122232425262728&gt;&gt;&gt; Nodes configuration updated&gt;&gt;&gt; Assign a different config epoch to each node&gt;&gt;&gt; Sending CLUSTER MEET messages to join the clusterWaiting for the cluster to join....&gt;&gt;&gt; Performing Cluster Check (using node 127.0.0.1:7291)M: dfdc9c0589219f727e4fd0ad8dafaf7e0cfb4f1c 127.0.0.1:7291 slots:[0-5460] (5461 slots) master 1 additional replica(s)M: 8c878b45905bba3d7366c89ec51bd0cd7ce959f8 127.0.0.1:7292 slots:[5461-10922] (5462 slots) master 1 additional replica(s)M: aeeb7d7076d9b25a7805ac6f508497b43887e599 127.0.0.1:7293 slots:[10923-16383] (5461 slots) master 1 additional replica(s)S: 8d6227aefc4830065624ff6c1dd795d2d5ad094a 127.0.0.1:7296 slots: (0 slots) slave replicates aeeb7d7076d9b25a7805ac6f508497b43887e599S: ebc479e609ff8f6ca9283947530919c559a08f80 127.0.0.1:7294 slots: (0 slots) slave replicates dfdc9c0589219f727e4fd0ad8dafaf7e0cfb4f1cS: 49385ed6e58469ef900ec48e5912e5f7b7505f6e 127.0.0.1:7295 slots: (0 slots) slave replicates 8c878b45905bba3d7366c89ec51bd0cd7ce959f8[OK] All nodes agree about slots configuration.&gt;&gt;&gt; Check for open slots...&gt;&gt;&gt; Check slots coverage...[OK] All 16384 slots covered. 重置集群的方式是在每个节点上个执行cluster reset，然后重新创建集群 连接到客户端 123redis-cli -p 7291redis-cli -p 7292redis-cli -p 7293 批量写入值 12cd /usr/local/soft/redis-5.0.5/redis-cluster/vim setkey.sh 脚本内容 1234567#!/bin/bashfor ((i=0;i&lt;20000;i++))doecho -en &quot;helloworld&quot; | redis-cli -h 192.168.8.207 -p 7291 -c -x set name$i &gt;&gt;redis.logdonechmod +x setkey.sh./setkey.sh 每个节点分布的数据 123456127.0.0.1:7292&gt; dbsize(integer) 6683127.0.0.1:7293&gt; dbsize(integer) 6665127.0.0.1:7291&gt; dbsize(integer) 6652 其他命令，比如添加节点、删除节点，重新分布数据： 1234567891011121314151617181920212223242526272829303132333435363738redis-cli --cluster helpCluster Manager Commands: create host1:port1 ... hostN:portN --cluster-replicas &lt;arg&gt; check host:port --cluster-search-multiple-owners info host:port fix host:port --cluster-search-multiple-owners reshard host:port --cluster-from &lt;arg&gt; --cluster-to &lt;arg&gt; --cluster-slots &lt;arg&gt; --cluster-yes --cluster-timeout &lt;arg&gt; --cluster-pipeline &lt;arg&gt; --cluster-replace rebalance host:port --cluster-weight &lt;node1=w1...nodeN=wN&gt; --cluster-use-empty-masters --cluster-timeout &lt;arg&gt; --cluster-simulate --cluster-pipeline &lt;arg&gt; --cluster-threshold &lt;arg&gt; --cluster-replace add-node new_host:new_port existing_host:existing_port --cluster-slave --cluster-master-id &lt;arg&gt; del-node host:port node_id call host:port command arg arg .. arg set-timeout host:port milliseconds import host:port --cluster-from &lt;arg&gt; --cluster-copy --cluster-replace help For check, fix, reshard, del-node, set-timeout you can specify the host and port of any working node in the cluster. 附录集群命令 123456cluster info ：打印集群的信息cluster nodes ：列出集群当前已知的所有节点（node），以及这些节点的相关信息。cluster meet ：将 ip 和 port 所指定的节点添加到集群当中，让它成为集群的一份子。cluster forget &lt;node_id&gt; ：从集群中移除 node_id 指定的节点(保证空槽道)。cluster replicate &lt;node_id&gt; ：将当前节点设置为 node_id 指定的节点的从节点。cluster saveconfig ：将节点的配置文件保存到硬盘里面。 槽slot命令 1234567cluster addslots [slot …] ：将一个或多个槽（slot）指派（assign）给当前节点。cluster delslots [slot …] ：移除一个或多个槽对当前节点的指派。cluster flushslots ：移除指派给当前节点的所有槽，让当前节点变成一个没有指派任何槽的节点。cluster setslot node &lt;node_id&gt; ：将槽 slot 指派给 node_id 指定的节点，如果槽已经指派给另一个节点，那么先让另一个节点删除该槽&gt;，然后再进行指派。cluster setslot migrating &lt;node_id&gt; ：将本节点的槽 slot 迁移到 node_id 指定的节点中。cluster setslot importing &lt;node_id&gt; ：从 node_id 指定的节点中导入槽 slot 到本节点。cluster setslot stable ：取消对槽 slot 的导入（import）或者迁移（migrate）。 键命令 123cluster keyslot ：计算键 key 应该被放置在哪个槽上。cluster countkeysinslot ：返回槽 slot 目前包含的键值对数量。cluster getkeysinslot ：返回 count 个 slot 槽中的键]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解Redis-集群篇]]></title>
    <url>%2F2019%2F11%2F25%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Redis-%E9%9B%86%E7%BE%A4%E7%AF%87%2F</url>
    <content type="text"><![CDATA[为什么需要 Redis 集群？性能Redis 本身的 QPS 已经很高了，但是如果在一些并发量非常高的情况下，性能还是会受到影响。这个时候我们希望有更多的 Redis 服务来完成工作。 扩展第二个是出于存储的考虑。因为 Redis 所有的数据都放在内存中，如果数据量大，很容易受到硬件的限制。升级硬件收效和成本比太低，所以我们需要有一种横向扩展的方法。 可用性第三个是可用性和安全的问题。如果只有一个 Redis 服务，一旦服务宕机，那么所有的客户端都无法访问，会对业务造成很大的影响。另一个，如果硬件发生故障，而单 机的数据无法恢复的话，带来的影响也是灾难性的。 总结可用性、数据安全、性能都可以通过搭建多个 Reids 服务实现。其中有一个是主节点(master)，可以有多个从节点(slave)。主从之间通过数据同步，存储完全相同的数据。如果主节点发生故障，则把某个从节点改成主节点，访问新的主节点。 Redis 主从复制(replication)配置例如一主多从,203 是主节点，在每个 slave 节点的 redis.conf 配置文件增加一行 1slaveof 192.168.8.203 6379 在主从切换的时候，这个配置会被重写成: 12# Generated by CONFIG REWRITEreplicaof 192.168.8.203 6379 或者在启动服务时通过参数指定 master 节点: 1./redis-server --slaveof 192.168.8.203 6379 或在客户端直接执行 slaveof xx xx，使该 Redis 实例成为从节点。启动后，查看集群状态: 1redis&gt; info replication 从节点不能写入数据(只读)，只能从 master 节点同步数据。get 成功，set 失败。 12127.0.0.1:6379&gt; set test 666(error) READONLY You can&apos;t write against a read only replica. 主节点写入后，slave 会自动从 master 同步数据。断开主节点: 1redis&gt; slaveof no one 此时从节点会变成自己的主节点，不再复制数据。 原理连接阶段1、slave node 启动时(执行 slaveof 命令)，会在自己本地保存 master node 的信息，包括 master node 的 host 和 ip。2、slave node 内部有个定时任务 replicationCron(源码 replication.c)，每隔 1 秒钟检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络接，如果连接成功，从节点为该 socket 建立一个专门处理复制工作的文件 事件处理器，负责后续的复制工作，如接收 RDB 文件、接收命令传播等。当从节点变成了主节点的一个客户端之后，会给主节点发送 ping 请求。 数据同步阶段3、master node 第一次执行全量复制，通过 bgsave 命令在本地生成一份 RDB 快照，将 RDB 快照文件发给 slave node(如果超时会重连，可以调大 repl-timeout 的值)。 slave node 首先清除自己的旧数据，然后用 RDB 文件加载数据。 生成 RDB 期间，master 接收到的命令怎么处理?开始生成 RDB 文件时，master 会把所有新的写命令缓存在内存中。在 slave node 保存了 RDB 之后，再将新的写命令复制给 slave node。 命令传播阶段4、master node 持续将写命令，异步复制给 slave node延迟是不可避免的，只能通过优化网络。 1repl-disable-tcp-nodelay no 当设置为 yes 时，TCP 会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差;具体发送频率与 Linux 内核的配置有关，默认配置为 40ms。当设置为 no 时，TCP 会立马将主节点的数据发送给从节点，带宽增加但延迟变小。一般来说，只有当应用对 Redis 数据不一致的容忍度较高，且主从节点之间网络状况不好时，才会设置为 yes;多数情况使用默认值 no。如果从节点有一段时间断开了与主节点的连接是不是要重新全量复制一遍? 如果可以增量复制，怎么知道上次复制到哪里?通过 master_repl_offset 记录的偏移量 12redis&gt; info replicationmaster_repl_offset:7324993 不足主从模式解决了数据备份和性能(通过读写分离)的问题，但是还是存在一些不足: RDB 文件过大的情况下，同步非常耗时。 在一主一从或者一主多从的情况下，如果主服务器挂了，对外提供的服务就不可 用了，单点问题没有得到解决。如果每次都是手动把之前的从服务器切换成主服务器， 这个比较费时费力，还会造成一定时间的服务不可用。 SentinelSentinel原理如何实现主从的自动切换?我们的思路:创建一台监控服务器来监控所有 Redis 服务节点的状态，比如，master 节点超过一定时间没有给监控服务器发送心跳报文，就把 master 标记为下线，然后把某一个 slave 变成 master。应用每一次都是从这个监控服务器拿到 master 的地址。问题是:如果监控服务器本身出问题了怎么办?那我们就拿不到 master 的地址了，应用也没有办法访问。那我们再创建一个监控服务器，来监控监控服务器……似乎陷入死循环了，这个问题 怎么解决?这个问题先放着。Redis 的 Sentinel 就是这种思路:通过运行监控服务器来保证服务的可用性。从 Redis2.8 版本起，提供了一个稳定版本的 Sentinel(哨兵)，用来解决高可用的问题。它是一个特殊状态的 redis 实例。我们会启动一个或者多个 Sentinel 的服务(通过 src/redis-sentinel)，它本质上只是一个运行在特殊模式之下的 Redis，Sentinel 通过 info 命令得到被监听 Redis 机器的 master，slave 等信息。为了保证监控服务器的可用性，我们会对 Sentinel 做集群的部署。Sentinel 既监控所有的 Redis 服务，Sentinel 之间也相互监控。注意:Sentinel 本身没有主从之分，只有 Redis 服务节点有主从之分。概念梳理:master，slave(redis group)，sentinel，sentinel 集合 服务下线Sentinel 默认以每秒钟 1 次的频率向 Redis 服务节点发送 PING 命令。如果在 down-after-milliseconds 内都没有收到有效回复，Sentinel 会将该服务器标记为下线 (主观下线)。 12# sentinel.confsentinel down-after-milliseconds &lt;master-name&gt; &lt;milliseconds&gt; 这个时候 Sentinel 节点会继续询问其他的 Sentinel 节点，确认这个节点是否下线，如果多数 Sentinel 节点都认为 master 下线，master 才真正确认被下线(客观下线)，这个时候就需要重新选举 master。 故障转移如果 master 被标记为下线，就会开始故障转移流程。既然有这么多的 Sentinel 节点，由谁来做故障转移的事情呢?故障转移流程的第一步就是在 Sentinel 集群选择一个 Leader，由 Leader 完成故障转移流程。Sentinle 通过 Raft 算法，实现 Sentinel 选举。 Ratf 算法在分布式存储系统中，通常通过维护多个副本来提高系统的可用性，那么多个节点之间必须要面对数据一致性的问题。Raft 的目的就是通过复制的方式，使所有节点达成一致，但是这么多节点，以哪个节点的数据为准呢?所以必须选出一个 Leader。大体上有两个步骤:领导选举，数据复制。Raft 是一个共识算法(consensus algorithm)。比如比特币之类的加密货币，就 需要共识算法。Spring Cloud 的注册中心解决方案 Consul 也用到了 Raft 协议。Raft 的核心思想:先到先得，少数服从多数。Raft 算法演示:http://thesecretlivesofdata.com/raft/Sentinle 的 Raft 算法和 Raft 论文略有不同。 master 客观下线触发选举，而不是过了 election timeout 时间开始选举。 Leader 并不会把自己成为 Leader 的消息发给其他 Sentinel。其他 Sentinel 等待 Leader 从 slave 选出 master 后，检测到新的 master 正常工作后，就会去掉客观下线的标识，从而不需要进入故障转移流程。 问题:怎么让一个原来的 slave 节点成为主节点? 选出 Sentinel Leader 之后，由 Sentinel Leader 向某个节点发送 slaveof no one 命令，让它成为独立节点。 然后向其他节点发送 slaveof x.x.x.x xxxx(本机服务)，让它们成为这个节点的子节点，故障转移完成。 问题:这么多从节点，选谁成为主节点?关于从节点选举，一共有四个因素影响选举的结果，分别是断开连接时长、优先级 排序、复制数量、进程id。如果与哨兵连接断开的比较久，超过了某个阈值，就直接失去了选举权。如果拥有选举权，那就看谁的优先级高，这个在配置文件里可以设置(replica-priority 100)， 数值越小优先级越高。如果优先级相同，就看谁从 master 中复制的数据最多(复制偏移量最大)，选最多的那个，如果复制数量也相同，就选择进程 id 最小的那个。 功能总结 监控:Sentinel 会不断检查主服务器和从服务器是否正常运行。 通知:如果某一个被监控的实例出现问题，Sentinel 可以通过 API 发出通知。 自动故障转移(failover):如果主服务器发生故障，Sentinel 可以启动故障转移过程。把某台服务器升级为主服务器，并发出通知。 配置管理:客户端连接到 Sentinel，获取当前的 Redis 主服务器的地址。 Sentinel实战Sentinel 配置为了保证 Sentinel 的高可用，Sentinel 也需要做集群部署，集群中至少需要三个 Sentinel 实例(推荐奇数个，防止脑裂)。 hostname IP 地址 节点角色&amp;端口 master 192.168.8.203 Master:6379 / Sentinel : 26379 slave1 192.168.8.204 Slave :6379 / Sentinel : 26379 Slave2 192.168.8.205 Slave :6379 / Sentinel : 26379 以 Redis 安装路径/usr/local/soft/redis-5.0.5/为例。 在 204 和 205 的 src/redis.conf 配置文件中添加 1slaveof 192.168.8.203 6379 在203、204、205 创建 sentinel 配置文件( 安装后根目录下默认有 sentinel.conf ): 1234cd /usr/local/soft/redis-5.0.5 mkdir logsmkdir rdbsmkdir sentinel-tmpvim sentinel.conf 三台服务器内容相同: 12345daemonize yesport 26379protected-mode nodir &quot;/usr/local/soft/redis-5.0.5/sentinel-tmp&quot;sentinel monitor redis-master 192.168.8.203 6379 2 sentinel down-after-milliseconds redis-master 30000 sentinel failover-timeout redis-master 180000 sentinel parallel-syncs redis-master 1 上面出现了 4 个’redis-master’，这个名称要统一，并且使用客户端(比如 Jedis) 连接的时候名称要正确。 hostname IP 地址 protected-mode 是否允许外部网络访问 dir sentinel 的工作目录 sentinel monitor sentinel 监控的 redis 主节点 down-after-milliseconds(毫秒) master 宕机多久，才会被 Sentinel 主观认为下线 sentinel failover-timeout(毫秒) 1 同一个 sentinel 对同一个 master 两次 failover 之间的间隔时间。2. 当一个 slave 从一个错误的 master 那里同步数据开始计算时间。直到 slave 被纠正为向正确的 master 那里同步数据时。3.当想要取消一个正在进行的 failover 所需要的时间。4.当进行 failover 时，配置所有 slaves 指向新的 master 所需的最大时间。 parallel-syncs 这个配置项指定了在发生 failover 主备切换时最多可以有多少个 slave 同 时对新的 master 进行 同步，这个数字越小，完成 failover 所需的时间就 越长，但是如果这个数字越大，就意味着越 多的 slave 因为 replication 而 不可用。可以通过将这个值设为 1 来保证每次只有一个 slave 处于不能处 理命令请求的状态。 Sentinel 验证启动 Redis 服务和 Sentinel 1234567cd /usr/local/soft/redis-5.0.5/src # 启动 Redis 节点 ./redis-server ../redis.conf# 启动 Sentinel 节点 ./redis-sentinel ../sentinel.conf # 或者./redis-server ../sentinel.conf --sentinel 查看集群状态: 1redis&gt; info replication 模拟 master 宕机，在 203 执行: 1redis&gt; shutdown 205 被选为新的 Master，只有一个 Slave 节点。注意看 sentinel.conf 里面的 redis-master 被修改了! 模拟原 master 恢复，在 203 启动 redis-server。它还是 slave，但是 master 又有 两个 slave 了。 Sentinel 连接使用Jedis 连接 Sentinel， 来自于 sentinel.conf 的配置。 123456789private static JedisSentinelPool createJedisPool() &#123; String masterName = &quot;redis-master&quot;; Set&lt;String&gt; sentinels = new HashSet&lt;String&gt;(); sentinels.add(&quot;192.168.8.203:26379&quot;); sentinels.add(&quot;192.168.8.204:26379&quot;); sentinels.add(&quot;192.168.8.205:26379&quot;); pool = new JedisSentinelPool(masterName, sentinels); return pool; &#125; Spring Boot 连接 Sentinel 1spring.redis.sentinel.master=redis-master spring.redis.sentinel.nodes=192.168.8.203:26379,192.168.8.204:26379,192.168.8.205:26379 无论是 Jedis 还是 Spring Boot(2.x 版本默认是 Lettuce)，都只需要配置全部哨 兵的地址，由哨兵返回当前的 master 节点地址。 哨兵机制的不足 主从切换的过程中会丢失数据，因为只有一个 master。 只能单点写，没有解决水平扩容的问题。 如果数据量非常大，这个时候我们需要多个 master-slave 的 group，把数据分布到不同的 group 中。 Redis 分布式方案如果要实现 Redis 数据的分片，我们有三种方案。第一种是在客户端实现相关的逻辑，例如用取模或者一致性哈希对 key 进行分片，查询和修改都先判断 key 的路由。第二种是把做分片处理的逻辑抽取出来，运行一个独立的代理服务，客户端连接到这个代理服务，代理服务做请求的转发。第三种就是基于服务端实现。 客户端 ShardingJedis 客户端提供了 Redis Sharding 的方案，并且支持连接池。 ShardedJedis1234567891011121314151617181920212223242526public class ShardingTest &#123; public static void main(String[] args) &#123; JedisPoolConfig poolConfig = new JedisPoolConfig(); ​ // Redis 服务器 JedisShardInfo shardInfo1 = new JedisShardInfo(&quot;127.0.0.1&quot;, 6379); JedisShardInfo shardInfo2 = new JedisShardInfo(&quot;192.168.8.205&quot;, 6379);​ // 连接池 List&lt;JedisShardInfo&gt; infoList = Arrays.asList(shardInfo1, shardInfo2); ShardedJedisPool jedisPool = new ShardedJedisPool(poolConfig, infoList); ShardedJedis jedis = null; try &#123; jedis = jedisPool.getResource(); for (int i = 0; i &lt; 100; i++) &#123; jedis.set(&quot;k&quot; + i, &quot;&quot; + i); &#125; for (int i = 0; i &lt; 100; i++) &#123; System.out.println(jedis.get(&quot;k&quot; + i)); &#125; ​ &#125; finally &#123; if (jedis != null) &#123; jedis.close(); &#125; &#125; &#125;&#125; 使用 ShardedJedis 之类的客户端分片代码的优势是配置简单，不依赖于其他中间件，分区的逻辑可以自定义，比较灵活。但是基于客户端的方案，不能实现动态的服务增减，每个客户端需要自行维护分片策略，存在重复代码。 代理 Proxy第二种思路就是把分片的代码抽取出来，做成一个公共服务，所有的客户端都连接 到这个代理层。由代理层来实现请求和转发。典型的代理分区方案有 Twitter 开源的 Twemproxy 和国内的豌豆荚开源的 Codis。 Twemproxytwo-em-proxy：https://github.com/twitter/twemproxyTwemproxy 的优点:比较稳定，可用性高。不足:1、出现故障不能自动转移，架构复杂，需要借助其他组件(LVS/HAProxy + Keepalived)实现 HA2、扩缩容需要修改配置，不能实现平滑地扩缩容(需要重新分布数据)。 Codishttps://github.com/CodisLabs/codisCodis 是一个代理中间件，用 Go 语言开发的。功能:客户端连接 Codis 跟连接 Redis 没有区别。 Codis Twemproxy Redis Cluster 重新分片不需要重启 Yes No Yes pipeline Yes Yes No 多 key 操作的 hash tags {} Yes Yes Yes 重新分片时的多 key 操作 Yes - No 客户端支持 所有 所有 支持 cluster 协议的客户 端 分片原理:Codis 把所有的 key 分成了 N 个槽(例如 1024)，每个槽对应一个分组， 一个分组对应于一个或者一组 Redis 实例。Codis 对 key 进行 CRC32 运算，得到一个 32 位的数字，然后模以 N(槽的个数)，得到余数，这个就是 key 对应的槽，槽后面就 是 Redis 的实例。比如 4 个槽:Codis 的槽位映射关系是保存在 Proxy 中的，如果要解决单点的问题，Codis 也要做集群部署，多个Codis节点怎么同步槽和实例的关系呢?需要运行一个Zookeepe(r 或者 etcd/本地文件)。在新增节点的时候，可以为节点指定特定的槽位。Codis 也提供了自动均衡策略。Codis 不支持事务，其他的一些命令也不支持。不支持的命令https://github.com/CodisLabs/codis/blob/release3.2/doc/unsupported_cmds.md获取数据原理(mget):在 Redis 中的各个实例里获取到符合的 key，然后再汇总 到 Codis 中。Codis 是第三方提供的分布式解决方案，在官方的集群功能稳定之前，Codis 也得到 了大量的应用。 Redis Clusterhttps://redis.io/topics/cluster-tutorial/Redis Cluster 是在 Redis 3.0 的版本正式推出的，用来解决分布式的需求，同时也 可以实现高可用。跟 Codis 不一样，它是去中心化的，客户端可以连接到任意一个可用节点。数据分片有几个关键的问题需要解决: 数据怎么相对均匀地分片 客户端怎么访问到相应的节点和数据 重新分片的过程，怎么保证正常服务架构Redis Cluster 可以看成是由多个 Redis 实例组成的数据集合。客户端不需要关注数据的子集到底存储在哪个节点，只需要关注这个集合整体。以 3 主 3 从为例，节点之间两两交互，共享数据分片、节点状态等信息。数据分布如果是希望数据分布相对均匀的话，我们首先可以考虑哈希后取模。哈希后取模例如，hash(key)%N，根据余数，决定映射到那一个节点。这种方式比较简单，属于静态的分片规则。但是一旦节点数量变化，新增或者减少，由于取模的 N 发生变化，数据需要重新分布。为了解决这个问题，我们又有了一致性哈希算法。一致性哈希一致性哈希的原理:把所有的哈希值空间组织成一个虚拟的圆环(哈希环)，整个空间按顺时针方向组 织。因为是环形空间，0 和 2^32-1 是重叠的。假设我们有四台机器要哈希环来实现映射(分布数据)，我们先根据机器的名称或 者 IP 计算哈希值，然后分布到哈希环中(红色圆圈)。现在有 4 条数据或者 4 个访问请求，对 key 计算后，得到哈希环中的位置(绿色圆圈)。沿哈希环顺时针找到的第一个 Node，就是数据存储的节点。在这种情况下，新增了一个 Node5 节点，不影响数据的分布。删除了一个节点 Node4，只影响相邻的一个节点。谷歌的 MurmurHash 就是一致性哈希算法。在分布式系统中，负载均衡、分库分表 等场景中都有应用。一致性哈希解决了动态增减节点时，所有数据都需要重新分布的问题，它只会影响到下一个相邻的节点，对其他节点没有影响。但是这样的一致性哈希算法有一个缺点，因为节点不一定是均匀地分布的，特别是在节点数比较少的情况下，所以数据不能得到均匀分布。解决这个问题的办法是引入虚 拟节点(Virtual Node)。比如:2 个节点，5 条数据，只有 1 条分布到 Node2，4 条分布到 Node1，不均匀。Node1 设置了两个虚拟节点，Node2 也设置了两个虚拟节点(虚线圆圈)。 这时候有 3 条数据分布到 Node1，1 条数据分布到 Node2。Redis 虚拟槽分区Redis 既没有用哈希取模，也没有用一致性哈希，而是用虚拟槽来实现的。Redis 创建了 16384 个槽(slot)，每个节点负责一定区间的 slot。比如 Node1 负责 0-5460，Node2 负责 5461-10922，Node3 负责 10923-16383。Redis 的每个 master 节点维护一个 16384 位(2048bytes=2KB)的位序列，比如:序列的第 0 位是 1，代表第一个 slot 是它负责;序列的第 1 位是 0，代表第二个 slot 不归它负责。对象分布到 Redis 节点上时，对 key 用 CRC16 算法计算再%16384，得到一个 slot 的值，数据落到负责这个 slot 的 Redis 节点上。查看 key 属于哪个 slot:1redis&gt; cluster keyslot test 注意:key 与 slot 的关系是永远不会变的，会变的只有 slot 和 Redis 节点的关系。问题:怎么让相关的数据落到同一个节点上?比如有些 multi key 操作是不能跨节点的，如果要让某些数据分布到一个节点上，例 如用户 2673 的基本信息和金融信息，怎么办?在 key 里面加入{hash tag}即可。Redis 在计算槽编号的时候只会获取{}之间的字符 串进行槽编号计算，这样由于上面两个不同的键，{}里面的字符串是相同的，因此他们可 以被计算出相同的槽。 12set user&#123;2673&#125;base ... set user&#123;2673&#125;fin ... 问题:客户端连接到哪一台服务器?访问的数据不在当前节点上，怎么办?比如在 7291 端口的 Redis 的 redis-cli 客户端操作: 12127.0.0.1:7291&gt; set ts 1(error) MOVED 13724 127.0.0.1:7293 服务端返回 MOVED，也就是根据 key 计算出来的 slot 不归 7191 端口管理，而是 归 7293 端口管理，服务端返回 MOVED 告诉客户端去 7293 端口操作。这个时候更换端口，用 redis-cli –p 7293 操作，才会返回 OK。或者用./redis-cli -c -p port 的命令(c 代表 cluster)。这样客户端需要连接两次。Jedis 等客户端会在本地维护一份 slot——node 的映射关系，大部分时候不需要重 定向，所以叫做 smart jedis(需要客户端支持)。 数据迁移因为 key 和 slot 的关系是永远不会变的，当新增了节点的时候，需要把原有的 slot 分配给新的节点负责，并且把相关的数据迁移过来。添加新节点(新增一个 7297): 1redis-cli --cluster add-node 127.0.0.1:7291 127.0.0.1:7297 新增的节点没有哈希槽，不能分布数据，在原来的任意一个节点上执行: 1redis-cli --cluster reshard 127.0.0.1:7291 输入需要分配的哈希槽的数量(比如 500)，和哈希槽的来源节点(可以输入 all 或者 id)。 高可用和主从切换原理当 slave 发现自己的 master 变为 FAIL 状态时，便尝试进行 Failover，以期成为新 的 master。由于挂掉的 master 可能会有多个 slave，从而存在多个 slave 竞争成为 master 节点的过程， 其过程如下: slave 发现自己的 master 变为 FAIL 将自己记录的集群 currentEpoch 加 1，并广播 FAILOVER_AUTH_REQUEST 信息 其他节点收到该信息，只有 master 响应，判断请求者的合法性，并发送 FAILOVER_AUTH_ACK，对每一个 epoch 只发送一次 ack 尝试 failover 的 slave 收集 FAILOVER_AUTH_ACK 超过半数后变成新 Master 广播 Pong 通知其他集群节点。 Redis Cluster 既能够实现主从的角色分配，又能够实现主从切换，相当于集成了 Replication 和 Sentinal 的功能。 总结优势 无中心架构。 数据按照 slot 存储分布在多个节点，节点间数据共享，可动态调整数据分布。 可扩展性，可线性扩展到 1000 个节点(官方推荐不超过 1000 个)，节点可动态添加或删除。 高可用性，部分节点不可用时，集群仍可用。通过增加 Slave 做 standby 数据副本，能够实现故障自动 failover，节点之间通过 gossip 协议交换状态信息，用投票机制 完成 Slave 到 Master 的角色提升。 降低运维成本，提高系统的扩展性和可用性。 不足 Client 实现复杂，驱动要求实现 Smart Client，缓存 slots mapping 信息并及时 更新，提高了开发难度，客户端的不成熟影响业务的稳定性。 节点会因为某些原因发生阻塞(阻塞时间大于 clutser-node-timeout)，被判断下线，这种 failover 是没有必要的。 数据通过异步复制，不保证数据的强一致性。 多个业务使用同一套集群时，无法根据统计区分冷热数据，资源隔离性较差，容易出现相互影响的情况。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解Redis-进阶篇]]></title>
    <url>%2F2019%2F11%2F19%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Redis-%E8%BF%9B%E9%98%B6%E7%AF%87%2F</url>
    <content type="text"><![CDATA[发布订阅模式列表的局限前面我们说通过队列的 rpush 和 lpop 可以实现消息队列（队尾进队头出），但是消费者需要不停地调用 lpop 查看 List 中是否有等待处理的消息（比如写一个 while 循环）。为了减少通信的消耗，可以 sleep()一段时间再消费，但是会有两个问题： 如果生产者生产消息的速度远大于消费者消费消息的速度，List 会占用大量的内存。 消息的实时性降低。 list 还提供了一个阻塞的命令：blpop，没有任何元素可以弹出的时候，连接会被阻塞。基于 list 实现的消息队列，不支持一对多的消息分发。 发布/订阅模式除了通过 list 实现消息队列之外，Redis 还提供了一组命令实现发布/订阅模式。这种方式，发送者和接收者没有直接关联（实现了解耦），接收者也不需要持续尝试获取消息。 订阅频道首先，我们有很多的频道（channel），我们也可以把这个频道理解成 queue。订阅者可以订阅一个或者多个频道。消息的发布者（生产者）可以给指定的频道发布消息。只要有消息到达了频道，所有订阅了这个频道的订阅者都会收到这条消息。需要注意的注意是，发出去的消息不会被持久化，因为它已经从队列里面移除了，所以消费者只能收到它开始订阅这个频道之后发布的消息。下面我们来看一下发布订阅命令的使用方法。订阅者订阅频道：可以一次订阅多个，比如这个客户端订阅了 3 个频道 1subscribe channel-1 channel-2 channel-3 发布者可以向指定频道发布消息（并不支持一次向多个频道发送消息）： 1publish channel-1 2673 取消订阅（不能在订阅状态下使用）： 1unsubscribe channel-1 按规则（Pattern）订阅频道支持?和占位符。?代表一个字符，代表 0 个或者多个字符。消费端 1，关注运动信息: 1psubscribe *sport 消费端 2，关注所有新闻： 1psubscribe news* 消费端 3，关注天气新闻： 1psubscribe news-weather 生产者，发布 3 条信息 123publish news-sport yaomingpublish news-music jaychoupublish news-weather rain Redis事务为什么要用事务？我们知道 Redis 的单个命令是原子性的（比如 get set mget mset），如果涉及到多个命令的时候，需要把多个命令作为一个不可分割的处理序列，就需要用到事务。例如我们之前说的用 setnx 实现分布式锁，我们先 set，然后设置对 key 设置 expire，防止 del 发生异常的时候锁不会被释放，业务处理完了以后再 del，这三个动作我们就希望它们作为一组命令执行。Redis 的事务有两个特点： 按进入队列的顺序执行。 不会受到其他客户端的请求的影响。 Redis 的事务涉及到四个命令：multi（开启事务），exec（执行事务），discard（取消事务），watch（监视） 事务的用法通过 multi 的命令开启事务。事务不能嵌套，多个 multi 命令效果一样。 12345multiset k1 1set k2 2set k3 3exec multi 执行后，客户端可以继续向服务器发送任意多条命令， 这些命令不会立即被执行， 而是被放到一个队列中， 当 exec 命令被调用时， 所有队列中的命令才会被执行。通过 exec 的命令执行事务。如果没有执行 exec，所有的命令都不会被执行。如果中途不想执行事务了，怎么办？可以调用 discard 可以清空事务队列，放弃执行。 12345multiset k1 1set k2 2set k3 3discard watch 命令在 Redis 中还提供了一个 watch 命令。它可以为 Redis 事务提供 CAS 乐观锁行为（Check and Set / Compare and Swap），也就是多个线程更新变量的时候，会跟原值做比较，只有它没有被其他线程修改的情况下，才更新成新的值。我们可以用 watch 监视一个或者多个 key，如果开启事务之后，至少有一个被监视 key 键在 exec 执行之前被修改了， 那么整个事务都会被取消（key 提前过期除外）。可以用 unwatch 取消。 事务可能遇到的问题我们把事务执行遇到的问题分成两种，一种是在执行 exec 之前发生错误，一种是在执行 exec 之后发生错误。 在执行 exec 之前发生错误比如：入队的命令存在语法错误，包括参数数量，参数名等等（编译器错误）。 12345678127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set test 666QUEUED127.0.0.1:6379&gt; hset test1 2673(error) ERR wrong number of arguments for &apos;hset&apos; command127.0.0.1:6379&gt; exec(error) EXECABORT Transaction discarded because of previous errors 在这种情况下事务会被拒绝执行，也就是队列中所有的命令都不会得到执行。 在执行 exec 之后发生错误比如，类型错误，比如对 String 使用了 Hash 的命令，这是一种运行时错误。 12345678910111213127.0.0.1:6379&gt; flushallOK127.0.0.1:6379&gt; multiOK127.0.0.1:6379&gt; set k1 1QUEUED127.0.0.1:6379&gt; hset k1 a bQUEUED127.0.0.1:6379&gt; exec1) OK2) (error) WRONGTYPE Operation against a key holding the wrong kind of value127.0.0.1:6379&gt; get k1&quot;1&quot; 最后我们发现 set k1 1 的命令是成功的，也就是在这种发生了运行时异常的情况下，只有错误的命令没有被执行，但是其他命令没有受到影响。这个显然不符合我们对原子性的定义，也就是我们没办法用 Redis 的这种事务机制来实现原子性，保证数据的一致。Redis这样做，主要是因为: 只有当发生语法错误(这个问题在命令队列时无法检测到)，Redis命令才会执行失败, 或对keys赋予了一个类型错误的数据：这意味着这些都是程序性错误，这类错误在开发的过程中就能够发现并解决掉，几乎不会出现在生产环境。 由于不需要回滚，这使得Redis内部更加简单，而且运行速度更快。 有种观点认为 Redis 处理事务的做法会产生 bug ， 然而需要注意的是， 在通常情况下， 回滚并不能解决编程错误带来的问题。 举个例子， 如果你本来想通过 INCR 命令将键的值加上 1 ， 却不小心加上了 2 ， 又或者对错误类型的键执行了 INCR ， 回滚是没有办法处理这些情况的。鉴于没有任何机制能避免程序员自己造成的错误， 并且这类错误通常不会在生产环境中出现， 所以 Redis 选择了更简单、更快速的无回滚方式来处理事务。 Lua脚本Lua/ˈluə/是一种轻量级脚本语言，它是用 C 语言编写的，跟数据的存储过程有点类似。 使用 Lua 脚本来执行 Redis 命令的好处： 一次发送多个命令，减少网络开销。 Redis 会将整个脚本作为一个整体执行，不会被其他请求打断，保持原子性。 对于复杂的组合命令，我们可以放在文件中，可以实现程序之间的命令集复用。在 Redis 中调用 Lua 脚本使用 eval /ɪ’væl/ 方法，语法格式：1redis&gt; eval lua-script key-num [key1 key2 key3 ....] [value1 value2 value3 ....] eval 代表执行 Lua 语言的命令。 lua-script 代表 Lua 语言脚本内容。 key-num 表示参数中有多少个 key，需要注意的是 Redis 中 key 是从 1 开始的，如果没有 key 的参数，那么写 0。 [key1 key2 key3…]是 key 作为参数传递给 Lua 语言，也可以不填，但是需要和 key-num 的个数对应起来。 [value1 value2 value3 ….]这些参数传递给 Lua 语言，它们是可填可不填的。 示例，返回一个字符串，0 个参数： 1redis&gt; eval &quot;return &apos;Hello World&apos;&quot; 0 在 Lua 脚本中调用 Redis 命令使用 redis.call(command, key [param1, param2…])进行操作。语法格式： 1redis&gt; eval &quot;redis.call(&apos;set&apos;,KEYS[1],ARGV[1])&quot; 1 lua-key lua-value command 是命令，包括 set、get、del 等。 key 是被操作的键。 param1,param2…代表给 key 的参数。 注意跟 Java 不一样，定义只有形参，调用只有实参。Lua 是在调用时用 key 表示形参，argv 表示参数值（实参）。 设置键值对在 Redis 中调用 Lua 脚本执行 Redis 命令 1redis&gt; eval &quot;return redis.call(&apos;set&apos;,KEYS[1],ARGV[1])&quot; 1 test 123 以上命令等价于 set test 123在 redis-cli 中直接写 Lua 脚本不够方便，也不能实现编辑和复用，通常我们会把脚本放在文件里面，然后执行这个文件。 Redis调用Lua脚本创建 Lua 脚本文件： 12345vim test.lua脚本内容：redis.call(&apos;set&apos;,&apos;test&apos;,&apos;1&apos;)return redis.call(&apos;get&apos;,&apos;test&apos;) 在 Redis 客户端中调用 Lua 脚本 1redis-cli --eval test.lua 0 得到返回值 1 案例：对 IP 进行限流需求：在 X 秒内只能访问 Y 次。设计思路：用 key 记录 IP，用 value 记录访问次数。拿到 IP 以后，对 IP+1。如果是第一次访问，对 key 设置过期时间（参数 1）。否则判断次数，超过限定的次数（参数 2），返回 0。如果没有超过次数则返回 1。超过时间，key 过期之后，可以再次访问。KEY[1]是 IP， ARGV[1]是过期时间 X，ARGV[2]是限制访问的次数 Y。 1234567891011-- ip_limit.lua-- IP 限流，对某个 IP 频率进行限制 ，6 秒钟访问 10 次local num=redis.call(&apos;incr&apos;,KEYS[1])if tonumber(num)==1 then redis.call(&apos;expire&apos;,KEYS[1],ARGV[1]) return 1 elseif tonumber(num)&gt;tonumber(ARGV[2]) then return 0 else return 1end 6 秒钟内限制访问 10 次，调用测试（连续调用 10 次）： 1./redis-cli --eval &quot;ip_limit.lua&quot; app:ip:limit:192.168.0.1 , 6 10 app:ip:limit:192.168.0.1 是 key 值 ，后面是参数值，中间要加上一个空格 和一个逗号，再加上一个 空格 。即：./redis-cli –eval [lua 脚本] [key…]空格,空格[args…] 多个参数之间用一个 空格 分割 。缓存 Lua 脚本在脚本比较长的情况下，如果每次调用脚本都需要把整个脚本传给 Redis 服务端，会产生比较大的网络开销。为了解决这个问题，Redis 提供了 EVALSHA 命令，允许开发者通过脚本内容的 SHA1 摘要来执行脚本。Redis 在执行 script load 命令时会计算脚本的 SHA1 摘要并记录在脚本缓存中，执行 EVALSHA 命令时 Redis 会根据提供的摘要从脚本缓存中查找对应的脚本内容，如果找到了则执行脚本，否则会返回错误：”NOSCRIPT No matching script. Please use EVAL.” 123127.0.0.1:6379&gt; script load &quot;return &apos;Hello World&apos;&quot;&quot;470877a599ac74fbfda41caa908de682c5fc7d4b&quot; 127.0.0.1:6379&gt; evalsha &quot;470877a599ac74fbfda41caa908de682c5fc7d4b&quot; 0&quot;Hello World&quot; 脚本超时Redis 的指令执行本身是单线程的，这个线程还要执行客户端的 Lua 脚本，如果 Lua 脚本执行超时或者陷入了死循环，是不是没有办法为客户端提供服务了呢？ 1eval &apos;while(true) do end&apos; 0 为 了防 止 某个 脚本 执 行时 间 过长 导 致 Redis 无 法提 供 服务 ， Redis 提 供 了 lua-time-limit 参数限制脚本的最长运行时间，默认为 5 秒钟。 1lua-time-limit 5000（redis.conf 配置文件中） 当脚本运行时间超过这一限制后，Redis 将开始接受其他命令但不会执行（以确保脚本的原子性，因为此时脚本并没有被终止），而是会返回“BUSY”错误。Redis 提供了一个 script kill 的命令来中止脚本的执行。新开一个客户端： 1script kill 如果当前执行的 Lua 脚本对 Redis 的数据进行了修改（SET、DEL 等），那么通过 script kill 命令是不能终止脚本运行的。 1eval &quot;redis.call(&apos;set&apos;,&apos;gupao&apos;,&apos;666&apos;) while true do end&quot; 0 因为要保证脚本运行的原子性，如果脚本执行了一部分终止，那就违背了脚本原子性的要求。最终要保证脚本要么都执行，要么都不执行。遇到这种情况，只能通过 shutdown nosave 命令来强行终止 redis。shutdown nosave 和 shutdown 的区别在于 shutdown nosave 不会进行持久化操作，意味着发生在上一次快照后的数据库修改都会丢失。Redis 不是只有一个线程吗？它已经卡死了，怎么接受 spript kill 指令的？Redis 单线程指的是网络请求模块使用了一个线程（所以不需考虑并发安全性），即一个线程处理所有网络请求，其他模块仍用了多个线程。总结：如果我们有一些特殊的需求，可以用 Lua 来实现，但是要注意那些耗时的操作。 Redis 为什么这么快？进入Redis安装目录的src文件夹下，执行 1redis-benchmark -t set,lpush -n 100000 -q 结果（本地虚拟机）：SET: 51813.47 requests per second —— 每秒钟处理 5 万多次 set 请求LPUSH: 51706.31 requests per second —— 每秒钟处理 5 万多次 lpush 请求 1redis-benchmark -n 100000 -q script load &quot;redis.call(&apos;set&apos;,&apos;foo&apos;,&apos;bar&apos;)&quot; 结果（本地虚拟机）：script load redis.call(‘set’,’foo’,’bar’): 46816.48 requests per second —— 每秒钟 46000 次 lua 脚本调用 横轴：连接数；纵轴：QPS 根据官方的数据，Redis 的 QPS 可以达到 10 万左右（每秒请求数）。 Redis为什么这么快？ 纯内存结构 单线程 多路复用 内存KV 结构的内存数据库，时间复杂度 O(1)。 单线程要实现这么高的并发性能，是不是要创建非常多的线程？恰恰相反，Redis 是单线程的。单线程有什么好处呢？ 没有创建线程、销毁线程带来的消耗 避免了上线文切换导致的 CPU 消耗 避免了线程之间带来的竞争问题，例如加锁释放锁死锁等等 Redis为什么是单线程的？不是白白浪费了 CPU 的资源吗？因为单线程已经够用了，CPU 不是 redis 的瓶颈。Redis 的瓶颈最有可能是机器内存或者网络带宽。既然单线程容易实现，而且 CPU 不会成为瓶颈，那就顺理成章地采用单线程的方案了。 异步非阻塞异步非阻塞 I/O，多路复用处理并发连接。 单线程为什么这么快？因为 Redis 是基于内存的操作，我们先从内存开始说起。 虚拟存储器（虚拟内存 Vitual Memory）名词解释：主存：内存；辅存：磁盘（硬盘） 计算机主存（内存）可看作一个由 M 个连续的字节大小的单元组成的数组，每个字节有一个唯一的地址，这个地址叫做物理地址（PA）。早期的计算机中，如果 CPU 需要内存，使用物理寻址，直接访问主存储器。这种方式有几个弊端： 在多用户多任务操作系统中，所有的进程共享主存，如果每个进程都独占一块物理地址空间，主存很快就会被用完。我们希望在不同的时刻，不同的进程可以共用同一块物理地址空间。 如果所有进程都是直接访问物理内存，那么一个进程就可以修改其他进程的内存数据，导致物理地址空间被破坏，程序运行就会出现异常。 为了解决这些问题，我们就想了一个办法，在 CPU 和主存之间增加一个中间层。CPU不再使用物理地址访问，而是访问一个虚拟地址，由这个中间层把地址转换成物理地址，最终获得数据。这个中间层就叫做虚拟存储器（Virtual Memory）。在每一个进程开始创建的时候，都会分配一段虚拟地址，然后通过虚拟地址和物理地址的映射来获取真实数据，这样进程就不会直接接触到物理地址，甚至不知道自己调用的哪块物理地址的数据。目前，大多数操作系统都使用了虚拟内存，如 Windows 系统的虚拟内存、Linux 系统的交换空间等等。Windows 的虚拟内存（pagefile.sys）是磁盘空间的一部分。在 32 位的系统上，虚拟地址空间大小是 2^32bit=4G。在 64 位系统上，最大虚拟地址空间大小是多少？是不是 2^64bit=1024*1014TB=1024PB=16EB？实际上没有用到 64 位，因为用不到这么大的空间，而且会造成很大的系统开销。Linux 一般用低 48 位来表示虚拟地址空间，也就是 2^48bit=256T。 总结：引入虚拟内存，可以提供更大的地址空间，并且地址空间是连续的，使得程序编写、链接更加简单。并且可以对物理内存进行隔离，不同的进程操作互不影响。还可以通过把同一块物理内存映射到不同的虚拟地址空间实现内存共享。 用户空间和内核空间为了避免用户进程直接操作内核，保证内核安全，操作系统将虚拟内存划分为两部分，一部分是内核空间（Kernel-space），一部分是用户空间（User-space）。内核是操作系统的核心，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的权限。内核空间中存放的是内核代码和数据，而进程的用户空间中存放的是用户程序的代码和数据。不管是内核空间还是用户空间，它们都处于虚拟空间中，都是对物理地址的映射。在 Linux 系统中, 内核进程和用户进程所占的虚拟内存比例是 1:3。当进程运行在内核空间时就处于内核态，而进程运行在用户空间时则处于用户态。进程在内核空间以执行任意命令，调用系统的一切资源；在用户空间只能执行简单的运算，不能直接调用系统资源，必须通过系统接口（又称 system call），才能向内核发出指令。 top命令：us 代表 CPU 消耗在 User space 的时间百分比;sy 代表 CPU 消耗在 Kernel space 的时间百分比。 进程切换（上下文切换）多任务操作系统是怎么实现运行远大于 CPU 数量的任务个数的？当然，这些任务实际上并不是真的在同时运行，而是因为系统通过时间片分片算法，在很短的时间内，将 CPU 轮流分配给它们，造成多任务同时运行的错觉。为了控制进程的执行，内核必须有能力挂起正在 CPU 上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。 什么叫上下文？ 在每个任务运行前，CPU 都需要知道任务从哪里加载、又从哪里开始运行，也就是说，需要系统事先帮它设置好 CPU 寄存器和程序计数器(Program Counter)，这个叫做CPU 的上下文。而这些保存下来的上下文，会存储在系统内核中，并在任务重新调度执行时再次加载进来。这样就能保证任务原来的状态不受影响，让任务看起来还是连续运行。在切换上下文的时候，需要完成一系列的工作，这是一个很消耗资源的操作。 进程的阻塞正在运行的进程由于提出系统服务请求（如 I/O 操作），但因为某种原因未得到操作系统的立即响应，该进程只能把自己变成阻塞状态，等待相应的事件出现后才被唤醒。进程在阻塞状态不占用 CPU 资源。 文件描述符 FDLinux 系统将所有设备都当作文件来处理，而 Linux 用文件描述符来标识每个文件对象。文件描述符（File Descriptor）是内核为了高效管理已被打开的文件所创建的索引，用于指向被打开的文件，所有执行 I/O 操作的系统调用都通过文件描述符；文件描述符是一个简单的非负整数，用以表明每个被进程打开的文件。Linux 系统里面有三个标准文件描述符。0：标准输入（键盘）；1：标准输出（显示器）；2：标准错误输出（显示器）。 传统 I/O 数据拷贝当应用程序执行 read 系统调用读取文件描述符（FD）的时候，如果这块数据已经存在于用户进程的页内存中，就直接从内存中读取数据。如果数据不存在，则先将数据从磁盘加载数据到内核缓冲区中，再从内核缓冲区拷贝到用户进程的页内存中。（两次拷贝，两次 user 和 kernel 的上下文切换）。 Blocking I/O当使用 read 或 write 对某个文件描述符进行过读写时，如果当前 FD 不可读，系统就不会对其他的操作做出响应。从设备复制数据到内核缓冲区是阻塞的，从内核缓冲区拷贝到用户空间，也是阻塞的，直到 copy complete，内核返回结果，用户进程才解除 block 的状态。I/O 的阻塞是指等待数据和从内核空间复制数据到用户空间两个步骤上。 为了解决阻塞的问题，我们有几个思路。 在服务端创建多个线程或者使用线程池，但是在高并发的情况下需要的线程会很多，系统无法承受，而且创建和释放线程都需要消耗资源。 由请求方定期轮询，在数据准备完毕后再从内核缓存缓冲区复制数据到用户空间（非阻塞式 I/O），这种方式会存在一定的延迟。 I/O 多路复用（I/O Multiplexing）I/O 指的是网络 I/O。多路指的是多个 TCP 连接（Socket 或 Channel）。复用指的是复用一个或多个线程。它的基本原理就是不再由应用程序自己监视连接，而是由内核替应用程序监视文件描述符。客户端在操作的时候，会产生具有不同事件类型的 socket。在服务端，I/O 多路复用程序（I/O Multiplexing Module）会把消息放入队列中，然后通过文件事件分派器（File event Dispatcher），转发到不同的事件处理器中。多路复用有很多的实现，以 select 为例，当用户进程调用了多路复用器，进程会被阻塞。内核会监视多路复用器负责的所有 socket，当任何一个 socket 的数据准备好了，多路复用器就会返回。这时候用户进程再调用 read 操作，把数据从内核缓冲区拷贝到用户空间。所以，I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪（readable）状态，select()函数就可以返回。Redis 的多路复用， 提供了 select, epoll, evport, kqueue 几种选择，在编译的时候来选择一种。evport 是 Solaris 系统内核提供支持的；epoll 是 LINUX 系统内核提供支持的；kqueue 是 Mac 系统提供支持的；select 是 POSIX 提供的，一般的操作系统都有支撑（保底方案）；源码 ae_epoll.c、ae_select.c、ae_kqueue.c、ae_evport.c 内存回收Reids 所有的数据都是存储在内存中的，在某些情况下需要对占用的内存空间进行回收。内存回收主要分为两类，一类是 key 过期，一类是内存使用达到上限（max_memory）触发内存淘汰。 过期策略定时过期（主动淘汰）每个设置过期时间的 key 都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的 CPU 资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。 惰性过期（被动淘汰）只有当访问一个 key 时，才会判断该 key 是否已过期，过期则清除。该策略可以最大化地节省 CPU 资源，却对内存非常不友好。极端情况可能出现大量的过期 key 没有再次被访问，从而不会被清除，占用大量内存。第二种情况，每次写入 key 时，发现内存不够，调用 activeExpireCycle 释放一部分内存。 定期过期每隔一定的时间，会扫描一定数量的数据库的 expires 字典中一定数量的 key，并清除其中已过期的 key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得 CPU 和内存资源达到最优的平衡效果。Redis 中同时使用了惰性过期和定期过期两种过期策略。 淘汰策略Redis 的内存淘汰策略，是指当内存使用达到最大内存极限时，需要使用淘汰算决定清理掉哪些数据，以保证新数据的存入。 最大内存设置redis.conf 参数配置： 1# maxmemory &lt;bytes&gt; 如果不设置 maxmemory 或者设置为 0，64 位系统不限制内存，32 位系统最多使用 3GB 内存。动态修改： 1redis&gt; config set maxmemory 2GB 策略类型redis.conf 淘汰策略设置：maxmemory-policy noeviction 12345678volatile-lru -&gt; Evict using approximated LRU among the keys with an expire set. allkeys-lru -&gt; Evict any key using approximated LRU. volatile-lfu -&gt; Evict using approximated LFU among the keys with an expire set. allkeys-lfu -&gt; Evict any key using approximated LFU. volatile-random -&gt; Remove a random key among the ones with an expire set. allkeys-random -&gt; Remove a random key, any key. volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)noeviction -&gt; Don&apos;t evict anything, just return an error on write operations. LRU，Least Recently Used：最近最少使用。判断最近被使用的时间，目前最远的数据优先被淘汰。LFU，Least Frequently Used，最不常用，4.0 版本新增。random，随机删除。 策略 含义 volatile-lru 根据 LRU 算法删除设置了超时属性（expire）的键，直到腾出足够内存为止。如果没有可删除的键对象，回退到 noeviction 策略。 allkeys-lru 根据 LRU 算法删除键，不管数据有没有设置超时属性，直到腾出足够内存为止。 volatile-lfu 在带有过期时间的键中选择最不常用的。 allkeys-lfu 在所有的键中选择最不常用的，不管数据有没有设置超时属性。 volatile-random 在带有过期时间的键中随机选择。 allkeys-random 随机删除所有键，直到腾出足够内存为止。 volatile-ttl 根据键值对象的 ttl 属性，删除最近将要过期数据。如果没有，回退到 noeviction 策略。 noeviction 默认策略，不会删除任何数据，拒绝所有写入操作并返回客户端错误信息（error）OOM command not allowed when used memory，此时 Redis 只响应读操作。 如果没有符合前提条件的 key 被淘汰，那么 volatile-lru、volatile-random 、volatile-ttl 相当于 noeviction（不做内存回收）。 动态修改淘汰策略： 1redis&gt; config set maxmemory-policy volatile-lru 建议使用 volatile-lru，在保证正常服务的情况下，优先删除最近最少使用的 key。 LRU 淘汰原理如果基于传统 LRU 算法实现 Redis LRU 会有什么问题？ 需要额外的数据结构存储，消耗内存。 Redis LRU 对传统的 LRU 算法进行了改良，通过随机采样来调整算法的精度。 如果淘汰策略是 LRU，则根据配置的采样值maxmemory_samples（默认是 5 个）, 随机从数据库中选择 m 个 key, 淘汰其中热度最低的 key 对应的缓存数据。所以采样参数m配置的数值越大, 就越能精确的查找到待淘汰的缓存数据,但是也消耗更多的CPU计算,执行效率降低。 如何找出热度最低的数据？Redis 中所有对象结构都有一个 lru 字段, 且使用了 unsigned 的低 24 位，这个字段用来记录对象的热度。对象被创建时会记录 lru 值。在被访问的时候也会更新 lru 的值。但是不是获取系统当前的时间戳，而是设置为全局变量 server.lruclock 的值。 123456789 typedef struct redisObject &#123; unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or * LFU data (least significant 8 bits frequency * and most significant 16 bits access time). */ int refcount; void *ptr;&#125; robj; server.lruclock 的值怎么来的？Redis 中 有 个 定 时 处 理 的 函 数serverCron ， 默 认 每 100 毫 秒 调 用 函数 updateCachedTime 更新一次全局变量的 server.lruclock 的值，它记录的是当前 unix时间戳。 为什么不获取精确的时间而是放在全局变量中？不会有延迟的问题吗？这样函数 lookupKey 中更新数据的 lru 热度值时,就不用每次调用系统函数 time，可以提高执行效率 函数 estimateObjectIdleTime 评估指定对象的 lru 热度，思想就是对象的 lru 值和全局的 server.lruclock 的差值越大（越久没有得到更新）， 该对象热度越低。 为什么不用常规的哈希表+双向链表的方式实现？需要额外的数据结构，消耗资源。而 Redis LRU 算法在 sample 为 10 的情况下，已经能接近传统 LRU 算法了。 除了消耗资源之外，传统 LRU 还有什么问题？如图，假设 A 在 10 秒内被访问了 5 次，而 B 在 10 秒内被访问了 3 次。因为 B 最后一次被访问的时间比 A 要晚，在同等的情况下，A 反而先被回收。 LFU 淘汰原理 123456789 typedef struct redisObject &#123; unsigned type:4; unsigned encoding:4; unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or * LFU data (least significant 8 bits frequency * and most significant 16 bits access time). */ int refcount; void *ptr;&#125; robj; 当这 24 bits 用作 LFU 时，其被分为两部分： 高 16 位用来记录访问时间（单位为分钟，ldt，last decrement time） 低 8 位用来记录访问频率，简称 counter（logc，logistic counter） counter 是用基于概率的对数计数器实现的，8 位可以表示百万次的访问频率。对象被读写的时候，lfu 的值会被更新。 12345void updateLFU(robj *val) &#123; unsigned long counter = LFUDecrAndReturn(val); counter = LFULogIncr(counter); val-&gt;lru = (LFUGetTimeInMinutes()&lt;&lt;8) | counter;&#125; 增长的速率由，lfu-log-factor 越大，counter 增长的越慢redis.conf 配置文件 1# lfu-log-factor 10 如果计数器只会递增不会递减，也不能体现对象的热度。没有被访问的时候，计数器怎么递减呢？减少的值由衰减因子 lfu-decay-time（分钟）来控制，如果值是 1 的话，N 分钟没有访问就要减少 N。redis.conf 配置文件 1# lfu-decay-time 1 持久化机制Redis 速度快，很大一部分原因是因为它所有的数据都存储在内存中。如果断电或者宕机，都会导致内存中的数据丢失。为了实现重启后数据不丢失，Redis 提供了两种持久化的方案，一种是 RDB 快照（Redis DataBase），一种是 AOF（Append Only File）。 RDBRDB 是 Redis 默认的持久化方案。当满足一定条件的时候，会把当前内存中的数据写入磁盘，生成一个快照文件 dump.rdb。Redis 重启会通过加载 dump.rdb 文件恢复数据。 自动触发redis.conf， SNAPSHOTTING，其中定义了触发把数据保存到磁盘的触发频率。如果不需要 RDB 方案，注释 save 或者配置成空字符串””。 123save 900 1 # 900 秒内至少有一个 key 被修改（包括添加）save 300 10 # 400 秒内至少有 10 个 key 被修改save 60 10000 # 60 秒内至少有 10000 个 key 被修改 注意上面的配置是不冲突的，只要满足任意一个都会触发。RDB 文件位置和目录： 12345678# 文件路径，dir ./# 文件名称dbfilename dump.rdb# 是否是 LZF 压缩 rdb 文件rdbcompression yes# 开启数据校验rdbchecksum yes 参数 说明 dir rdb 文件默认在启动目录下（相对路径） config get dir 获取 dbfilename 文件名称 rdbcompression 开启压缩可以节省存储空间，但是会消耗一些 CPU 的计算时间，默认开启 rdbchecksum 使用 CRC64 算法来进行数据校验，但是这样做会增加大约 10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能。 为什么停止 Redis 服务的时候没有 save，重启数据还在？RDB 还有两种触发方式： shutdown 触发，保证服务器正常关闭。 flushall，RDB 文件是空的，没什么意义（删掉 dump.rdb 演示一下）。手动触发如果我们需要重启服务或者迁移数据，这个时候就需要手动触 RDB 快照保存。Redis提供了两条命令：a）savesave 在生成快照的时候会阻塞当前 Redis 服务器， Redis 不能处理其他命令。如果内存中的数据比较多，会造成 Redis 长时间的阻塞。生产环境不建议使用这个命令。为了解决这个问题，Redis 提供了第二种方式。b）bgsave执行 bgsave 时，Redis 会在后台异步进行快照操作，快照同时还可以响应客户端请求。 具体操作是 Redis 进程执行 fork 操作创建子进程（copy-on-write），RDB 持久化过程由子进程负责，完成后自动结束。它不会记录 fork 之后后续的命令。阻塞只发生在 fork 阶段，一般时间很短。用 lastsave 命令可以查看最近一次成功生成快照的时间。 RDB 数据的恢复可以通过将 dump.rdb 放到指定位置，即可恢复备份的数据。 RDB 文件的优势和劣势优势 RDB 是一个非常紧凑(compact)的文件，它保存了 redis 在某个时间点上的数据集。这种文件非常适合用于进行备份和灾难恢复。 生成 RDB 文件的时候，redis 主进程会 fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘 IO 操作。 RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 劣势 RDB 方式数据没办法做到实时持久化/秒级持久化。因为 bgsave 每次运行都要执行 fork 操作创建子进程，频繁执行成本过高。 在一定间隔时间做一次备份，所以如果 redis 意外 down 掉的话，就会丢失最后一次快照之后的所有修改（数据有丢失）。 如果数据相对来说比较重要，希望将损失降到最小，则可以使用 AOF 方式进行持久化。 AOFAOF：Redis 默认不开启。AOF 采用日志的形式来记录每个写操作，并追加到文件中。开启后，执行更改 Redis 数据的命令时，就会把命令写入到 AOF 文件中。Redis 重启时会根据日志文件的内容把写指令从前到后执行一次以完成数据的恢复工作。 AOF 配置配置文件 redis.conf 1234# 开关appendonly no# 文件名appendfilename &quot;appendonly.aof&quot; 参数 说明 appendonly Redis 默认只开启 RDB 持久化，开启 AOF 需要修改为 yes appendfilename “appendonly.aof” 路径也是通过 dir 参数配置 config get dir 数据都是实时持久化到磁盘吗？由于操作系统的缓存机制，AOF 数据并没有真正地写入硬盘，而是进入了系统的硬盘缓存。什么时候把缓冲区的内容写入到 AOF 文件？配置文件 redis.conf 1appendfsync everysec AOF 持久化策略（硬盘缓存到磁盘），默认 everysec no 表示不执行 fsync，由操作系统保证数据同步到磁盘，速度最快，但是不太安全； always 表示每次写入都执行 fsync，以保证数据同步到磁盘，效率很低； everysec 表示每秒执行一次 fsync，可能会导致丢失这 1s 数据。通常选择 everysec ，兼顾安全性和效率。 “appendonly.aof” 文件越来越大，怎么办？由于 AOF 持久化是 Redis 不断将写命令记录到 AOF 文件中，随着 Redis 不断的进行，AOF 的文件会越来越大，文件越大，占用服务器内存越大以及 AOF 恢复要求时间越长。例如 set gupao 666，执行 1000 次，结果都是 gupao=666。为了解决这个问题，Redis 新增了重写机制，当 AOF 文件的大小超过所设定的阈值时，Redis 就会启动 AOF 文件的内容压缩，只保留可以恢复数据的最小指令集。可以使用命令 bgrewriteaof 来重写。AOF 文件重写并不是对原文件进行重新整理，而是直接读取服务器现有的键值对，然后用一条命令去代替之前记录这个键值对的多条命令，生成一个新的文件后去替换原来的 AOF 文件。 123# 重写触发机制auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb 参数 说明 auto-aof-rewrite-percentage 默认值为 100。aof 自动重写配置，当目前 aof 文件大小超过上一次重写的 aof 文件大小的百分之多少进行重写，即当 aof 文件增长到一定大小的时候，Redis 能够调用 bgrewriteaof 对日志文件进行重写。当前 AOF 文件大小是上次日志重写得到 AOF 文件大小的二倍（设置为 100）时，自动启动新的日志重写过程。 auto-aof-rewrite-min-size 默认 64M。设置允许重写的最小 aof 文件大小，避免了达到约定百分比但尺寸仍然很小的情况还要重写。 重写过程中，AOF 文件被更改了怎么办？ 另外有两个与 AOF 相关的参数：|参数| 说明|| — | — ||no-appendfsync-on-rewrite|在 aof 重写或者写入 rdb 文件的时候，会执行大量 IO，此时对于 everysec 和 always 的 aof模式来说，执行 fsync 会造成阻塞过长时间，no-appendfsync-on-rewrite 字段设置为默认设置为 no。如果对延迟要求很高的应用，这个字段可以设置为 yes，否则还是设置为 no，这样对持久化特性来说这是更安全的选择。设置为 yes 表示 rewrite 期间对新写操作不 fsync, 暂时存在内存中,等 rewrite 完成后再写入，默认为 no，建议修改为 yes。Linux 的默认 fsync策略是 30 秒。可能丢失 30 秒数据。|| aof-load-truncated |aof 文件可能在尾部是不完整的，当 redis 启动的时候，aof 文件的数据被载入内存。重启可能发生在 redis 所在的主机操作系统宕机后，尤其在 ext4 文件系统没有加上 data=ordered 选项，出现这种现象。redis 宕机或者异常终止不会造成尾部不完整现象，可以选择让 redis 退出，或者导入尽可能多的数据。如果选择的是 yes，当截断的 aof 文件被导入的时候，会自动发布一个 log 给客户端然后 load。如果是 no，用户必须手动 redis-check-aof 修复 AOF 文件才可以。默认值为 yes。| AOF 数据恢复重启 Redis 之后就会进行 AOF 文件的恢复。 AOF 优势与劣势优势 AOF 持久化的方法提供了多种的同步频率，即使使用默认的同步频率每秒同步一次，Redis 最多也就丢失 1 秒的数据而已。 劣势 对于具有相同数据的的 Redis，AOF 文件通常会比 RDF 文件体积更大（RDB存的是数据快照）。 虽然 AOF 提供了多种同步的频率，默认情况下，每秒同步一次的频率也具有较高的性能。在高并发的情况下，RDB 比 AOF 具好更好的性能保证。 两种方案比较那么对于 AOF 和 RDB 两种持久化方式，我们应该如何选择呢？如果可以忍受一小段时间内数据的丢失，毫无疑问使用 RDB 是最好的，定时生成RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快。否则就使用 AOF 重写。但是一般情况下建议不要单独使用某一种持久化机制，而是应该两种一起用，在这种情况下,当 redis 重启的时候会优先载入 AOF 文件来恢复原始的数据，因为在通常情况下 AOF 文件保存的数据集要比 RDB 文件保存的数据集要完整。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解Redis-基础篇]]></title>
    <url>%2F2019%2F11%2F18%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Redis-%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Redis介绍硬件层面有 CPU 的缓存;浏览器也有缓存;手机的应用也有缓存。我们把数据缓存起来的原因就是从原始位置取数据的代价太大了，放在一个临时位置存储起来，取回就可以快一些。 Redis特性 支持多种编程语言 高可用、支持集群 能够存储丰富的数据类型 可以实现跨进程、服务器实现数据共享 功能丰富：支持持久化机制、过期策略等 Redis数据类型Redis一共有8种数据类型： String Hash Set List Zset BitMap Hyperloglog Geo Streams 接下来我们深入分析每一种数据类型。 String（字符串）存储类型可以用来存储字符串、整数、浮点数。 常见的操作设置单个值 1set key value 设置多个值(批量操作，原子性) 1mset key1 value1 key2 value2 删除值 1del key 基于此可实现分布式锁，用 del key 释放锁。 但如果释放锁的操作失败了，导致其他节点永远获取不到锁，怎么办? 加过期时间。单独用 expire 加过期，也失败了，无法保证原子性，怎么办?可以使用多参数 1234set key value [expiration EX seconds|PX milliseconds][NX|XX]使用参数的方式set lock 1 EX 10 NX (整数)值递增 12incr key incrby key 100 (整数)值递减 12decr key decrby key 100 浮点数增量 1incrbyfloat key 7.3 获取多个值 1mget key1 key2 获取值长度 1strlen key 字符串追加内容 1append key content 获取指定范围的字符 1getrange key 0 8 数据模型set hello word 为例，因为 Redis 是 KV 的数据库，它是通过 hashtable 实现的(我们把这个叫做外层的哈希)。所以每个键值对都会有一个 dictEntry(源码位置:dict.h)， 里面指向了 key 和 value 的指针。next 指向下一个 dictEntry。 12345678typedef struct dictEntry &#123; void *key; /* key 关键字定义 */ union &#123; void *val; uint64_t u64; /* value 定义 */ int64_t s64; double d; &#125; v; struct dictEntry *next; /* 指向下一个键值对节点 */&#125; dictEntry; key 是字符串，但是 Redis 没有直接使用 C 的字符数组，而是存储在自定义的 SDS 中。value 既不是直接作为字符串存储，也不是直接存储在 SDS 中，而是存储在 redisObject 中。实际上五种常用的数据类型的任何一种，都是通过 redisObject 来存储的。 redisObjectredisObject 定义在 src/server.h 文件中。 123456typedef struct redisObject &#123; unsigned type:4; /* 对象的类型，包括:OBJ_STRING、OBJ_LIST、OBJ_HASH、OBJ_SET、OBJ_ZSET */ unsigned encoding:4; /* 具体的数据结构 */ unsigned lru:LRU_BITS; /* 24 位，对象最后一次被命令程序访问的时间，与内存回收有关 */ int refcount; /* 引用计数。当 refcount 为 0 的时候，表示该对象已经不被任何对象引用，则可以进行垃圾回收了*/ void *ptr; /* 指向对象实际的数据结构 */&#125; robj; 可以使用 type 命令来查看对外的类型。 1type key 内部编码 1object encoding key 字符串类型的内部编码有三种: int，存储 8 个字节的长整型(long，2^63-1)。 embstr, 代表 embstr 格式的 SDS(Simple Dynamic String 简单动态字符串)，存储小于 44 个字节的字符串。 raw，存储大于 44 个字节的字符串(3.2 版本之前是 39 字节)。 SDSRedis 中字符串的实现。在 3.2 以后的版本中，SDS 又有多种结构(sds.h):sdshdr5、sdshdr8、sdshdr16、sdshdr32、sdshdr64，用于存储不同的长度的字符串，分别代表 2^5=32byte， 2^8=256byte，2^16=65536byte=64KB，2^32byte=4GB。 1234567/* sds.h */struct __attribute__ ((__packed__)) sdshdr8 &#123; uint8_t len; /* 当前字符数组的长度 */ uint8_t alloc; /*当前字符数组总共分配的内存大小 */ unsigned char flags; /* 当前字符数组的属性、用来标识到底是 sdshdr8 还是 sdshdr16 等 */ char buf[]; /* 字符串真正的值 */&#125;; SDS 的特点: 不用担心内存溢出问题，如果需要会对 SDS 进行扩容。 获取字符串长度时间复杂度为 O(1)，因为定义了 len 属性。 通过“空间预分配”( sdsMakeRoomFor)和“惰性空间释放”，防止多次重分配内存。 判断是否结束的标志是 len 属性(它同样以’\0’结尾是因为这样就可以使用 C语言中函数库操作字符串的函数了)，可以包含’\0’。 数据类型int 什么时候转化为 raw? 当 int 类型被数据被修改为不再是整数或大小超过了 long 的范围 (2^63-1=9223372036854775807)时，自动转化为 raw。 embstr 和 raw 的区别? embstr 的使用只分配一次内存空间(因为 RedisObject 和 SDS 是连续的)，而 raw 需要分配两次内存空间(分别为 RedisObject 和 SDS 分配空间)。 因此与 raw 相比，embstr 的好处在于创建时少分配一次空间，删除时少释放一次空间，以及对象的所有数据连在一起，寻找方便。 而 embstr 的坏处也很明显，如果字符串的长度增加需要重新分配内存时，整个 RedisObject 和 SDS 都需要重新分配空间。因此 Redis 中的 embstr 实现为只读，只要进行修改就会转换为 raw，无论是否达到了 44 个字节。 当长度小于阈值时，会还原吗? Redis 内部编码的转换，都符合以下规律:编码转换在 Redis 写入数据时完 成，且转换过程不可逆，只能从小内存编码向大内存编码转换(但是不包括重新 set)。 为什么要对底层的数据结构进行一层包装呢? 通过封装，可以根据对象的类型动态地选择存储结构和可以使用的命令，实现节省空间和优化查询速度。应用场景 缓存 数据共享 分布式锁 全局ID 计数器 限流 Hash（哈希）存储类型包含键值对的无序散列表。value 只能是字符串，不能嵌套其他类型。 同样是存储字符串，Hash 与 String 的主要区别? 把所有相关的值聚集到一个 key 中，节省内存空间 只使用一个 key，减少 key 冲突 当需要批量获取值的时候，只需要使用一个命令，减少内存/IO/CPU 的消耗 Hash 不适合的场景: Field 不能单独设置过期时间 没有 bit 操作 需要考虑数据量分布的问题(value 值非常大的时候，无法分布到多个节点)常用命令123456789101112131415161718192021222324252627282930单个设置值：hset h1 f 6hset h1 e 5批量设置值：hmset h1 a 1 b 2 c 3 d 4获取单个值：hget h1 a批量获取值：hmget h1 a b c d 获取所有key：hkeys h1获取所有value：hvals h1获取所有值：hgetall h1判断哈希是否存在：hget exists h1 删除哈希：hdel h1获取哈希长度：hlen h1 存储原理Redis 的 Hash 本身也是一个 KV 的结构，类似于 Java 中的 HashMap。外层的哈希(Redis KV 的实现)只用到了 hashtable。当存储 hash 数据类型时， 我们把它叫做内层的哈希。内层的哈希底层可以使用两种数据结构实现:ziplist:OBJ_ENCODING_ZIPLIST(压缩列表) hashtable:OBJ_ENCODING_HT(哈希表) ziplist 压缩列表ziplist 是一个经过特殊编码的双向链表，它不存储指向上一个链表节点和指向下一个链表节点的指针，而是存储上一个节点长度和当前节点长度，通过牺牲部分读写性能，来换取高效的内存空间利用率，是一种时间换空间的思想。只用在字段个数少，字段值小的场景里面。 ziplist的内部结构 123456789typedef struct zlentry &#123; unsigned int prevrawlensize; /* 上一个链表节点占用的长度 */ unsigned int prevrawlen; /* 存储上一个链表节点的长度数值所需要的字节数 */ unsigned int lensize; /* 存储当前链表节点长度数值所需要的字节数 */ unsigned int len; /* 当前链表节点占用的长度 */ unsigned int headersize; /* 当前链表节点的头部大小(prevrawlensize + lensize)，即非数据域的大小 */ unsigned char encoding;/* 编码方式 */ unsigned char *p;/* 压缩链表以字符串的形式保存，该指针指向当前节点起始位置 */&#125; zlentry; 编码 encoding #define ZIP_STR_06B (0 &lt;&lt; 6) //长度小于等于 63 字节 #define ZIP_STR_14B (1 &lt;&lt; 6) //长度小于等于 16383 字节 #define ZIP_STR_32B (2 &lt;&lt; 6) //长度小于等于 4294967295 字节 什么时候使用 ziplist 存储?当 hash 对象同时满足以下两个条件的时候，使用 ziplist 编码: 所有的键值对的健和值的字符串长度都小于等于 64byte(一个英文字母一个字节); 哈希对象保存的键值对数量小于 512 个。 一个哈希对象超过配置的阈值(键和值的长度有&gt;64byte，键值对个数&gt;512 个)时， 会转换成哈希表(hashtable)。 hashtable(dict)在 Redis 中，hashtable 被称为字典(dictionary)，它是一个数组+链表的结构。前面我们知道了，Redis 的 KV 结构是通过一个 dictEntry 来实现的。Redis 又对 dictEntry 进行了多层的封装。 12345678typedef struct dictEntry &#123; void *key; /* key 关键字定义 */ union &#123; void *val; uint64_t u64; /* value 定义 */ int64_t s64; double d; &#125; v; struct dictEntry *next; /* 指向下一个键值对节点 */&#125; dictEntry; dictEntry 放到了 dictht（hashtable 里面）： 123456typedef struct dictht &#123; dictEntry **table; /* 哈希表数组 */ unsigned long size; /* 哈希表大小 */ unsigned long sizemask; /* 掩码大小，用于计算索引值。总是等于 size-1 */ unsigned long used; /* 已有节点数 */&#125; dictht; ht 放到了 dict 里面： 1234567typedef struct dict &#123; dictType *type; /* 字典类型 */ void *privdata; /* 私有数据 */ dictht ht[2]; /* 一个字典有两个哈希表 */ long rehashidx; /* rehash 索引 */ unsigned long iterators; /* 当前正在使用的迭代器数量 */&#125; dict; 从最底层到最高层 dictEntry——dictht——dict——OBJ_ENCODING_HT总结：哈希的存储结构注意：dictht 后面是 NULL 说明第二个 ht 还没用到。dictEntry* 后面是 NULL 说明没有 hash 到这个地址。dictEntry 后面是NULL 说明没有发生哈希冲突。 为什么要定义两个哈希表呢？ht[2] redis 的 hash 默认使用的是 ht[0]，ht[1]不会初始化和分配空间。哈希表 dictht 是用链地址法来解决碰撞问题的。在这种情况下，哈希表的性能取决于它的大小（size 属性）和它所保存的节点的数量（used 属性）之间的比率： 比率在 1:1 时（一个哈希表 ht 只存储一个节点 entry），哈希表的性能最好； 如果节点数量比哈希表的大小要大很多的话（这个比例用 ratio 表示，5 表示平均一个 ht 存储 5 个 entry），那么哈希表就会退化成多个链表，哈希表本身的性能优势就不再存在。 在这种情况下需要扩容。Redis 里面的这种操作叫做 rehash。rehash 的步骤： 为字符 ht[1]哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及 ht[0]当前包含的键值对的数量。扩展：ht[1]的大小为第一个大于等于ht[0].used * 2。 将所有的 ht[0]上的节点 rehash 到 ht[1]上，重新计算 hash 值和索引，然后放入指定的位置。 当 ht[0]全部迁移到了 ht[1]之后，释放 ht[0]的空间，将 ht[1]设置为 ht[0]表，并创建新的 ht[1]，为下次 rehash 做准备。 什么时候触发扩容？取决于负载因子，根据负载因子也会自动进行缩容。 12static int dict_can_resize = 1;static unsigned int dict_force_resize_ratio = 5; ratio = used / size，已使用节点与字典大小的比例dict_can_resize 为 1 并且 dict_force_resize_ratio 已使用节点数和字典大小之间的比率超过 1：5，触发扩容 应用场景 字符串类型的应用都满足 存储对象类型数据 List（列表）存储有序的字符串（从左到右），元素可以重复。可以充当队列和栈的角色。 操作命令1234567891011121314151617181920从队列左边入值：lpush queue a从队列右边入值：rpush queue b从队列左边出值：lpop queue从队列右边出值：rpop queue查看队列长度：llen queue展示队列指定范围值：lrange queue 0 10取指定下标值：lindex queue 0 存储原理在早期的版本中，数据量较小时用 ziplist 存储，达到临界值时转换为 linkedlist 进行存储，分别对应 OBJ_ENCODING_ZIPLIST 和OBJ_ENCODING_LINKEDLIST 。3.2 版本之后，统一用 quicklist 来存储。quicklist 存储了一个双向链表，每个节点都是一个 ziplist。 quicklistquicklist（快速列表）是 ziplist 和 linkedlist 的结合体。quicklist.h，head 和 tail 指向双向列表的表头和表尾 12345678typedef struct quicklist &#123; quicklistNode *head; /* 指向双向列表的表头 */ quicklistNode *tail; /* 指向双向列表的表尾 */ unsigned long count; /* 所有的 ziplist 中一共存了多少个元素 */ unsigned long len; /* 双向链表的长度，node 的数量 */ int fill : 16; /* fill factor for individual nodes */ unsigned int compress : 16; /* 压缩深度，0：不压缩； */&#125; quicklist; redis.conf 相关参数： 参数 含义 list-max-ziplist-size（fill） 正数表示单个 ziplist 最多所包含的 entry 个数。负数代表单个 ziplist 的大小，默认 8k。-1：4KB；-2：8KB；-3：16KB；-4：32KB；-5：64KB list-compress-depth（compress） 压缩深度，默认是 0。1：首尾的 ziplist 不压缩；2：首尾第一第二个 ziplist 不压缩，以此类推 quicklistNode 中的 * zl 指向一个 ziplist，一个 ziplist 可以存放多个元素。 123456789101112typedef struct quicklistNode &#123; struct quicklistNode *prev; /* 前一个节点 */ struct quicklistNode *next; /* 后一个节点 */ unsigned char *zl; /* 指向实际的 ziplist */ unsigned int sz; /* 当前 ziplist 占用多少字节 */ unsigned int count : 16; /* 当前 ziplist 中存储了多少个元素，占 16bit（下同），最大 65536 个 */ unsigned int encoding : 2; /* 是否采用了 LZF 压缩算法压缩节点，1：RAW 2：LZF */ unsigned int container : 2; /* 2：ziplist，未来可能支持其他结构存储 */ unsigned int recompress : 1; /* 当前 ziplist 是不是已经被解压出来作临时使用 */ unsigned int attempted_compress : 1; /* 测试用 */ unsigned int extra : 10; /* 预留给未来使用 */&#125; quicklistNode; 应用场景 用户消息时间线 消息队列 Set（集合）String 类型的无序集合，最大存储数量 2^32-1（40 亿左右）。 操作命令1234567891011121314151617181920添加一个或者多个元素:sadd myset a b c d e f g获取所有元素:smembers myset统计元素个数:scard myset随机获取一个元素:srandmember key随机弹出一个元素:spop myset移除一个或者多个元素:srem myset d e f查看元素是否存在:sismember myset a 存储原理Redis 用 intset 或 hashtable 存储 set。如果元素都是整数类型，就用 inset 存储。如果不是整数类型，就用 hashtable（数组+链表的存来储结构）。KV 怎么存储 set 的元素？key 就是元素的值，value 为 null。如果元素个数超过 512 个，也会用 hashtable 存储。 应用场景 抽奖 点赞、签到、打卡 商品标签 商品筛选 ZSet（有序集合）sorted set，有序的 set，每个元素有个 score。score 相同时，按照 key 的 ASCII 码排序。数据结构对比： 数据结构 是否允许重复元素 是否有序 有序实现方式 列表list 是 是 索引下标 集合set 否 否 无 有序集合zset 否 是 分值score 操作命令123456789101112131415161718192021222324252627添加元素:zadd myzset 10 java 20 php 30 ruby 40 cpp 50 python获取全部元素:zrange myzset 0 -1 withscoreszrevrange myzset 0 -1 withscores根据分值区间获取元素:zrangebyscore myzset 20 30移除元素,也可以根据 score rank 删除:zrem myzset php cpp统计元素个数:zcard myzset分值递增:zincrby myzset 5 python根据分值统计个数:zcount myzset 20 60获取元素 rank:zrank myzset java获取元素 score:zsocre myzset java 存储原理同时满足以下条件时使用 ziplist 编码： 元素数量小于 128 个 所有 member 的长度都小于 64 字节 在 ziplist 的内部，按照 score 排序递增来存储。插入的时候要移动之后的数据。 对应 redis.conf 参数：zset-max-ziplist-entries 128zset-max-ziplist-value 64 超过阈值之后，使用 skiplist+dict 存储。 skiplist我们先来看一下有序链表：在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为 O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。而二分查找法只适用于有序数组，不适用于链表。假如我们每相邻两个节点增加一个指针（或者理解为有三个元素进入了第二层），让指针指向下下个节点。这样所有新增加的指针连成了一个新的链表，但它包含的节点个数只有原来的一半（上图中是 7, 19, 26）。在插入一个数据的时候，决定要放到那一层，取决于一个算法（在 redis 中 t_zset.c 有一个 zslRandomLevel 这个方法）。现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中的下一层进行查找。比如，我们想查找 23，查找的路径是沿着下图中标红的指针所指向的方向进行的： 23 首先和 7 比较，再和 19 比较，比它们都大，继续向后比较。 但 23 和 26 比较的时候，比 26 要小，因此回到下面的链表（原链表），与 22比较。 23 比 22 要大，沿下面的指针继续向后和 26 比较。23 比 26 小，说明待查数据 23 在原链表中不存在。 在这个查找过程中，由于新增加的指针，我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。这就是跳跃表。 为什么不用 AVL 树或者红黑树？因为 skiplist 更加简洁。 123456789101112131415161718192021typedef struct zskiplistNode &#123; sds ele; /* zset 的元素 */ double score; /* 分值 */ struct zskiplistNode *backward; /* 后退指针 */ struct zskiplistLevel &#123; struct zskiplistNode *forward; /* 前进指针，对应 level 的下一个节点 */ unsigned long span; /* 从当前节点到下一个节点的跨度（跨越的节点数） */ &#125; level[]; /* 层 */&#125; zskiplistNode;typedef struct zskiplist &#123; struct zskiplistNode *header, *tail; /* 指向跳跃表的头结点和尾节点 */ unsigned long length; /* 跳跃表的节点数 */ int level; /* 最大的层数 */&#125; zskiplist;typedef struct zset &#123; dict *dict; zskiplist *zsl;&#125; zset; 随机获取层数的函数： 123456int zslRandomLevel(void) &#123; int level = 1; while ((random()&amp;0xFFFF) &lt; (ZSKIPLIST_P * 0xFFFF)) level += 1; return (level&lt;ZSKIPLIST_MAXLEVEL) ? level : ZSKIPLIST_MAXLEVEL;&#125; 应用场景 排行榜 其他数据介绍常用的是以上五种数据类型，其他四种在这里就简单介绍。 BitMapBitMap 就是通过一个 bit 位来表示某个元素对应的值或者状态, 其中的 key 就是对应元素本身，实际上底层也是通过对字符串的操作来实现。Redis 从 2.2 版本之后新增了setbit, getbit, bitcount 等几个 bitmap 相关命令。虽然是新命令，但是本身都是对字符串的操作，一个字节由 8 个二进制位组成。 常用命令1234567891011121314151617设置值：set k1 a获取 value 在 offset 处的值（a 对应的 ASCII 码是 97，转换为二进制数据是 01100001）：getbit k1 0修改二进制数据（b 对应的 ASCII 码是 98，转换为二进制数据是 01100010）：setbit k1 6 1setbit k1 7 0get k1统计二进制位中 1 的个数：bitcount k1获取第一个 1 或者 0 的位置：bitpos k1 1bitpos k1 0 BITOP 命令支持 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种参数：BITOP AND destkey srckey1 … srckeyN ，对一个或多个 key 求逻辑与，并将结果保存到 destkeyBITOP OR destkey srckey1 … srckeyN，对一个或多个 key 求逻辑或，并将结果保存到 destkeyBITOP XOR destkey srckey1 … srckeyN，对一个或多个 key 求逻辑异或，并将结果保存到 destkeyBITOP NOT destkey srckey，对给定 key 求逻辑非，并将结果保存到 destkey 应用场景 用户访问统计 在线用户统计 HyperloglogsRedis 的基数统计，这个结构可以非常省内存的去统计各种计数，比如注册 IP 数、每日访问 IP 数、页面实时UV）、在线用户数等。但是它也有局限性，就是只能统计数量，而没办法去知道具体的内容是什么并且存在一定的误差。 GeoRedis 的 GEO 特性在 Redis 3.2 版本中推出， 这个功能可以将用户给定的地理位置信息储存起来， 并对这些信息进行操作。 Streams5.0 推出的数据类型。支持多播的可持久化的消息队列，用于实现发布订阅功能，借鉴了 kafka 的设计。 总结数据对象总结 对象 对象 type 属性值 type 命令输出 底层可能的存储结构 object encoding 字符串对象 OBJ_STRING “string” OBJ_ENCODING_INT OBJ_ENCODING_EMBSTR OBJ_ENCODING_RAW int embstr raw 列表对象 OBJ_LIST “list” OBJ_ENCODING_QUICKLIST quicklist 哈希对象 OBJ_HASH “hash” OBJ_ENCODING_ZIPLIST OBJ_ENCODING_HT ziplist hashtable 集合对象 OBJ_SET “set” OBJ_ENCODING_INTSET OBJ_ENCODING_HT intset hashtable 有序集合对象 OBJ_ZSET “zset” OBJ_ENCODING_ZIPLIST OBJ_ENCODING_SKIPLIST ziplist skiplist（包含 ht） 编码转换总结 应用场景总结 缓存——提升热点数据的访问速度 共享数据——数据的存储和共享的问题 全局 ID —— 分布式全局 ID 的生成方案（分库分表） 分布式锁——进程间共享数据的原子操作保证 在线用户统计和计数 队列、栈——跨进程的队列/栈 消息队列——异步解耦的消息机制 服务注册与发现 —— RPC 通信机制的服务协调中心（Dubbo 支持 Redis） 购物车 新浪/Twitter 用户消息时间线 抽奖逻辑（礼物、转发） 点赞、签到、打卡 商品标签 用户（商品）关注（推荐）模型 电商产品筛选 排行榜]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SqlSessionTemplate为什么线程安全]]></title>
    <url>%2F2019%2F11%2F06%2FSqlSessionTemplate%E4%B8%BA%E4%BB%80%E4%B9%88%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[最近在看Mybatis源码，对于理解SqlSessionTemplate是如何保证线程安全的网上的文章不多。希望通过本文能够帮助大家清楚理解，类关系图如下： DefaultSqlSession与SqlSessionManager解析在Mybatis中SqlSession默认有DefaultSqlSession和SqlSessionManager两个实现类 DefaultSqlSession是真正的实现类调用Executor，但不是线程安全的。 Mybatis又实现了对SqlSession和SQLSessionFactory的封装类SqlSessionManager，线程安全并通过localSqlSession实现复用从而提高性能。 1private ThreadLocal&lt;SqlSession&gt; localSqlSession = new ThreadLocal(); SqlSessionManager通过SqlSessionInterceptor实现对DefaultSqlSession代理调用。 123456789101112131415161718192021222324252627282930313233private class SqlSessionInterceptor implements InvocationHandler &#123; public SqlSessionInterceptor() &#123; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //由调用者决定当前线程是否复用 SqlSession SqlSession sqlSession = (SqlSession)SqlSessionManager.this.localSqlSession.get(); if (sqlSession != null) &#123; try &#123; return method.invoke(sqlSession, args); &#125; catch (Throwable var12) &#123; throw ExceptionUtil.unwrapThrowable(var12); &#125; &#125; else &#123; //如果不复用，则每次调用都新建 SqlSession 并使用后销毁 SqlSession autoSqlSession = SqlSessionManager.this.openSession(); Object var7; try &#123; Object result = method.invoke(autoSqlSession, args); autoSqlSession.commit(); var7 = result; &#125; catch (Throwable var13) &#123; autoSqlSession.rollback(); throw ExceptionUtil.unwrapThrowable(var13); &#125; finally &#123; autoSqlSession.close(); &#125; return var7; &#125; &#125; &#125; DefaultSqlSession和SqlSessionManager之间的区别： 1、单例模式下DefaultSqlSession是线程不安全的，而SqlSessionManager是线程安全的； 2、SqlSessionManager可以选择通过localSqlSession这个ThreadLocal变量，记录与当前线程绑定的SqlSession对象，供当前线程循环使用，从而避免在同一个线程多次创建SqlSession对象造成的性能损耗； 3、使用DefaultSqlSession为了保证线程安全需要为每一个操作都创建一个SqlSession对象，其性能可想而知； SqlSessionTemplate是怎么保证线程安全SqlSessionTemplate是MyBatis专门为Spring提供的，支持Spring框架的一个SqlSession获取接口。主要是为了继承Spring，并同时将是否共用SqlSession的权限交给Spring去管理。 1、通过创建sqlSessionProxy代理类，将调用导向SqlSessionInterceptor的invoke方法。 12345678public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) &#123; Assert.notNull(sqlSessionFactory, &quot;Property &apos;sqlSessionFactory&apos; is required&quot;); Assert.notNull(executorType, &quot;Property &apos;executorType&apos; is required&quot;); this.sqlSessionFactory = sqlSessionFactory; this.executorType = executorType; this.exceptionTranslator = exceptionTranslator; this.sqlSessionProxy = (SqlSession)Proxy.newProxyInstance(SqlSessionFactory.class.getClassLoader(), new Class[]&#123;SqlSession.class&#125;, new SqlSessionTemplate.SqlSessionInterceptor()); &#125; 2、获取线程私有的SqlSession，调用DefaultSqlSession对应的实际方法上 1234567891011121314151617181920212223242526272829303132333435363738394041private class SqlSessionInterceptor implements InvocationHandler &#123; private SqlSessionInterceptor() &#123; &#125; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //获取同一个线程的复用sqlSession，如果没有则新生成一个并存到线程私有存储中 SqlSession sqlSession = SqlSessionUtils.getSqlSession(SqlSessionTemplate.this.sqlSessionFactory, SqlSessionTemplate.this.executorType, SqlSessionTemplate.this.exceptionTranslator); Object unwrapped; try &#123; //实际调用DefaultSession的对应方法 Object result = method.invoke(sqlSession, args); //判断当前sqlSession是否被Spring管理，如果没有直接commit if (!SqlSessionUtils.isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) &#123; sqlSession.commit(true); &#125; unwrapped = result; &#125; catch (Throwable var11) &#123; unwrapped = ExceptionUtil.unwrapThrowable(var11); if (SqlSessionTemplate.this.exceptionTranslator != null &amp;&amp; unwrapped instanceof PersistenceException) &#123; SqlSessionUtils.closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); sqlSession = null; Throwable translated = SqlSessionTemplate.this.exceptionTranslator.translateExceptionIfPossible((PersistenceException)unwrapped); if (translated != null) &#123; unwrapped = translated; &#125; &#125; throw (Throwable)unwrapped; &#125; finally &#123; if (sqlSession != null) &#123; //正常返回将线程私有sqlSession调用次数减一 SqlSessionUtils.closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory); &#125; &#125; return unwrapped; &#125; &#125; 3、查看getSqlSession()方法就知道每个线程对应的SqlSession都是私有的不会被共用，所以SqlSessionTemplate是线程安全的。 12345678910111213141516171819public static SqlSession getSqlSession(SqlSessionFactory sessionFactory, ExecutorType executorType, PersistenceExceptionTranslator exceptionTranslator) &#123; Assert.notNull(sessionFactory, &quot;No SqlSessionFactory specified&quot;); Assert.notNull(executorType, &quot;No ExecutorType specified&quot;); //从线程私有存储中获取SqlSession SqlSessionHolder holder = (SqlSessionHolder)TransactionSynchronizationManager.getResource(sessionFactory); SqlSession session = sessionHolder(executorType, holder); if (session != null) &#123; return session; &#125; else &#123; if (LOGGER.isDebugEnabled()) &#123; LOGGER.debug(&quot;Creating a new SqlSession&quot;); &#125; //没有则新建一个DefaultSqlSession session = sessionFactory.openSession(executorType); //存到线程私有存储中 registerSessionHolder(sessionFactory, executorType, exceptionTranslator, session); return session; &#125; &#125; 总结究其根本SqlSession真正的实现类只有DefaultSqlSession，SqlSessionManager和SqlSessionTemplate都是通过代理转发到DefaultSqlSession对应方法。 单例模式下的DefaultSqlSession不是线程安全的，SqlSessionManager和SqlSessionTemplate线程安全的根本就是每一个线程对应的SqlSession都是不同的。如果每一个操作都创建一个SqlSession对象，操作完又进行销毁导致性能极差。通过线程私有ThreadLocal存储SqlSession进行复用，从而提高性能。 参考：https://blog.csdn.net/bntx2jsqfehy7/article/details/79441545#commentsedit]]></content>
      <categories>
        <category>Mybatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM性能调优监测工具]]></title>
    <url>%2F2019%2F11%2F05%2FJVM%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%E7%9B%91%E6%B5%8B%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[查看正在运行的程序jps主要用来输出JVM中运行的进程状态信息。语法格式如下： 123456789jps [options] [hostid]options:-q 不输出类名、Jar名和传入main方法的参数-m 输出传入main方法的参数-l 输出main类或Jar的全限名-v 输出传入JVM的参数hostid：不填，默认为本机 CPU飙升排查jstack主要用来查看某个Java进程内的线程堆栈信息。 通过 top 命令找到 CPU 消耗最高的进程，并记住进程 ID。 再次通过 top -Hp [进程 ID] 找到 CPU 消耗最高的线程 ID，并记住线程 ID. 通过 JDK 提供的 jstack 工具 dump 线程堆栈信息到指定文件中。具体命令：jstack -l [进程 ID] &gt;jstack.log。 由于刚刚的线程 ID 是十进制的，而堆栈信息中的线程 ID 是16进制的，因此我们需要将10进制的转换成16进制的，并用这个线程 ID 在堆栈中查找。使用 printf “%x\n” [十进制数字] ，可以将10进制转换成16进制。 通过刚刚转换的16进制数字从堆栈信息里找到对应的线程堆栈。就可以从该堆栈中看出端倪。 C2 编译器执行编译时也会抢占 CPU，什么是 C2编译器呢？当 Java 某一段代码执行次数超过10000次（默认）后，就会将该段代码从解释执行改为编译执行，也就是编译成机器码以提高速度。而这个 C2编译器就是做这个的。如何解决呢？项目上线后，可以先通过压测工具进行预热，这样，等用户真正访问的时候，C2编译器就不会干扰应用程序了。如果是 GC 线程导致的，那么极有可能是 Full GC ，那么就要进行 GC 的优化。 内存使用情况监测jmap用来查看堆内存使用状况。 使用jmap -heap pid查看进程堆内存使用情况，包括使用的GC算法、堆配置参数和各代中堆内存使用情况。 使用jmap -histo[:live] pid查看堆内存中的对象数目、大小统计直方图，如果带上live则只统计活对象。 使用jmap -permstat pid 打印classload和jvm heap长久层的信息. 包含每个classloader的名字,活泼性,地址,父classloader和加载的class数量. 另外,内部String的数量和占用内存数也会打印出来。 JVM统计监测 jstat命令可以查看堆内存各部分的使用量，以及加载类的数量。 1jstat [ generalOption | outputOptions vmid [interval[s|ms] [count]] ] 比如下面输出的是GC信息，采样时间间隔为250ms，采样数为4： 123456 jstat -gc 14157 250 4S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 192.0 192.0 64.0 0.0 6144.0 1854.9 32000.0 4111.6 55296.0 25472.7 702 0.431 3 0.218 0.649192.0 192.0 64.0 0.0 6144.0 1972.2 32000.0 4111.6 55296.0 25472.7 702 0.431 3 0.218 0.649192.0 192.0 64.0 0.0 6144.0 1972.2 32000.0 4111.6 55296.0 25472.7 702 0.431 3 0.218 0.649192.0 192.0 64.0 0.0 6144.0 2109.7 32000.0 4111.6 55296.0 25472.7 702 0.431 3 0.218 0.649 我们知道堆内存 = 年轻代 + 年老代 + 永久代年轻代 = Eden区 + 两个Survivor区（From和To） 1234567S0C、S1C、S0U、S1U：Survivor 0/1区容量（Capacity）和使用量（Used）EC、EU：Eden区容量和使用量OC、OU：年老代容量和使用量PC、PU：永久代容量和使用量YGC、YGT：年轻代GC次数和GC耗时FGC、FGCT：Full GC次数和Full GC耗时GCT：GC总耗时 查看文件字节码这里扩展一下如何查看文件字节码 12345Javac Foo.java 将文件解析成class字节码文件Javap Foo.class 打印所有非私有的字段和方法Javap -p Foo.class 还将打印私有的字段和方法Javap -v Foo.class 尽可能打印所有信息Javap -c Foo.class 打印方法对应的字节码 参考链接：https://mp.weixin.qq.com/s/-5vdgexMyoiMRVPlOO88Swhttps://mp.weixin.qq.com/s/LJGWPRBc_BJLTfopi7Lttg]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[linux基础指令基础]]></title>
    <url>%2F2019%2F11%2F01%2Flinux%E5%9F%BA%E7%A1%80%E6%8C%87%E4%BB%A4%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[常用命令1查看指令帮助：help 文件相关123456789101112131415161718192021查看当前目录内容：ls、ll清除终端内容：clear进入下一级目录：cd创建文件：touch 1.txt创建文件夹：mkdir 11复制文件或文件夹：cp 被复制的文件 复制的路径查看当前路径：pwd看当前用户所有历史指令(~/.bash_history,注：系统退出后才会保存)：history从历史指令中找最后一条以xx开头的指令并执行：!xx强制删除指定文件或文件夹：rm -rf 文件名查看文件内容并显示行号：cat -n 文件名 或 nl 文件名统计文件行数：wc -l 文件名查看文件前10行内容：head -10 文件名 查看文件后10行并实时展示最增内容：tail -10 -f 文件名分页查看文件内容：more 文件名（下一页：空格、上一页：b）分页查询并且搜索文件内容：less 文件名 （进入后按q退出，/加搜索内容并且高亮显示）比较两个文档不同：diff 文件1 文件2为路径创建短连接：ls -s 路径 短连接名(可以通过 cd 短连接名 直接进入对应路径)按100行分割一次，a表示分割后的文件名前缀，后缀是自动生成的：split -100 文件名 aaa文件按固定大小分割：split -b 1M 文件名文本合并，可以借助cat：cat a* &gt; a.txt 用户相关123456增加用户：useradd 用户名设置密码：passwd 用户名删除用户：userdel 用户名查看当前系统用户：uname -a给当前用户文件增加读写执行权限：chmod u+rwx 文件名 （u表示当前用户，还有g(group)、o(others)；如果省略，则表示给u、g、o均添加权限）给所有用户增加全部权限：chmod 777 文件名 系统相关123456789101112查看cpu、内存实时状态：top查看磁盘使用情况：df -h查看内存使用情况：free -h查看当前执行进程：ps -ef查看网络系统的状态信息：netstat -a强制终止指定某个进程：kill -9 pid查看系统时间：date查询命令位置：which java查看ip、网卡等信息：ifconfig系统日志的位置：/var/log/messages向系统日志写消息：logger &quot;xxxxx&quot; 安装程序命令：yum、wget、rpm、apt-get...(有的适用于CentOS，有的RedHat，有的Ubuntu) 指令123456789输出从2-10：seq 2 10保存脚本执行结果并同时并查看：sh a.sh | tee b.txt退出脚本：exit (正常退出返回0，异常退出返回非零)为指令设置别名：alias 1=&apos;df -h&apos;(重启终端还能使用别名需要将别名设置放到 ~/.bashrc文件中)接触别名：unalias 1定时执行某脚本：crontab 1 * * * * ~/1.sh（每小时1分的时候执行1.sh这个脚本）查看当前用户定时器设置：crontab -l删除当前用户定时器设置：crontab -r vim指令插入模式1234567891011G：跳到文档最后一行gg：调到文档第一行x：删除当前光标所在的字符nx：从光标向后删除n个字符dd：删除当前行ndd：删除当前及后n行D：删除当前行光标后边所有字符r：replace，替换当前光标的字符yy：粘贴p：paste复制u：撤销 命令行模式1234567i：从当前行进入插入模式o：从下一行进入插入模式q：退出vim模式q!：退出并不保存更改内容wq：退出并保持更改内容/**：查询字符串所在的位置，按下回车后用n进行搜索下一个，N返回上一个nu：设置行号 打包/压缩123456789打包当前目录所有文件：tar -cvf a.tar * 压缩包：gzip a.tar打包并压缩当前文件夹内容：tar -czvf a.tar.gz * 解压：tar -xvf a.tar查看内部文件列表：tar -tf a.tar解压并解包：tar -zxvf a.tar.gzzip压缩：zip target.zip sourceFilezip解压到指定目录：unzip zipFile -d dir Shell解释器1脚本开头：#!/bin/bash 或 #!/bin/sh(默认可以不写) 执行脚本12./shell.sh(需要执行权限)sh shell.sh 或 bash shell.sh(读取执行～只需要读取权限) 变量1234567a=3：声明变量a，并赋值3$a：等价于$&#123;a&#125;。但是 $&#123;a&#125;ook 和 $aook就不一样$?：上一个指令是否正确执行，上一个脚本是否正常退出(1-不正确，0-正确)$0：文件名$1～$9：第几个参数值(echo $&#123;1:-daily&#125;，取脚本第一个参数，如果没有，则赋值daily)$#：一共几个参数$*：所有入参 符号12345678910111213141516171819202122|：管道符号：把前边的输出作为后边的输入ls -s|sort -nr&gt;：覆盖输出；echo &quot;abc&quot; &gt; a.txt&gt;&gt;：拼接输出；echo &quot;abc&quot; &gt;&gt; a.txt;：一行执行多条语句的分隔符 cat a.txt ; ls&amp;&amp;：前边语句成功才会执行后边语句 cat a.txt &amp;&amp; ls||：前边语句失败才会执行后边语句lo || ls$$：输出当前脚本的执行进程idecho $$&quot;&quot;：输出变量值 a=10;echo &quot;$a&quot; -&gt; 10&apos;&apos;：输出本身 a=10;echo &apos;$a&apos; -&gt; $a``：执行内部语法。echo `date` 等价于 date&amp;&gt;filename：把标准输出和错误输出都重定向到文件filenamesh abc.sh &amp;&gt; a.txt read输入12345678910-t 阻塞时间-s 隐藏输入的字符-p 给出提示符-n 读取字符的个数，个数到达临界之后会自动执行把接下来输入的数据赋值给abc：read abc在10秒内最多输入3个字符：read -t 10 -n 3 -p &apos;请输入密码：&apos; passecho &quot;密码是：$pass&quot; 计算器123456789整数： expr 10 + 20 #注意+号两边的空格 echo $[10 + 20] echo $((10 + 20)) echo $[10 % 3] 取余 echo $[10 \* 2] 乘法需要对*转义bc计算器： echo &quot;scale=2;(1.2+2.3)/1&quot; | bc #把前边的输出作为后边的输入,scale只对乘除有效所以需要/1（2;表示保留两位小数） 条件判断1234567891011121314151617181920212223[ -e a.txt ] ：方括号必须有左右空格。[ ! -e a.txt ] ：感叹号！表示取反文件判断： -e exist 文件是否存在 -d directory 是不是目录 -f file 是不是文件权限： -r 是否有读权限 -w 是否有写权限 -x 是否有执行权限整数比较： -eq（equal） -ne (not equal) -gt (greater than) -lt (lesser than) -ge (greater or equal) -le (lesser or equal)小数比较：借助bc计算器 bc判断true返回1，false返回0 [ `echo &quot;1.2 &gt; 1&quot; | bc` -eq 1 ] &amp;&amp; echo &apos;大于&apos;字符串： 直接用 = 或 != 进行判断 [ &quot;1&quot; = &quot;1&quot; ] &amp;&amp; echo &quot;相等&quot; if判断123456789101112131415161718格式1：if [ 条件 ];then echo &quot;xxx&quot;fi格式2：if [ 条件1 ];then echo &quot;xxx&quot;elif [ 条件2 ];then echo &quot;yyy&quot;else echo &quot;zzz&quot;fi#拓展：[]和[[]]的其中一点区别（双括号要强大的多）if [ $a -ne 1] &amp;&amp; [ $a != 2 ] 等价于： if [[ $a != 1 &amp;&amp; $a != 2 ]] 等价于： if [ $a -ne 1 -a $a != 2 ] （其中-a是and，-o是or） for循环123456789101112131415161718192021222324示例1： for i in 1 2 3 4 5 do if [ $i -eq 2 ];then continue else echo &quot;$i&quot; sleep 1 fi done示例2： for i in $(cat 1.txt) do ping -c 2 $i done 注：-c表示最多ping几次。windows上是-n示例3：（(条件)） for (( i=1;i&lt;11;i++ )) do echo $i sleep 1 done 注：i++相当于i=i+1，只能在(())中使用，等价于：let &apos;i+=1&apos; case选择12345678910111213141516171819202122echo &apos;请输入城市：&apos;read citycase $city in &apos;上海&apos;) echo &apos;上海：35度&apos; ;; &apos;北京&apos;) echo &apos;北京：20度&apos; ;; *) echo &apos;未知城市&apos;esac #注：case倒叙 示例2：case接受简单正则case $1 in [a-z]|[A-Z]) echo &quot;字母&quot; ;; [0-9]) echo &quot;数字&quot; ;;esac while循环123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566示例1：输入数字，并求1到该数字的和 sum=0 i=0 while [ $i -lt $1 ] do sum=$(($sum+$i)) i=$[$i+1] done echo &quot;从1加到$1的和是：$sum&quot; 示例2：模拟菜单 while [ 1 ] do cat &lt;&lt;EOF #&lt;&lt;叫输入重定向,EOF是随便定义的（成对即可） 1.执行1.sh 2.执行2.sh q.退出EOF #末尾的EOF必须顶行写，且前后均不能有空格或tab read -p &apos;请选择：&apos; key case $key in 1) clear echo &apos;执行1.sh&apos; ;; 2) clear echo &apos;执行2.sh&apos; ;; q) clear echo &apos;退出&apos; break ;; esac done 示例3： num=0 while [ $# -gt 0 ] do num=$(($1+num)) echo &quot;剩余参数：$*&quot; shift #将参数向左偏移一个 done echo &quot;总和：$num&quot; 结果如下： [root@localhost xuan]# sh 5.sh 1 2 3 4 5 剩余参数：1 2 3 4 5 剩余参数：2 3 4 5 剩余参数：3 4 5 剩余参数：4 5 剩余参数：5 总和：15示例4：死循环 while true # 或 while : 或 while [ 1 ] do let i++ echo $i sleep 1 done 函数123456789101112131415161718示例1：两数求和function add()&#123; echo $(($1+$2))&#125;add 10 20示例2：遍历指定目录下所有目录function listFiles() &#123; for file in `ls $1` do dir=$1&quot;/&quot;$file if [ -d $dir ];then ls $dir listFiles $dir fi done&#125;listFiles $1 字体颜色和特效1234567echo -e &quot;\033[背景颜色;字体颜色;特效 \033[0m&quot;示例1：echo -e &quot;\033[32;41;5m 文本字体 \033[0m&quot; #注：最后的0m表示关闭属性，&quot;\033[&quot;是固定前缀和后缀示例2：read -t 10 -p &quot;`echo -e &quot;\033[32;41;5m 请输入密码 \033[0m&quot;`&quot; pass 去重/排序12345678相邻行去重：cat a.txt | uniq 相邻行去重并统计次数：cat a.txt | uniq -c所有行排序，然后相邻行去重并统计次数：cat a.txt | sort | uniq -c -t:表示以:为分隔符(默认空格),-k3表示以第3列排序(默认整行),-r表示反转：cat a.txt | sort -t: -k3 -r 文件搜索123456789101112简单正则+通配符：find . -name &quot;a[0-9]*&quot;有的大写，有的小写：find / -size 10c/10k/10M/10G d表示目录，f表示文件，l表示链接：find . -type d/f/l -mtime表示修改时间；-3表示3天以内，+3表示3天前：find . -mtime -3/+3 指定用户：find . -user rootxargs是将管道前的搜索作为参数传给后边，此处如果不加xargs，则后边的指令是没有意义的：find . -name &quot;[a-b]*.sh&quot; | xargs rm -rf 文本·行搜索1234567891011121314151617-v invert 反向选择-i ignore 忽略大小写-w word 单词匹配-n 显示行号&quot;^xx&quot; 某行以xx开头（行首匹配） grep &quot;^user&quot; a.txt 或：grep -E &quot;^user&quot; a.txt &quot;xx$&quot; 某行以xx结尾 grep &quot;user$&quot; a.txt-E regex 正则 grep -E &quot;sbi*&quot; /etc/passwd grep -Ein &quot;(linux)&#123;2&#125;&quot; a.txt #注：()表示单元 grep -Ein &quot;(linux)+&quot; a.txt grep -Ein &quot;[0-9]+&quot; a.txt grep -E &quot;\*&quot; a.txt #注：\转义等价于：grep &quot;*&quot; a.txt (默认不支持正则) grep -E &quot;[0-9]+\.+[0-9]+\.[0-9]+\.[0-9]+&quot; a.txt #注：搜索ip grep -E &quot;^$&quot; a.txt #注：找空行 grep -E &quot;^[^d]&quot; a.txt #注：找不是d开头的行。^放在[]外边表示&quot;以xx开头&quot;，放在[]中表示&quot;除了&quot; 文本·列操作1234567891011121314151617181920-d 指定分隔符，delimiter-f 指定第几列-c 按字符截取示例1：以冒号分割后，取第1列和第3列 cut -d &apos;:&apos; -f 1,3 /etc/passwd #注：1,3表示第1列和第3列；1-3表示第1列到第3列；3-表示第3列以后。示例2： cut -c 1-4 /etc/passwd #截取第1列到第4列字符，以字符为单位示例3：查找linux系统中所有不能登录的用户名 cat /etc/passwd | grep /sbin/nologin | cut -d &apos;:&apos; -f 1 #注1： cat /etc/passwd | cut -d &apos;:&apos; -f 1 等价于cut -d &apos;:&apos; -f 1 /etc/passwd #注2：/bin/bash结尾的用户是可以登录的；/sbin/nologin结尾的是系统默认用户，不可以登录 #注3：cut用空格分割的话，每一个空格都会计数1次。 Mem: 1837 107 298 0 1431 1532 free -m | grep -i mem | cut -d &quot; &quot; -f 12 -&gt; 1837 free -m | grep -i mem | cut -d &quot; &quot; -f 2 -&gt; 空格 文本操作123456789101112131415161718192021222324252627awk是一门语言，是3个创始人的姓名首字母缩写而成printf不换行打印；print 换行打印 printf &apos;%3s %3s\n&apos; 11 12 -&gt; %ns是字符串，n默认是1可省略 printf &apos;%2i %2i\n&apos; 11 12 -&gt; %ni是数字 printf &apos;%.2f\n&apos; 0.1234 -&gt; 0.12 &quot;%.nf&quot;是格式化浮点数3.1 awk (默认分隔符是空格)3.2 awk &apos;条件1 &#123;动作1&#125; 条件2 &#123;动作2&#125;&apos; 文件名 echo &quot;0.123+0.123&quot; | bc | awk &apos;&#123;printf &quot;%.2f\n&quot;, $0&#125;&apos; #注：$0表示整行，$1表示第一列... df -h | grep /dev/sda2 | awk &apos;&#123;print &quot;sda2的磁盘使用率：&quot; $3&#125;&apos; free -m | sed -n &apos;2p&apos; | awk &apos;&#123;print &quot;total:&quot;$2&quot;\n&quot; &quot;userd:&quot;$3&quot;\n&quot;&#125;&apos;3.3 awk 选项 &apos;条件1 &#123;动作1&#125; 条件2 &#123;动作2&#125;&apos; 文件名 cat /etc/passwd | awk -F &quot;:&quot; &apos;&#123;printf $1&#125;&apos; #注：-F指定分隔符，field-separator cat /etc/passwd | awk &apos;BEGIN &#123;FS=&quot;:&quot;&#125;&#123;print $1&#125; END &#123;print &quot;结束了&quot;&#125;&apos; #注1：FS是field separator #注2：BEGIN是在awk之前执行，一般用于修改内置变量；END是awk执行完之后执行3.4 NR 行号，是awk中的一个常量（num row）；NF列号（num field） df -h | awk &apos;NR==2 &#123;print $1&#125;&apos; df -h | awk &apos;(NR&gt;=2 &amp;&amp; NR &lt;=4) &#123;printf $1&quot;\n&quot;&#125;&apos; cat a.txt | awk &apos;END&#123;print NR&#125;&apos; #统计有多少行 cat a.txt | awk &apos;&#123;if(NR==1)&#123;print $0&#125;&#125;&apos; #打印第一行 cat a.txt | awk &apos;$0~/java/&apos; #$0表示整行，~表示匹配，/被匹配文本/ cat a.txt | awk &apos;$0!~/java/&apos; # !~表示不匹配 文本·行操作1234567891011121314151617181920-n和p一块使用： df -h | sed -n &apos;2&apos;p #查询第2行，p表示print，也可以写成sed -n &apos;2p&apos;d：删除 df -f | sed &apos;2&apos;d #删除第2行eg: cat a.txt | sed -n &apos;1,/java/&apos;p #搜索第1行到包含java的那一行。 cat a.txt | sed -n &apos;10,$&apos;p #搜索第10行到最后一行。等价于：sed -n &apos;10,$&apos;p a.txt cat a.txt | sed &apos;2,3&apos;d #搜索除了第2行到第3行的数据a：在下边插入 df -h | sed &apos;2a xuan&apos;i：在上边插入 df -h | sed &apos;2i xuan&apos;c：替换 df -h | sed &apos;2c xuan&apos; #把第2行替换成xuans/旧串/新串/g df -h | sed &apos;s/oldStr/newStr/g&apos;-i：对源文件进行修改(危险) sed -i &apos;s/oldStr/newStr/g&apos; a.txt sed -i &apos;2i 玄&apos; 5.txt-e：指定多个条件 简单例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748491.判断两个数是否相等 if [ $1 -eq $2 ];then 注：if后有空格 echo &quot;$1等于$2&quot; else echo &quot;$1不等于$2&quot; fi 等价于： [ $1 -eq $2 ] &amp;&amp; echo &quot;$1等于$2&quot; [ $1 -eq $2 ] &amp;&amp; echo &quot;$1不等于$2&quot; #注：多条件 if [ &quot;a&quot; = &quot;a&quot; ] &amp;&amp; [ &quot;b&quot; = &quot;b&quot; ]2.判断上一个指令是否执行成功 fName=22.txt touch $fName if [ $? -eq 0 ];then echo &quot;$fName创建成功&quot; fi3.判断输入的数字是否大于10 echo &apos;请输入一个数字：&apos; read number if [ $number -eq 10 ];then echo &apos;等于10&apos; elif [ $number -lt 10 ];then echo &apos;小于10&apos; else echo &apos;大于10&apos; fi 4.越过交互：给用户xuan设置密码。需要交互两次 方式1： passwd xuan &lt; password.txt #注：password.txt文件中有两行相同的密码 方式2： passwd xuan &lt;&lt;EOF #注：&lt;&lt;前是指令。heredoc语法 123 123 EOF #注：末尾的EOF必须顶行写，不能有空格或tab sh a.sh &lt;&lt;EOF 土豆 root EOF 方式3： echo 123 | passwd --stdin xuan #注：passwd的--stdin参数ubuntu不支持，centos才可以 巡检内存1234567891011121314total=`free -m | sed -n &apos;2p&apos; | awk &apos;&#123;printf $2&#125;&apos;` #注：awk &apos;&#123;&#125;&apos;别漏了单引号；printf别漏了fused=`free -m | grep -i mem | awk &apos;&#123;printf $3&#125;&apos;`percent_used=`echo &quot;scale=2;$used/$total&quot; | bc | awk &apos;&#123;printf &quot;%.2f&quot;,$1&#125;&apos;`echo -e &quot;总共：$&#123;total&#125;m&quot;echo &quot;已用：$&#123;used&#125;m&quot;echo &quot;内存使用率：$percent_used&quot;threshold=0.04flag=`echo &quot;$percent_used &gt; $threshold&quot; | bc`if [ $flag -eq 1 ];then echo -e &quot;\033[31m内存使用率超过$threshold \033[0m&quot; #注：31m后边如果有空格，则打印出来也有fi 批量创建用户12345678910111213141516read -p &apos;请输入用户名前缀：&apos; userread -p &apos;请输入创建用户个数：&apos; numecho &quot;$user $num&quot;for i in `seq 1 $num` #注：seq默认从1开始，所以1可以省略do #创建用户:先判断用户是否存在 newUser=$user$i cat /etc/passwd | awk -F&quot;:&quot; &apos;&#123;printf $1&quot;\n&quot;&#125;&apos; | grep $newUser if [ $? -eq 1 ];then #注：上边搜不到则$?=1 useradd $newUser echo &quot;创建用户$user$i&quot; #设置8位随机密码. /dev/urandom文件中会随机产生一些乱码数据，使用md5sum可以得到随机字符串 password=`head -1 /dev/urandom | md5sum | cut -c 1-8` echo &quot;用户名：$newUser\t密码：$password&quot; &gt;&gt; user_passwd.txt fidone 从mysql查询数据12345read -p &apos;请输入你要查询的商品名称：&apos; nameread -p &apos;请输入数据库用户名：&apos; username/usr/bin/mysql -u$&#123;username&#125; -p -e &quot;use test;select * from item where name = &apos;$name&apos;;&quot;#注：-e表示excute，不进入mysql命令窗口，直接执行语句 高效登录远程服务器12345ip=`cat ip.txt | grep $1 | awk &apos;&#123;printf $2&#125;&apos;`ssh $ip执行：sh aaa.sh beijing #注：beijing赋值给$1，ip.txt文件中地址和ip使用空格隔开 最后推荐大家一个常用搜索Linux命名的网站：https://man.linuxde.net/，可以查看Linux命令的选项参数]]></content>
      <categories>
        <category>Linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spring retry 重试机制]]></title>
    <url>%2F2019%2F07%2F10%2Fspring-retry-%E9%87%8D%E8%AF%95%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[当我们调用一个接口可能由于网络等原因造成第一次失败，再去尝试就成功了，这就是重试机制，spring支持重试机制，并且在Spring Cloud中可以与Hystaix结合使用，可以避免访问到已经不正常的实例。 但是切记非幂等情况下慎用重试 加入依赖 org.springframework.retry spring-retry #### 在主类上加入 @EnableRetry 注解 123456789@EnableRetry //开启重试机制@EnableAutoConfiguration //开启自动配置@SpringBootApplicationpublic class SpringBootApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootApplication.class, args); &#125;&#125; 测试用例12345678910111213141516171819202122232425262728293031323334353637import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.retry.annotation.Backoff;import org.springframework.retry.annotation.Recover;import org.springframework.retry.annotation.Retryable;import org.springframework.stereotype.Service;import java.time.LocalTime;@Servicepublic class RetryService &#123; private final static Logger logger = LoggerFactory.getLogger(RetryService.class); private final int totalNum = 100000; @Retryable(value = Exception.class, maxAttempts = 3, backoff = @Backoff(delay = 2000L, multiplier = 1.5)) public void retry(int num) &#123; logger.info(&quot;减库存开始&quot; + LocalTime.now()); try &#123; int i = 1 / 0; &#125; catch (Exception e) &#123; logger.error(&quot;illegal&quot;); &#125; if (num &lt;= 0) &#123; throw new IllegalArgumentException(&quot;数量不对&quot;); &#125; logger.info(&quot;减库存执行结束&quot; + LocalTime.now()); &#125; @Recover public void recover(Exception e) &#123; logger.warn(&quot;减库存失败！！！&quot; + LocalTime.now()); &#125;&#125; 1234567891011public class RetryServiceTest extends BaseTest &#123; @Autowired private RetryService retryService; @Test public void retry() &#123; int count = retryService.retry(-1); System.out.println(&quot;库存为 ：&quot; + count); &#125;&#125; 结果： 1234567892019-07-10 09:52:08.691 INFO 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : 减库存开始09:52:08.6912019-07-10 09:52:08.691 ERROR 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : illegal2019-07-10 09:52:10.695 INFO 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : 减库存开始09:52:10.6952019-07-10 09:52:10.696 ERROR 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : illegal2019-07-10 09:52:13.701 INFO 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : 减库存开始09:52:13.7012019-07-10 09:52:13.709 ERROR 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : illegal2019-07-10 09:52:18.212 INFO 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : 减库存开始09:52:18.2122019-07-10 09:52:18.214 ERROR 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : illegal2019-07-10 09:52:18.214 WARN 21433 --- [ XNIO-2 task-1] c.c.c.serviceImpl.TestServiceImpl : 减库存失败！！！09:52:18.214 注解说明@Retryable的参数说明： value：抛出指定异常才会重试 include：和value一样，默认空，当exclude也为空时，所有异常都重试 exclude：指定不处理的异常 maxAttempts：最大重试次数，默认3次 backoff：重试等待策略，默认使用@Backoff @Backoff注解 delay:指定延迟后重试 multiplier（指定延迟倍数）默认为0，表示固定暂停1秒后进行重试，如果把multiplier设置为1.5，则第一次重试为2秒，第二次为3秒，第三次为4.5秒。 @Recover注解 当重试到达指定次数时，被注解的方法将被回调，可以在该方法中进行日志处理。需要注意的是发生的异常和入参类型一致时才会回调。 注意 使用了@Retryable的方法不能在本类被调用，不然重试机制不会生效。也就是要标记为@Service，然后在其它类使用@Autowired注入或者@Bean去实例才能生效。 要触发@Recover方法，那么在@Retryable方法上不能有返回值，只能是void才能生效。 非幂等情况下慎用 使用了@Retryable的方法里面不能使用try…catch包裹，要在方法上抛出异常，不然不会触发。 参考：https://my.oschina.net/wangjunBlog/blog/1889015]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HTTPS加密过程]]></title>
    <url>%2F2019%2F05%2F29%2FHTTPS%E5%8A%A0%E5%AF%86%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[认证服务器浏览器内置一个受信任的CA机构列表，并保存了这些CA机构的证书。第一阶段服务器会提供经CA机构认证颁发的服务器证书，如果认证该服务器证书的CA机构，存在于浏览器的受信任CA机构列表中，并且服务器证书中的信息与当前正在访问的网站（域名等）一致，那么浏览器就认为服务端是可信的，并从服务器证书中取得服务器公钥，用于后续流程。否则，浏览器将提示用户，根据用户的选择，决定是否继续。当然，我们可以管理这个受信任CA机构列表，添加我们想要信任的CA机构，或者移除我们不信任的CA机构。 协商会话密钥客户端在认证完服务器，获得服务器的公钥之后，利用该公钥与服务器进行加密通信，协商出两个会话密钥，分别是用于加密客户端往服务端发送数据的客户端会话密钥，用于加密服务端往客户端发送数据的服务端会话密钥。在已有服务器公钥，可以加密通讯的前提下，还要协商两个对称密钥的原因，是因为非对称加密相对复杂度更高，在数据传输过程中，使用对称加密，可以节省计算资源。另外，会话密钥是随机生成，每次协商都会有不一样的结果，所以安全性也比较高。 加密通讯此时客户端服务器双方都有了本次通讯的会话密钥，之后传输的所有的Http数据，都通过会话密钥加密。这样网路上的其它用户，将很难窃取和篡改客户端和服务端之间传输的数据，从而保证了数据的私密性和完整性。]]></content>
      <categories>
        <category>Https</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[快速了解23种设计模式]]></title>
    <url>%2F2019%2F05%2F15%2F%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A323%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[设计模式的分类设计模式有两种分类方法，即根据模式的目的来分和根据模式的作用的范围来分。 根据目的来分根据模式是用来完成什么工作来划分，这种方式可分为创建型模式、结构型模式和行为型模式3种。 创建型模式：用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”。GoF 中提供了单例、原型、工厂方法、抽象工厂、建造者等 5 种创建型模式。 结构型模式：用于描述如何将类或对象按某种布局组成更大的结构，GoF 中提供了代理、适配器、桥接、装饰、外观、享元、组合等 7 种结构型模式。 行为型模式：用于描述类或对象之间怎样相互协作共同完成单个对象都无法单独完成的任务，以及怎样分配职责。GoF 中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等 11 种行为型模式。 根据作用范围来分根据模式是主要用于类上还是主要用于对象上来分，这种方式可分为类模式和对象模式两种。 类模式：用于处理类与子类之间的关系，这些关系通过继承来建立，是静态的，在编译时刻便确定下来了。GoF中的工厂方法、（类）适配器、模板方法、解释器属于该模式。 对象模式：用于处理对象之间的关系，这些关系可以通过组合或聚合来实现，在运行时刻是可以变化的，更具动态性。GoF 中除了以上 4 种，其他的都是对象模式。 设计模式的六大原则总原则：开闭原则（Open Close Principle）开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等。 单一职责原则不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。 里氏替换原则（Liskov Substitution Principle）里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。—— From Baidu 百科 历史替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。 依赖倒转原则（Dependence Inversion Principle）这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。 接口隔离原则（Interface Segregation Principle）这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。 迪米特法则（最少知道原则）（Demeter Principle）就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。 最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。 合成复用原则（Composite Reuse Principle）原则是尽量首先使用合成/聚合的方式，而不是使用继承。 Java的23中设计模式 单例（Singleton）模式：某个类只能生成一个实例，该类提供了一个全局访问点供外部获取该实例，其拓展是有限多例模式。 原型（Prototype）模式：将一个对象作为原型，通过对其进行复制而克隆出多个和原型类似的新实例。 工厂方法（Factory Method）模式：定义一个用于创建产品的接口，由子类决定生产什么产品。 抽象工厂（AbstractFactory）模式：提供一个创建产品族的接口，其每个子类可以生产一系列相关的产品。 建造者（Builder）模式：将一个复杂对象分解成多个相对简单的部分，然后根据不同需要分别创建它们，最后构建成该复杂对象。 代理（Proxy）模式：为某对象提供一种代理以控制对该对象的访问。即客户端通过代理间接地访问该对象，从而限制、增强或修改该对象的一些特性。 适配器（Adapter）模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。 桥接（Bridge）模式：将抽象与实现分离，使它们可以独立变化。它是用组合关系代替继承关系来实现，从而降低了抽象和实现这两个可变维度的耦合度。 装饰（Decorator）模式：动态的给对象增加一些职责，即增加其额外的功能。 外观（Facade）模式：为多个复杂的子系统提供一个一致的接口，使这些子系统更加容易被访问。 享元（Flyweight）模式：运用共享技术来有效地支持大量细粒度对象的复用。 组合（Composite）模式：将对象组合成树状层次结构，使用户对单个对象和组合对象具有一致的访问性。 模板方法（TemplateMethod）模式：定义一个操作中的算法骨架，而将算法的一些步骤延迟到子类中，使得子类可以不改变该算法结构的情况下重定义该算法的某些特定步骤。 策略（Strategy）模式：定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的改变不会影响使用算法的客户。 命令（Command）模式：将一个请求封装为一个对象，使发出请求的责任和执行请求的责任分割开。 职责链（Chain of Responsibility）模式：把请求从链中的一个对象传到下一个对象，直到请求被响应为止。通过这种方式去除对象之间的耦合。 状态（State）模式：允许一个对象在其内部状态发生改变时改变其行为能力。 观察者（Observer）模式：多个对象间存在一对多关系，当一个对象发生改变时，把这种改变通知给其他多个对象，从而影响其他对象的行为。 中介者（Mediator）模式：定义一个中介对象来简化原有对象之间的交互关系，降低系统中对象间的耦合度，使原有对象之间不必相互了解。 迭代器（Iterator）模式：提供一种方法来顺序访问聚合对象中的一系列数据，而不暴露聚合对象的内部表示。 访问者（Visitor）模式：在不改变集合元素的前提下，为一个集合中的每个元素提供多种访问方式，即每个元素有多个访问者对象访问。 备忘录（Memento）模式：在不破坏封装性的前提下，获取并保存一个对象的内部状态，以便以后恢复它。 解释器（Interpreter）模式：提供如何定义语言的文法，以及对语言句子的解释方法，即解释器。 总结设计模式的本质是面向对象设计原则的实际运用，是对类的封装性、继承性和多态性以及类的关联关系和组合关系的充分理解。正确使用设计模式具有以下优点。 可以提高程序员的思维能力、编程能力和设计能力。 使程序设计更加标准化、代码编制更加工程化，使软件开发效率大大提高，从而缩短软件的开发周期。 使设计的代码可重用性高、可读性强、可靠性高、灵活性好、可维护性强。 当然，软件设计模式只是一个引导。在具体的软件幵发中，必须根据设计的应用系统的特点和要求来恰当选择。对于简单的程序开发，苛能写一个简单的算法要比引入某种设计模式更加容易。但对大项目的开发或者框架设计，用设计模式来组织代码显然更好。 参考博客：https://blog.csdn.net/tsite/article/details/62420091https://www.cnblogs.com/geek6/p/3951677.htmlhttp://c.biancheng.net/view/1320.html]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基准测试神器-JMH]]></title>
    <url>%2F2019%2F05%2F14%2F%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E7%A5%9E%E5%99%A8-JMH%2F</url>
    <content type="text"><![CDATA[原文地址：https://sq.163yun.com/blog/article/179671960481783808 概述性能测试这个话题非常庞大，我们可以从网络聊到操作系统，再从操作系统聊到内核，再从内核聊到你怀疑人生有木有。 先拍几个砖出来吧，我在写代码的时候经常有这种怀疑：写法A快还是写法B快，某个位置是用ArrayList还是LinkedList，HashMap还是TreeMap，HashMap的初始化size要不要指定，指定之后究竟比默认的DEFAULT_SIZE性能好多少。。。 如果你还是通过for循环或者手撸method来测试你的内容的话，那么JMH就是你必须要明白的内容了，因为已经有人把基准测试的轮子造好了，接下来我们就一起看看这个轮子怎么用： JMH只适合细粒度的方法测试，并不适用于系统之间的链路测试！JMH只适合细粒度的方法测试，并不适用于系统之间的链路测试！JMH只适合细粒度的方法测试，并不适用于系统之间的链路测试！ JMH入门JMH是一个工具包，如果我们要通过JMH进行基准测试的话，直接在我们的pom文件中引入JMH的依赖即可： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-core&lt;/artifactId&gt; &lt;version&gt;1.19&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.openjdk.jmh&lt;/groupId&gt; &lt;artifactId&gt;jmh-generator-annprocess&lt;/artifactId&gt; &lt;version&gt;1.19&lt;/version&gt;&lt;/dependency&gt; 通过一个HelloWorld程序来看一下JMH如果工作： @Warmup(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)@Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS)public class JMHSample_01_HelloWorld { static class Demo { int id; String name; public Demo(int id, String name) { this.id = id; this.name = name; } } static List&lt;Demo&gt; demoList; static { demoList = new ArrayList(); for (int i = 0; i &lt; 10000; i ++) { demoList.add(new Demo(i, &quot;test&quot;)); } } @Benchmark @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.MICROSECONDS) public void testHashMapWithoutSize() { Map map = new HashMap(); for (Demo demo : demoList) { map.put(demo.id, demo.name); } } @Benchmark @BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.MICROSECONDS) public void testHashMap() { Map map = new HashMap((int)(demoList.size() / 0.75f) + 1); for (Demo demo : demoList) { map.put(demo.id, demo.name); } } public static void main(String[] args) throws RunnerException { Options opt = new OptionsBuilder() .include(JMHSample_01_HelloWorld.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); }} ======================================执行结果======================================Benchmark Mode Cnt Score Error UnitschargeProject.List.JMHSample_01_HelloWorld.testHashMap avgt 5 130.865 ± 5.851 us/opchargeProject.List.JMHSample_01_HelloWorld.testHashMapWithoutSize avgt 5 172.087 ± 66.252 us/op======================================执行结果====================================== 上面的代码用中文翻译一下：分别定义两个基准测试的方法testHashMapWithoutSize和 testHashMap，这两个基准测试方法执行流程是：每个方法执行前都进行5次预热执行，每隔1秒进行一次预热操作，预热执行结束之后进行5次实际测量执行，每隔1秒进行一次实际执行，我们此次基准测试测量的是平均响应时长，单位是us。 预热？为什么要预热？因为 JVM 的 JIT 机制的存在，如果某个函数被调用多次之后，JVM 会尝试将其编译成为机器码从而提高执行速度。为了让 benchmark 的结果更加接近真实情况就需要进行预热。 从上面的执行结果我们看出，针对一个Map的初始化参数的给定其实有很大影响，当我们给定了初始化参数执行执行的速度是没给定参数的2/3，这个优化速度还是比较明显的，所以以后大家在初始化Map的时候能给定参数最好都给定了，代码是处处优化的，积少成多。 通过上面的内容我们已经基本可以看出来JMH的写法雏形了，后面的介绍主要是一些注解的使用： @Benchmark@Benchmark标签是用来标记测试方法的，只有被这个注解标记的话，该方法才会参与基准测试，但是有一个基本的原则就是被@Benchmark标记的方法必须是public的。 @Warmup@Warmup用来配置预热的内容，可用于类或者方法上，越靠近执行方法的地方越准确。一般配置warmup的参数有这些： iterations：预热的次数。 time：每次预热的时间。 timeUnit：时间单位，默认是s。 batchSize：批处理大小，每次操作调用几次方法。（后面用到） @Measurement用来控制实际执行的内容，配置的选项本warmup一样。 @BenchmarkMode@BenchmarkMode主要是表示测量的纬度，有以下这些纬度可供选择： Mode.Throughput 吞吐量纬度 Mode.AverageTime 平均时间 Mode.SampleTime 抽样检测 Mode.SingleShotTime 检测一次调用 Mode.All 运用所有的检测模式 在方法级别指定@BenchmarkMode的时候可以一定指定多个纬度，例如： @BenchmarkMode({Mode.Throughput, Mode.AverageTime, Mode.SampleTime, Mode.SingleShotTime})，代表同时在多个纬度对目标方法进行测量。 @OutputTimeUnit@OutputTimeUnit代表测量的单位，比如秒级别，毫秒级别，微妙级别等等。一般都使用微妙和毫秒级别的稍微多一点。该注解可以用在方法级别和类级别，当用在类级别的时候会被更加精确的方法级别的注解覆盖，原则就是离目标更近的注解更容易生效。 @State在很多时候我们需要维护一些状态内容，比如在多线程的时候我们会维护一个共享的状态，这个状态值可能会在每隔线程中都一样，也有可能是每个线程都有自己的状态，JMH为我们提供了状态的支持。该注解只能用来标注在类上，因为类作为一个属性的载体。 @State的状态值主要有以下几种： Scope.Benchmark 该状态的意思是会在所有的Benchmark的工作线程中共享变量内容。 Scope.Group 同一个Group的线程可以享有同样的变量 Scope.Thread 每隔线程都享有一份变量的副本，线程之间对于变量的修改不会相互影响。 下面看两个常见的@State的写法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541.直接在内部类中使用@State作为“PropertyHolder”public class JMHSample_03_States &#123; @State(Scope.Benchmark) public static class BenchmarkState &#123; volatile double x = Math.PI; &#125; @State(Scope.Thread) public static class ThreadState &#123; volatile double x = Math.PI; &#125; @Benchmark public void measureUnshared(ThreadState state) &#123; state.x++; &#125; @Benchmark public void measureShared(BenchmarkState state) &#123; state.x++; &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_03_States.class.getSimpleName()) .threads(4) .forks(1) .build(); new Runner(opt).run(); &#125;&#125;2.在Main类中直接使用@State作为注解，是Main类直接成为“PropertyHolder”@State(Scope.Thread)public class JMHSample_04_DefaultState &#123; double x = Math.PI; @Benchmark public void measure() &#123; x++; &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_04_DefaultState.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); &#125;&#125; 我们试想以下@State的含义，它主要是方便框架来控制变量的过程逻辑，通过@State标示的类都被用作属性的容器，然后框架可以通过自己的控制来配置不同级别的隔离情况。被@Benchmark标注的方法可以有参数，但是参数必须是被@State注解的，就是为了要控制参数的隔离。 但是有些情况下我们需要对参数进行一些初始化或者释放的操作，就像Spring提供的一些init和destory方法一样，JHM也提供有这样的钩子： @Setup 必须标示在@State注解的类内部，表示初始化操作 @TearDown 必须表示在@State注解的类内部，表示销毁操作 初始化和销毁的动作都只会执行一次。 1234567891011121314151617181920212223242526272829@State(Scope.Thread)public class JMHSample_05_StateFixtures &#123; double x; @Setup public void prepare() &#123; x = Math.PI; &#125; @TearDown public void check() &#123; assert x &gt; Math.PI : &quot;Nothing changed?&quot;; &#125; @Benchmark public void measureRight() &#123; x++; &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_05_StateFixtures.class.getSimpleName()) .forks(1) .jvmArgs(&quot;-ea&quot;) .build(); new Runner(opt).run(); &#125;&#125; 虽然我们可以执行初始化和销毁的动作，但是总是感觉还缺点啥？对，就是初始化的粒度。因为基准测试往往会执行多次，那么能不能保证每次执行方法的时候都初始化一次变量呢？ @Setup和@TearDown提供了以下三种纬度的控制： Level.Trial 只会在个基础测试的前后执行。包括Warmup和Measurement阶段，一共只会执行一次。 Level.Iteration 每次执行记住测试方法的时候都会执行，如果Warmup和Measurement都配置了2次执行的话，那么@Setup和@TearDown配置的方法的执行次数就4次。 Level.Invocation 每个方法执行的前后执行（一般不推荐这么用） @Param在很多情况下，我们需要测试不同的参数的不同结果，但是测试的了逻辑又都是一样的，因此如果我们编写镀铬benchmark的话会造成逻辑的冗余，幸好JMH提供了@Param参数来帮助我们处理这个事情，被@Param注解标示的参数组会一次被benchmark消费到。 1234567891011121314151617181920@State(Scope.Benchmark)public class ParamTest &#123; @Param(&#123;&quot;1&quot;, &quot;2&quot;, &quot;3&quot;&#125;) int testNum; @Benchmark public String test() &#123; return String.valueOf(testNum); &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(ParamTest.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); &#125;&#125; @Threads测试线程的数量，可以配置在方法或者类上，代表执行测试的线程数量。 通常看到这里我们会比较迷惑Iteration和Invocation区别，我们在配置Warmup的时候默认的时间是的1s，即1s的执行作为一个Iteration，假设每次方法的执行是100ms的话，那么1个Iteration就代表10个Invocation。 JMH进阶通过以上的内容我们已经基本可以掌握JMH的使用了，下面就主要介绍一下JMH提供的一些高级特性了。 不要编写无用代码 因为现代的编译器非常聪明，如果我们在代码使用了没有用处的变量的话，就容易被编译器优化掉，这就会导致实际的测量结果可能不准确，因为我们要在测量的方法中避免使用void方法，然后记得在测量的结束位置返回结果。这么做的目的很明确，就是为了与编译器斗智斗勇，让编译器不要改变这段代码执行的初衷。 Blackhole介绍Blackhole会消费传进来的值，不提供任何信息来确定这些值是否在之后被实际使用。 Blackhole处理的事情主要有以下几种： 死代码消除：入参应该在每次都被用到，因此编译器就不会把这些参数优化为常量或者在计算的过程中对他们进行其他优化。 处理内存壁：我们需要尽可能减少写的量，因为它会干扰缓存，污染写缓冲区等。 这很可能导致过早地撞到内存壁 我们在上面说到需要消除无用代码，那么其中一种方式就是通过Blackhole，我们可以用Blackhole来消费这些返回的结果。 1234567891011121:返回测试结果，防止编译器优化@Benchmarkpublic double measureRight_1() &#123; return Math.log(x1) + Math.log(x2);&#125;2.通过Blackhole消费中间结果，防止编译器优化@Benchmarkpublic void measureRight_2(Blackhole bh) &#123; bh.consume(Math.log(x1)); bh.consume(Math.log(x2));&#125; 循环处理我们虽然可以在Benchmark中定义循环逻辑，但是这么做其实是不合适的，因为编译器可能会将我们的循环进行展开或者做一些其他方面的循环优化，所以JHM建议我们不要在Beanchmark中使用循环，如果我们需要处理循环逻辑了，可以结合@BenchmarkMode(Mode.SingleShotTime)和@Measurement(batchSize = N)来达到同样的效果. 123456789101112131415161718192021222324252627282930@State(Scope.Thread)public class JMHSample_26_BatchSize &#123; List&lt;String&gt; list = new LinkedList&lt;&gt;(); // 每个iteration中做5000次Invocation @Benchmark @Warmup(iterations = 5, batchSize = 5000) @Measurement(iterations = 5, batchSize = 5000) @BenchmarkMode(Mode.SingleShotTime) public List&lt;String&gt; measureRight() &#123; list.add(list.size() / 2, &quot;something&quot;); return list; &#125; @Setup(Level.Iteration) public void setup()&#123; list.clear(); &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_26_BatchSize.class.getSimpleName()) .forks(1) .build(); new Runner(opt).run(); &#125;&#125; 方法内联方法内联：如果JVM监测到一些小方法被频繁的执行，它会把方法的调用替换成方法体本身。比如说下面这个： 1234567private int add4(int x1, int x2, int x3, int x4) &#123; return add2(x1, x2) + add2(x3, x4); &#125; private int add2(int x1, int x2) &#123; return x1 + x2; &#125; 运行一段时间后JVM会把add2方法去掉，并把你的代码翻译成： 123private int add4(int x1, int x2, int x3, int x4) &#123; return x1 + x2 + x3 + x4; &#125; JMH提供了CompilerControl注解来控制方法内联，但是实际上我感觉比较有用的就是两个了： CompilerControl.Mode.DONT_INLINE：强制限制不能使用内联 CompilerControl.Mode.INLINE：强制使用内联 看一下官方提供的例子把： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@State(Scope.Thread)@BenchmarkMode(Mode.AverageTime)@OutputTimeUnit(TimeUnit.NANOSECONDS)public class JMHSample_16_CompilerControl &#123; public void target_blank() &#123; &#125; @CompilerControl(CompilerControl.Mode.DONT_INLINE) public void target_dontInline() &#123; &#125; @CompilerControl(CompilerControl.Mode.INLINE) public void target_inline() &#123; &#125; @Benchmark public void baseline() &#123; &#125; @Benchmark public void dontinline() &#123; target_dontInline(); &#125; @Benchmark public void inline() &#123; target_inline(); &#125; public static void main(String[] args) throws RunnerException &#123; Options opt = new OptionsBuilder() .include(JMHSample_16_CompilerControl.class.getSimpleName()) .warmupIterations(0) .measurementIterations(3) .forks(1) .build(); new Runner(opt).run(); &#125;&#125;======================================执行结果==============================Benchmark Mode Cnt Score Error UnitsJMHSample_16_CompilerControl.baseline avgt 3 0.896 ± 3.426 ns/opJMHSample_16_CompilerControl.dontinline avgt 3 0.344 ± 0.126 ns/opJMHSample_16_CompilerControl.inline avgt 3 0.391 ± 2.622 ns/op======================================执行结果==============================]]></content>
      <categories>
        <category>JMH</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[八大排序算法总结基于java实现]]></title>
    <url>%2F2019%2F05%2F13%2F%E5%85%AB%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%E5%9F%BA%E4%BA%8Ejava%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[原文链接：https://github.com/iTimeTraveler/SortAlgorithms 概述对常用的八大排序算法进行总结。 直接插入排序 希尔排序 简单选择排序 堆排序 冒泡排序 快速排序 归并排序 基数排序 它们都属于内部排序，也就是只考虑数据量较小仅需要使用内存的排序算法，他们之间关系如下： 直接插入排序（Insertion Sort）插入排序的设计初衷是往有序的数组中快速插入一个新的元素。它的算法思想是：把要排序的数组分为了两个部分, 一部分是数组的全部元素(除去待插入的元素), 另一部分是待插入的元素; 先将第一部分排序完成, 然后再插入这个元素.。其中第一部分的排序也是通过再次拆分为两部分来进行的。 插入排序由于操作不尽相同, 可分为 直接插入排序 , 折半插入排序(又称二分插入排序), 链表插入排序 , 希尔排序 。我们先来看下直接插入排序。 基本思想直接插入排序的基本思想是：将数组中的所有元素依次跟前面已经排好的元素相比较，如果选择的元素比已排序的元素小，则交换，直到全部元素都比较过为止。 算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： ①. 从第一个元素开始，该元素可以认为已经被排序②. 取出下一个元素，在已经排序的元素序列中从后向前扫描③. 如果该元素（已排序）大于新元素，将该元素移到下一位置④. 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置⑤. 将新元素插入到该位置后⑥. 重复步骤②~⑤ 算法实现中比较有意思的一点是，在每次比较操作发现取出来的新元素小于等于已排序的元素时，可以将已排序的元素移到下一位置，然后将取出来的新元素插入该位置（即相邻位置对调），接着再与前面的已排序的元素进行比较，如上图所示，这样做缺点是交换操作代价比较大。另一种做法是：将新元素取出（挖坑），从左到右依次与已排序的元素比较，如果已排序的元素大于取出的新元素，那么将该元素移动到下一个位置（填坑），接着再与前面的已排序的元素比较，直到找到已排序的元素小于等于新元素的位置，这时再将新元素插入进去。就像基本思想中的动图演示的那样。 如果比较操作的代价比交换操作大的话，可以采用二分查找法来减少比较操作的数目。可以认为是插入排序的一个变种，称为二分查找插入排序。 代码实现1234567891011121314151617181920212223242526272829303132333435/** * 插入排序 * * 1. 从第一个元素开始，该元素可以认为已经被排序 * 2. 取出下一个元素，在已经排序的元素序列中从后向前扫描 * 3. 如果该元素（已排序）大于新元素，将该元素移到下一位置 * 4. 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 * 5. 将新元素插入到该位置后 * 6. 重复步骤2~5 * @param arr 待排序数组 */public static void insertionSort(int[] arr)&#123; for( int i = 1; i &lt; arr.length; i++ ) &#123; int temp = arr[i]; // 取出下一个元素，在已经排序的元素序列中从后向前扫描 for( int j = i; j &gt;= 0; j-- ) &#123; if( j &gt; 0 &amp;&amp; arr[j-1] &gt; temp ) &#123; arr[j] = arr[j-1]; // 如果该元素（已排序）大于取出的元素temp，将该元素移到下一位置 System.out.println(&quot;Temping: &quot; + Arrays.toString(arr)); &#125; else &#123; // 将新元素插入到该位置后 arr[j] = temp; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); break; &#125; &#125; &#125;&#125;// 交换次数较多的实现public static void insertionSort(int[] arr)&#123; for( int i=0; i&lt;arr.length-1; i++ ) &#123; for( int j=i+1; j&gt;0; j-- ) &#123; if( arr[j-1] &lt;= arr[j] ) break; int temp = 复杂度直接插入排序复杂度如下： 最好情况下，排序前对象已经按照要求的有序。比较次数(KCN)：[Math Processing Error]；移动次数(RMN)为[Math Processing Error]。则对应的时间复杂度为[Math Processing Error]。 最坏情况下，排序前对象为要求的顺序的反序。第i趟时第i个对象必须与前面i个对象都做排序码比较，并且每做1次比较就要做1次数据移动（从上面给出的代码中看出）。比较次数(KCN)：[Math Processing Error] ; 移动次数(RMN)为：[Math Processing Error]。则对应的时间复杂度为[Math Processing Error]。 如果排序记录是随机的，那么根据概率相同的原则，在平均情况下的排序码比较次数和对象移动次数约为[Math Processing Error]，因此，直接插入排序的平均时间复杂度为[Math Processing Error]。 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n) O(n²) O(1) Tips: 由于直接插入排序每次只移动一个元素的位， 并不会改变值相同的元素之间的排序， 因此它是一种稳定排序。 希尔排序（Shell Sort）希尔排序，也称递减增量排序算法，1959年Shell发明。是插入排序的一种高速而稳定的改进版本。 希尔排序是先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。 第一个突破O(n^2)的排序算法；是简单插入排序的改进版；它与插入排序的不同之处在于，它会优先比较距离较远的元素。基本思想将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；每次再将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。 可以看到步长的选择是希尔排序的重要部分。只要最终步长为1任何步长序列都可以工作。一般来说最简单的步长取值是初次取数组长度的一半为增量，之后每次再减半，直到增量为1。更好的步长序列取值可以参考维基百科。 算法描述①. 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；（一般初次取数组半长，之后每次再减半，直到增量为1）②. 按增量序列个数k，对序列进行k 趟排序；③. 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 代码实现以下是我自己的实现，可以看到实现很幼稚，但是好处是理解起来很简单。因为没有经过任何的优化，所以不建议大家直接使用。建议对比下方的维基百科官方实现代码，特别是步长取值策略部分。 123456789101112131415161718192021222324/** * 希尔排序 * * 1. 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；（一般初次取数组半长，之后每次再减半，直到增量为1） * 2. 按增量序列个数k，对序列进行k 趟排序； * 3. 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。 * 仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 * @param arr 待排序数组 */public static void shellSort(int[] arr)&#123; int gap = arr.length / 2; for (; gap &gt; 0; gap /= 2) &#123; //不断缩小gap，直到1为止 for (int j = 0; (j+gap) &lt; arr.length; j++)&#123; //使用当前gap进行组内插入排序 for(int k = 0; (k+gap)&lt; arr.length; k += gap)&#123; if(arr[k] &gt; arr[k+gap]) &#123; int temp = arr[k+gap]; //交换操作 arr[k+gap] = arr[k]; arr[k] = temp; System.out.println(&quot; Sorting: &quot; + Arrays.toString(arr)); &#125; &#125; &#125; &#125;&#125; 注意： ①. 第一层for循环表示一共有多少个增量。增量的序列的个数，就是希尔排序的趟数。上面的增量序列为： arr.length/2, arr.length/2/2, arr.length/2/2/2, …. 2, 1②. 里层的两个for循环，实际上就是以一个gap拆分为一组的组内插入排序。 下面是维基百科官方实现，大家注意gap步长取值部分： 1234567891011121314151617181920212223/** * 希尔排序（Wiki官方版） * * 1. 选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；（注意此算法的gap取值） * 2. 按增量序列个数k，对序列进行k 趟排序； * 3. 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。 * 仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 * @param arr 待排序数组 */public static void shell_sort(int[] arr) &#123; int gap = 1, i, j, len = arr.length; int temp; while (gap &lt; len / 3) gap = gap * 3 + 1; // &lt;O(n^(3/2)) by Knuth,1973&gt;: 1, 4, 13, 40, 121, ... for (; gap &gt; 0; gap /= 3) &#123; for (i = gap; i &lt; len; i++) &#123; temp = arr[i]; for (j = i - gap; j &gt;= 0 &amp;&amp; arr[j] &gt; temp; j -= gap) arr[j + gap] = arr[j]; arr[j + gap] = temp; &#125; &#125;&#125; 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog2 n) O(nlog2 n) O(nlog2 n) O(1) 选择排序（Selection Sort）从算法逻辑上看，选择排序是一种简单直观的排序算法，在简单选择排序过程中，所需移动记录的次数比较少。 基本思想选择排序的基本思想：比较 + 交换。 在未排序序列中找到最小（大）元素，存放到未排序序列的起始位置。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。 算法描述①. 从待排序序列中，找到关键字最小的元素；②. 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换；③. 从余下的 N - 1 个元素中，找出关键字最小的元素，重复①、②步，直到排序结束。 代码实现选择排序比较简单，以下是我自己的实现，跟官方版差不多，所以完全可以参考。 12345678910111213141516171819202122232425/** * 选择排序 * * 1. 从待排序序列中，找到关键字最小的元素； * 2. 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换； * 3. 从余下的 N - 1 个元素中，找出关键字最小的元素，重复①、②步，直到排序结束。 * 仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 * @param arr 待排序数组 */public static void selectionSort(int[] arr)&#123; for(int i = 0; i &lt; arr.length-1; i++)&#123; int min = i; for(int j = i+1; j &lt; arr.length; j++)&#123; //选出之后待排序中值最小的位置 if(arr[j] &lt; arr[min])&#123; min = j; &#125; &#125; if(min != i)&#123; int temp = arr[min]; //交换操作 arr[min] = arr[i]; arr[i] = temp; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); &#125; &#125;&#125; 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n²) O(n²) O(1) 选择排序的简单和直观名副其实，这也造就了它”出了名的慢性子”，无论是哪种情况，哪怕原数组已排序完成，它也将花费将近n²/2次遍历来确认一遍。即便是这样，它的排序结果也还是不稳定的。 唯一值得高兴的是，它并不耗费额外的内存空间。 堆排序（Heap Sort）1991年的计算机先驱奖获得者、斯坦福大学计算机科学系教授罗伯特·弗洛伊德(Robert W．Floyd) 和威廉姆斯(J．Williams) 在1964年共同发明了著名的堆排序算法(Heap Sort). 堆的定义如下：n个元素的序列{k1,k2,···,kn}，当且仅当满足下关系时，称之为堆。 ki &lt;= k(2i) 且 ki &lt;= k(2i+1) 或： ki &gt;= k(2i) 且 ki &gt;= k(2i+1) 把此序列对应的二维数组看成一个完全二叉树。那么堆的含义就是完全二叉树中任何一个非叶子节点的值均不大于（或不小于）其左，右孩子节点的值。由上述性质可知大顶堆的堆顶的关键字肯定是所有关键字中最大的，小顶堆的堆顶的关键字是所有关键字中最小的。因此我们可使用大顶堆进行升序排序, 使用小顶堆进行降序排序。 基本思想此处以大顶堆为例，堆排序的过程就是将待排序的序列构造成一个堆，选出堆中最大的移走，再把剩余的元素调整成堆，找出最大的再移走，重复直至有序。 算法描述①. 先将初始序列K[1..n]建成一个大顶堆, 那么此时第一个元素K1最大, 此堆为初始的无序区.②. 再将关键字最大的记录K1 (即堆顶, 第一个元素)和无序区的最后一个记录 Kn 交换, 由此得到新的无序区K[1..n-1]和有序区K[n], 且满足K[1..n-1].keys &lt;= K[n].key③. 交换K1 和 Kn 后, 堆顶可能违反堆性质, 因此需将K[1..n-1]调整为堆. 然后重复步骤②, 直到无序区只有一个元素时停止. 动图效果如下所示： 代码实现从算法描述来看，堆排序需要两个过程，一是建立堆，二是堆顶与堆的最后一个元素交换位置。所以堆排序有两个函数组成。一是建堆函数，二是反复调用建堆函数以选择出剩余未排元素中最大的数来实现排序的函数。 总结起来就是定义了以下几种操作： 最大堆调整（Max_Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点创建最大堆（Build_Max_Heap）：将堆所有数据重新排序堆排序（HeapSort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算对于堆节点的访问： 父节点i的左子节点在位置：(2*i+1); 父节点i的右子节点在位置：(2*i+2); 子节点i的父节点在位置：floor((i-1)/2);1234567891011121314151617181920212223242526272829303132333435363738/** * 堆排序 * * 1. 先将初始序列K[1..n]建成一个大顶堆, 那么此时第一个元素K1最大, 此堆为初始的无序区. * 2. 再将关键字最大的记录K1 (即堆顶, 第一个元素)和无序区的最后一个记录 Kn 交换, 由此得到新的无序区K[1..n−1]和有序区K[n], 且满足K[1..n−1].keys⩽K[n].key * 3. 交换K1 和 Kn 后, 堆顶可能违反堆性质, 因此需将K[1..n−1]调整为堆. 然后重复步骤②, 直到无序区只有一个元素时停止. * @param arr 待排序数组 */public static void heapSort(int[] arr)&#123; for(int i = arr.length; i &gt; 0; i--)&#123; max_heapify(arr, i); int temp = arr[0]; //堆顶元素(第一个元素)与Kn交换 arr[0] = arr[i-1]; arr[i-1] = temp; &#125;&#125;private static void max_heapify(int[] arr, int limit)&#123; if(arr.length &lt;= 0 || arr.length &lt; limit) return; int parentIdx = limit / 2; for(; parentIdx &gt;= 0; parentIdx--)&#123; if(parentIdx * 2 &gt;= limit)&#123; continue; &#125; int left = parentIdx * 2; //左子节点位置 int right = (left + 1) &gt;= limit ? left : (left + 1); //右子节点位置，如果没有右节点，默认为左节点位置 int maxChildId = arr[left] &gt;= arr[right] ? left : right; if(arr[maxChildId] &gt; arr[parentIdx])&#123; //交换父节点与左右子节点中的最大值 int temp = arr[parentIdx]; arr[parentIdx] = arr[maxChildId]; arr[maxChildId] = temp; &#125; &#125; System.out.println(&quot;Max_Heapify: &quot; + Arrays.toString(arr));&#125; 注: x&gt;&gt;1 是位运算中的右移运算, 表示右移一位, 等同于x除以2再取整, 即 x&gt;&gt;1 == Math.floor(x/2) . 复杂度以上, ①. 建立堆的过程, 从length/2 一直处理到0, 时间复杂度为O(n); ②. 调整堆的过程是沿着堆的父子节点进行调整, 执行次数为堆的深度, 时间复杂度为O(lgn); ③. 堆排序的过程由n次第②步完成, 时间复杂度为O(nlgn). 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog2 n) O(nlog2 n) O(nlog2 n) O(1) Tips: 由于堆排序中初始化堆的过程比较次数较多, 因此它不太适用于小序列。同时由于多次任意下标相互交换位置, 相同元素之间原本相对的顺序被破坏了, 因此, 它是不稳定的排序. #### 冒泡排序（Bubble Sort） 我想对于它每个学过C语言的都会了解，这可能是很多人接触的第一个排序算法。 基本思想冒泡排序（Bubble Sort）是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 算法描述冒泡排序算法的运作如下： ①. 比较相邻的元素。如果第一个比第二个大，就交换他们两个。②. 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。③. 针对所有的元素重复以上的步骤，除了最后一个。④. 持续每次对越来越少的元素重复上面的步骤①~③，直到没有任何一对数字需要比较。 代码实现冒泡排序需要两个嵌套的循环. 其中, 外层循环移动游标; 内层循环遍历游标及之后(或之前)的元素, 通过两两交换的方式, 每次只确保该内循环结束位置排序正确, 然后内层循环周期结束, 交由外层循环往后(或前)移动游标, 随即开始下一轮内层循环, 以此类推, 直至循环结束. 123456789101112131415161718192021/** * 冒泡排序 * * ①. 比较相邻的元素。如果第一个比第二个大，就交换他们两个。 * ②. 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数。 * ③. 针对所有的元素重复以上的步骤，除了最后一个。 * ④. 持续每次对越来越少的元素重复上面的步骤①~③，直到没有任何一对数字需要比较。 * @param arr 待排序数组 */public static void bubbleSort(int[] arr)&#123; for (int i = arr.length - 1; i &gt; 0; i--) &#123; //外层循环移动游标 for(int j = 0; j &lt; i; j++)&#123; //内层循环遍历游标及之后(或之前)的元素 if(arr[j] &gt; arr[j+1])&#123; int temp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = temp; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); &#125; &#125; &#125;&#125; 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(n²) O(n) O(n²) O(1) 冒泡排序是最容易实现的排序, 最坏的情况是每次都需要交换, 共需遍历并交换将近n²/2次, 时间复杂度为O(n²). 最佳的情况是内循环遍历一次后发现排序是对的, 因此退出循环, 时间复杂度为O(n). 平均来讲, 时间复杂度为O(n²). 由于冒泡排序中只有缓存的temp变量需要内存空间, 因此空间复杂度为常量O(1). Tips: 由于冒泡排序只在相邻元素大小不符合要求时才调换他们的位置, 它并不改变相同元素之间的相对顺序, 因此它是稳定的排序算法. 快速排序（Quick Sort）快速排序（Quicksort）是对冒泡排序的一种改进，借用了分治的思想，由C. A. R. Hoare在1962年提出。 基本思想快速排序的基本思想：挖坑填数+分治法。 首先选一个轴值(pivot，也有叫基准的)，通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 算法描述快速排序使用分治策略来把一个序列（list）分为两个子序列（sub-lists）。步骤为： ①. 从数列中挑出一个元素，称为”基准”（pivot）。②. 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。③. 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 递归到最底部时，数列的大小是零或一，也就是已经排序好了。这个算法一定会结束，因为在每次的迭代（iteration）中，它至少会把一个元素摆到它最后的位置去。 代码实现用伪代码描述如下： ①. i = L; j = R; 将基准数挖出形成第一个坑a[i]。②．j–，由后向前找比它小的数，找到后挖出此数填前一个坑a[i]中。③．i++，由前向后找比它大的数，找到后也挖出此数填到前一个坑a[j]中。④．再重复执行②，③二步，直到i==j，将基准数填入a[i]中 1234567891011121314151617181920212223242526272829303132/** * 快速排序（递归） * * ①. 从数列中挑出一个元素，称为&quot;基准&quot;（pivot）。 * ②. 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 * ③. 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 * @param arr 待排序数组 * @param low 左边界 * @param high 右边界 */public static void quickSort(int[] arr, int low, int high)&#123; if(arr.length &lt;= 0) return; if(low &gt;= high) return; int left = low; int right = high; int temp = arr[left]; //挖坑1：保存基准的值 while (left &lt; right)&#123; while(left &lt; right &amp;&amp; arr[right] &gt;= temp)&#123; //坑2：从后向前找到比基准小的元素，插入到基准位置坑1中 right--; &#125; arr[left] = arr[right]; while(left &lt; right &amp;&amp; arr[left] &lt;= temp)&#123; //坑3：从前往后找到比基准大的元素，放到刚才挖的坑2中 left++; &#125; arr[right] = arr[left]; &#125; arr[left] = temp; //基准值填补到坑3中，准备分治递归快排 System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); quickSort(arr, low, left-1); quickSort(arr, left+1, high);&#125; 上面是递归版的快速排序：通过把基准temp插入到合适的位置来实现分治，并递归地对分治后的两个划分继续快排。那么非递归版的快排如何实现呢？ 因为递归的本质是栈，所以我们非递归实现的过程中，可以借助栈来保存中间变量就可以实现非递归了。在这里中间变量也就是通过Pritation函数划分区间之后分成左右两部分的首尾指针，只需要保存这两部分的首尾指针即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 快速排序（非递归） * * ①. 从数列中挑出一个元素，称为&quot;基准&quot;（pivot）。 * ②. 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 * ③. 把分区之后两个区间的边界（low和high）压入栈保存，并循环①、②步骤 * @param arr 待排序数组 */public static void quickSortByStack(int[] arr)&#123; if(arr.length &lt;= 0) return; Stack&lt;Integer&gt; stack = new Stack&lt;Integer&gt;(); //初始状态的左右指针入栈 stack.push(0); stack.push(arr.length - 1); while(!stack.isEmpty())&#123; int high = stack.pop(); //出栈进行划分 int low = stack.pop(); int pivotIdx = partition(arr, low, high); //保存中间变量 if(pivotIdx &gt; low) &#123; stack.push(low); stack.push(pivotIdx - 1); &#125; if(pivotIdx &lt; high &amp;&amp; pivotIdx &gt;= 0)&#123; stack.push(pivotIdx + 1); stack.push(high); &#125; &#125;&#125;private static int partition(int[] arr, int low, int high)&#123; if(arr.length &lt;= 0) return -1; if(low &gt;= high) return -1; int l = low; int r = high; int pivot = arr[l]; //挖坑1：保存基准的值 while(l &lt; r)&#123; while(l &lt; r &amp;&amp; arr[r] &gt;= pivot)&#123; //坑2：从后向前找到比基准小的元素，插入到基准位置坑1中 r--; &#125; arr[l] = arr[r]; while(l &lt; r &amp;&amp; arr[l] &lt;= pivot)&#123; //坑3：从前往后找到比基准大的元素，放到刚才挖的坑2中 l++; &#125; arr[r] = arr[l]; &#125; arr[l] = pivot; //基准值填补到坑3中，准备分治递归快排 return l;&#125; 复杂度快速排序是通常被认为在同数量级（O(nlog2n)）的排序方法中平均性能最好的。但若初始序列按关键码有序或基本有序时，快排序反而蜕化为冒泡排序。为改进之，通常以“三者取中法”来选取基准记录，即将排序区间的两个端点与中点三个记录关键码居中的调整为支点记录。快速排序是一个不稳定的排序方法。 以下是快速排序算法复杂度:|平均时间复杂度| 最好情况 |最坏情况 |空间复杂度|| — | — | — | — ||O(nlog₂n) |O(nlog₂n) |O(n²) |O(1)（原地分区递归版）| 快速排序排序效率非常高。 虽然它运行最糟糕时将达到O(n²)的时间复杂度, 但通常平均来看, 它的时间复杂为O(nlogn), 比同样为O(nlogn)时间复杂度的归并排序还要快. 快速排序似乎更偏爱乱序的数列, 越是乱序的数列, 它相比其他排序而言, 相对效率更高. Tips: 同选择排序相似, 快速排序每次交换的元素都有可能不是相邻的, 因此它有可能打破原来值为相同的元素之间的顺序. 因此, 快速排序并不稳定. 归并排序（Merging Sort）归并排序是建立在归并操作上的一种有效的排序算法，1945年由约翰·冯·诺伊曼首次提出。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用，且各层分治递归可以同时进行。 基本思想归并排序算法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 算法描述归并排序可通过两种方式实现： 自上而下的递归自下而上的迭代一、递归法（假设序列共有n个元素）： ①. 将序列每相邻两个数字进行归并操作，形成 floor(n/2)个序列，排序后每个序列包含两个元素；②. 将上述序列再次归并，形成 floor(n/4)个序列，每个序列包含四个元素；③. 重复步骤②，直到所有元素排序完毕。二、迭代法 ①. 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列②. 设定两个指针，最初位置分别为两个已经排序序列的起始位置③. 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置④. 重复步骤③直到某一指针到达序列尾⑤. 将另一序列剩下的所有元素直接复制到合并序列尾 代码实现归并排序其实要做两件事： 分解：将序列每次折半拆分合并：将划分后的序列段两两排序合并因此，归并排序实际上就是两个操作，拆分+合并 如何合并？ L[first…mid]为第一段，L[mid+1…last]为第二段，并且两端已经有序，现在我们要将两端合成达到L[first…last]并且也有序。 首先依次从第一段与第二段中取出元素比较，将较小的元素赋值给temp[]重复执行上一步，当某一段赋值结束，则将另一段剩下的元素赋值给temp[]此时将temp[]中的元素复制给L[]，则得到的L[first…last]有序 如何分解？ 在这里，我们采用递归的方法，首先将待排序列分成A,B两组；然后重复对A、B序列 分组；直到分组后组内只有一个元素，此时我们认为组内所有元素有序，则分组结束。 这里我写了递归算法如下： 12345678910111213141516171819202122232425262728293031323334353637/** * 归并排序（递归） * * ①. 将序列每相邻两个数字进行归并操作，形成 floor(n/2)个序列，排序后每个序列包含两个元素； * ②. 将上述序列再次归并，形成 floor(n/4)个序列，每个序列包含四个元素； * ③. 重复步骤②，直到所有元素排序完毕。 * @param arr 待排序数组 */public static int[] mergingSort(int[] arr)&#123; if(arr.length &lt;= 1) return arr; int num = arr.length &gt;&gt; 1; int[] leftArr = Arrays.copyOfRange(arr, 0, num); int[] rightArr = Arrays.copyOfRange(arr, num, arr.length); System.out.println(&quot;split two array: &quot; + Arrays.toString(leftArr) + &quot; And &quot; + Arrays.toString(rightArr)); return mergeTwoArray(mergingSort(leftArr), mergingSort(rightArr)); //不断拆分为最小单元，再排序合并&#125;private static int[] mergeTwoArray(int[] arr1, int[] arr2)&#123; int i = 0, j = 0, k = 0; int[] result = new int[arr1.length + arr2.length]; //申请额外的空间存储合并之后的数组 while(i &lt; arr1.length &amp;&amp; j &lt; arr2.length)&#123; //选取两个序列中的较小值放入新数组 if(arr1[i] &lt;= arr2[j])&#123; result[k++] = arr1[i++]; &#125;else&#123; result[k++] = arr2[j++]; &#125; &#125; while(i &lt; arr1.length)&#123; //序列1中多余的元素移入新数组 result[k++] = arr1[i++]; &#125; while(j &lt; arr2.length)&#123; //序列2中多余的元素移入新数组 result[k++] = arr2[j++]; &#125; System.out.println(&quot;Merging: &quot; + Arrays.toString(result)); return result;&#125; 由上, 长度为n的数组, 最终会调用mergeSort函数2n-1次。通过自上而下的递归实现的归并排序, 将存在堆栈溢出的风险。 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(n) 从效率上看，归并排序可算是排序算法中的”佼佼者”. 假设数组长度为n，那么拆分数组共需logn，, 又每步都是一个普通的合并子数组的过程， 时间复杂度为O(n)， 故其综合时间复杂度为O(nlogn)。另一方面， 归并排序多次递归过程中拆分的子数组需要保存在内存空间， 其空间复杂度为O(n)。 和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(n log n）的时间复杂度。代价是需要额外的内存空间。 基数排序（Radix Sort）基数排序的发明可以追溯到1887年赫尔曼·何乐礼在打孔卡片制表机（Tabulation Machine）, 排序器每次只能看到一个列。它是基于元素值的每个位上的字符来排序的。 对于数字而言就是分别基于个位，十位， 百位或千位等等数字来排序。 基数排序（Radix sort）是一种非比较型整数排序算法，其原理是将整数按位数切割成不同的数字，然后按每个位数分别比较。由于整数也可以表达字符串（比如名字或日期）和特定格式的浮点数，所以基数排序也不是只能使用于整数。 基本思想它是这样实现的：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。 基数排序按照优先从高位或低位来排序有两种实现方案： MSD（Most significant digital） 从最左侧高位开始进行排序。先按k1排序分组, 同一组中记录, 关键码k1相等, 再对各组按k2排序分成子组, 之后, 对后面的关键码继续这样的排序分组, 直到按最次位关键码kd对各子组排序后. 再将各组连接起来, 便得到一个有序序列。MSD方式适用于位数多的序列。 LSD （Least significant digital）从最右侧低位开始进行排序。先从kd开始排序，再对kd-1进行排序，依次重复，直到对k1排序后便得到一个有序序列。LSD方式适用于位数少的序列。 算法描述我们以LSD为例，从最低位开始，具体算法描述如下： ①. 取得数组中的最大数，并取得位数；②. arr为原始数组，从最低位开始取每个位组成radix数组；③. 对radix进行计数排序（利用计数排序适用于小范围数的特点）； 代码实现基数排序：通过序列中各个元素的值，对排序的N个元素进行若干趟的“分配”与“收集”来实现排序。 分配：我们将L[i]中的元素取出，首先确定其个位上的数字，根据该数字分配到与之序号相同的桶中 收集：当序列中所有的元素都分配到对应的桶中，再按照顺序依次将桶中的元素收集形成新的一个待排序列L[]。对新形成的序列L[]重复执行分配和收集元素中的十位、百位…直到分配完该序列中的最高位，则排序结束 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 基数排序（LSD 从低位开始） * * 基数排序适用于： * (1)数据范围较小，建议在小于1000 * (2)每个数值都要大于等于0 * * ①. 取得数组中的最大数，并取得位数； * ②. arr为原始数组，从最低位开始取每个位组成radix数组； * ③. 对radix进行计数排序（利用计数排序适用于小范围数的特点）； * @param arr 待排序数组 */public static void radixSort(int[] arr)&#123; if(arr.length &lt;= 1) return; //取得数组中的最大数，并取得位数 int max = 0; for(int i = 0; i &lt; arr.length; i++)&#123; if(max &lt; arr[i])&#123; max = arr[i]; &#125; &#125; int maxDigit = 1; while(max / 10 &gt; 0)&#123; maxDigit++; max = max / 10; &#125; System.out.println(&quot;maxDigit: &quot; + maxDigit); //申请一个桶空间 int[][] buckets = new int[10][arr.length]; int base = 10; //从低位到高位，对每一位遍历，将所有元素分配到桶中 for(int i = 0; i &lt; maxDigit; i++)&#123; int[] bktLen = new int[10]; //存储各个桶中存储元素的数量 //分配：将所有元素分配到桶中 for(int j = 0; j &lt; arr.length; j++)&#123; int whichBucket = (arr[j] % base) / (base / 10); buckets[whichBucket][bktLen[whichBucket]] = arr[j]; bktLen[whichBucket]++; &#125; //收集：将不同桶里数据挨个捞出来,为下一轮高位排序做准备,由于靠近桶底的元素排名靠前,因此从桶底先捞 int k = 0; for(int b = 0; b &lt; buckets.length; b++)&#123; for(int p = 0; p &lt; bktLen[b]; p++)&#123; arr[k++] = buckets[b][p]; &#125; &#125; System.out.println(&quot;Sorting: &quot; + Arrays.toString(arr)); base *= 10; &#125;&#125; 复杂度 平均时间复杂度 最好情况 最坏情况 空间复杂度 O(d*(n+r)) O(d*(n+r)) O(d*(n+r)) O(n+r) 其中，d 为位数，r 为基数，n 为原数组个数。在基数排序中，因为没有比较操作，所以在复杂上，最好的情况与最坏的情况在时间上是一致的，均为 O(d*(n + r))。 基数排序更适合用于对时间, 字符串等这些整体权值未知的数据进行排序。 Tips: 基数排序不改变相同元素之间的相对顺序，因此它是稳定的排序算法。 基数排序 vs 计数排序 vs 桶排序 这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异： 基数排序：根据键值的每位数字来分配桶 计数排序：每个桶只存储单一键值 桶排序：每个桶存储一定范围的数值 总结 排序类型 平均情况 最好情况 最坏情况 辅助空间 稳定性 冒泡排序 O(n²) O(n) O(n²) O(1) 稳定 选择排序 O(n²) O(n²) O(n²) O(1) 不稳定 直接插入排序 O(n²) O(n) O(n²) O(1) 稳定 折半插入排序 O(n²) O(n) O(n²) O(1) 稳定 希尔排序 O(n^1.3) O(nlogn) O(n²) O(1) 不稳定 归并排序 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(n) 稳定 快速排序 O(nlog₂n) O(nlog₂n) O(n²) O(nlog₂n) 不稳定 堆排序 O(nlog₂n) O(nlog₂n) O(nlog₂n) O(1) 不稳定 计数排序 O(n+k) O(n+k) O(n+k) O(k) 稳定 桶排序 O(n+k) O(n+k) O(n²) O(n+k) (不)稳定 基数排序 O(d(n+k)) O(d(n+k)) O(d(n+kd)) O(n+kd) 稳定 从时间复杂度来说： (1). 平方阶O(n²)排序：各类简单排序：直接插入、直接选择和冒泡排序； (2). 线性对数阶O(nlog₂n)排序： 快速排序、堆排序和归并排序； (3). O(n1+§))排序，§是介于0和1之间的常数：希尔排序 (4). 线性阶O(n)排序：基数排序，此外还有桶、箱排序。 到此，很多人会注意到基数排序的时间复杂度是最小的，那么为什么却没有快排、堆排序流行呢？我们看看下图算法导论的相关说明： 基数排序只适用于有基数的情况，而基于比较的排序适用范围就广得多。另一方面是内存上的考虑。作为一种通用的排序方法，最好不要带来意料之外的内存开销，所以各语言的默认实现都没有用基数排序，但是不能否认基数排序在各领域的应用。 时间复杂度极限当被排序的数有一些性质的时候（比如是整数，比如有一定的范围），排序算法的复杂度是可以小于O(nlgn)的。比如： 计数排序 复杂度O( k+n) 要求：被排序的数是0~k范围内的整数 基数排序 复杂度O( d(k+n) ) 要求：d位数，每个数位有k个取值 桶排序 复杂度 O( n ) （平均） 要求：被排序数在某个范围内，并且服从均匀分布 但是，当被排序的数不具有任何性质的时候，一般使用基于比较的排序算法，而基于比较的排序算法时间复杂度的下限必须是O(nlgn)。 参考很多高效排序算法的代价是 nlogn，难道这是排序算法的极限了吗？ 说明 当原表有序或基本有序时，直接插入排序和冒泡排序将大大减少比较次数和移动记录的次数，时间复杂度可降至O（n）； 而快速排序则相反，当原表基本有序时，将蜕化为冒泡排序，时间复杂度提高为O（n2）； 原表是否有序，对简单选择排序、堆排序、归并排序和基数排序的时间复杂度影响不大。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-HashMap、Hashtable、ConcurrentHashMap的区别]]></title>
    <url>%2F2019%2F05%2F12%2FJava-HashMap%E3%80%81Hashtable%E3%80%81ConcurrentHashMap%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[以下是基于java1.8源码进行对比： 类型 HashMap Hashtable ConcurrentHashMap 数据结构 数组+链表+红黑树 数组+链表 数组+链表+红黑树 数组容量 默认容量为16，且要求底层数组的容量一定为2的整数次幂 默认容量为11，且不要求底层数组的容量一定为2的整数次幂 默认容量为16，且要求底层数组的容量一定为2的整数次幂 是否支持NULL值 支持 不支持 不支持 size获取 size字段值 count字段值 通过累加baseCount和CounterCell数组中的数量，得到元素的总个数 hash计算方式 key.hashCode() ^ (key.hashCode() &gt;&gt;&gt; 16) key.hashCode() (key.hashCode() ^ (key.hashCode() &gt;&gt;&gt; 16)) &amp; 0x7fffffff index计算方式 (n - 1) &amp; hash (hash &amp; 0x7FFFFFFF) % tab.length (hash &amp; 0x7FFFFFFF) % tab.length 是否线程安全 否 是 是 性能高低 高 低 中 线程安全实现方式 无 synchronized对象锁，get、add、remove等方法不能同时进行 CAS + synchronized保证多线程安全，可以同时执行多个方法 扩容方式 单线程进行扩容（多线程进行扩容，可能会导致死循环造成CPU100%） 单线程进行扩容 单线程对新数组进行初始化，可多线程同步复制不同节点数据到新数组 总结： HashTable是已过时的类，不建议再使用; 需要线程安全的场景，使用ConcurrentHashMap; 无需线程安全，使用HashMap;]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-ConcurrentHashMap在JDK1.7和JDK1.8的差异]]></title>
    <url>%2F2019%2F05%2F12%2FJava-ConCurrentHashMap%E5%9C%A8JDK1-7%E5%92%8CJDK1-8%E7%9A%84%E5%B7%AE%E5%BC%82%2F</url>
    <content type="text"><![CDATA[版本 JDK1.7 JDK1.8 概览 同步机制 分段锁，每个segment继承ReentrantLock CAS + synchronized保证并发更新 数据结构 数组+链表 数组+链表+红黑树 键值对 HashEntry Node put操作 当执行put方法插入数据时，根据key的hash值，在Segment数组中找到相应的位置，如果相应位置的Segment还未初始化，则通过CAS进行赋值，接着执行Segment对象的put方法通过加锁机制插入数据。多个线程同时竞争获取同一个segment锁，获取成功的线程更新map；失败的线程尝试多次获取锁仍未成功，则挂起线程，等待释放锁 访问相应的bucket时，使用sychronizeded关键字，防止多个线程同时操作同一个bucket，如果该节点的hash不小于0，则遍历链表更新节点或插入新节点；如果该节点是TreeBin类型的节点，说明是红黑树结构，则通过putTreeVal方法往红黑树中插入节点；更新了节点数量，还要考虑扩容和链表转红黑树 size实现 统计每个Segment对象中的元素个数，然后进行累加，但是这种方式计算出来的结果并不一样的准确的。先采用不加锁的方式，连续计算元素的个数，最多计算3次：如果前后两次计算结果相同，则说明计算出来的元素个数是准确的；如果前后两次计算结果都不同，则给每个Segment进行加锁，再计算一次元素的个数； 使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据或者删除数据时，会通过addCount()方法更新baseCount。通过累加baseCount和CounterCell数组中的数量，即可得到元素的总个数； 扩容实现 某个线程获得了锁之后，单线程执行扩容操作。将数组大小翻倍，复制数据到新数组。 支持并发迁移节点，遍历table的每一个节点。如果该节点被其他线程处理过了，就会创建一个 ForwardingNode 放到该节点原位置，hash值为-1。其他线程通过判断是 ForwardingNode 就知道是否已被处理过。在复制节点数据的过程中，会通过 synchronized 锁防止多个线程同时复制同一个节点的数据。 参考链接：https://www.jianshu.com/p/e694f1e868ec]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-ConcurrentHashMap工作原理及实现]]></title>
    <url>%2F2019%2F05%2F12%2FJava-ConCurrentHashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[概述关于Java集合的小抄是这么描述： 并发优化的HashMap。 在JDK5里的经典设计，默认16把写锁（可以设置更多），有效分散了阻塞的概率。数据结构为Segment[]，每个Segment一把锁。Segment里面才是哈希桶数组。Key先算出它在哪个Segment里，再去算它在哪个哈希桶里。 也没有读锁，因为put/remove动作是个原子动作（比如put的整个过程是一个对数组元素/Entry 指针的赋值操作），读操作不会看到一个更新动作的中间状态。 但在JDK8里，Segment[]的设计被抛弃了，改为精心设计的，只在需要锁的时候加锁。 支持ConcurrentMap接口，如putIfAbsent（key，value）与相反的replace（key，value）与以及实现CAS的replace（key, oldValue, newValue）。 成员变量 table：默认为null，初始化发生在第一次插入操作，默认大小为16的数组，用来存储Node节点数据，扩容时大小总是2的幂次方。 nextTable：默认为null，扩容时新生成的数组，其大小为原数组的两倍。 sizeCtl ：默认为0，用来控制table的初始化和扩容操作，具体应用在后续会体现出来。 1234等于-1时，代表 table 正在初始化 等于-N时，表示有N-1个线程正在进行扩容操作 如果table未初始化，表示table需要初始化的大小。 如果table初始化完成，表示table的容量，默认是table大小的0.75倍，居然用这个公式算0.75（n - (n &gt;&gt;&gt; 2)）。 Node：保存key，value及key的hash值的数据结构。 其中value和next都用volatile修饰，保证并发的可见性。 1234567class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; //... 省略部分代码&#125; ForwardingNode：一个特殊的Node节点，hash值为-1，其中存储nextTable的引用。 只有table发生扩容的时候，ForwardingNode才会发挥作用，作为一个占位符放在table中表示当前节点为null或则已经被移动。 1234567final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125;&#125; 初始化数组初始化数组的时候需要判断是否有其他线程正在执行初始化，采用CAS操作更新 sizeCtl 的值。具体代码如下： 1234567891011121314151617181920212223242526private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; //如果一个线程发现sizeCtl&lt;0，意味着另外的线程执行CAS操作成功正在初始化表，当前线程只需要让出cpu时间片 if ((sc = sizeCtl) &lt; 0) Thread.yield(); //SIZECTL：表示当前对象的内存偏移量，sc表示期望值，-1表示要替换的值，设定为-1表示要初始化表了。执行CAS操作 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; //没有指定初始化容量大小，则默认为16 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; //0.75 * capacity sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; put函数与 HashMap 的 put 操作类似，主要增加多线程情况的判断。具体实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778public V put(K key, V value) &#123; return putVal(key, value, false);&#125;final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //如果tab为空则初始化table if (tab == null || (n = tab.length) == 0) tab = initTable(); //如果对应数组位置为空，则通过CAS操作进行赋值（这时候是不会加锁的）。 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; // 当前hash为MOVED表示Map在扩容，先协助扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; //hash冲突，通过 synchronized 加锁当前位置对象防止多线程更新 V oldVal = null; synchronized (f) &#123; //再次判断是否已被其他线程更新值 if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; //从链表头节点开始遍历 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; // 节点已经存在，修改链表节点的值 oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; // 节点不存在，新增节点到链表末尾 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //链表节点 &gt;= 8个则已经转换为红黑树，遍历寻找进行替换值或新增节点 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; if (binCount != 0) &#123; //如果链表节点个数 &gt;= 8，则转换为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 统计节点个数，检查是否需要扩容 addCount(1L, binCount); return null;&#125; size与mappingCount为了更好地统计size，ConcurrentHashMap提供了baseCount、counterCells两个辅助变量和一个CounterCell辅助内部类。 123456789@sun.misc.Contended static final class CounterCell &#123; volatile long value; CounterCell(long x) &#123; value = x; &#125; &#125; //ConcurrentHashMap中元素个数,但返回的不一定是当前Map的真实元素个数。基于CAS无锁更新 private transient volatile long baseCount; private transient volatile CounterCell[] counterCells; mappingCount与size方法的类似 从Java工程师给出的注释来看，应该使用mappingCount代替size方法 两个方法都没有直接返回basecount 而是统计一次这个值，而这个值其实也是一个大概的数值，因此可能在统计的时候有其他线程正在执行插入或删除操作。 12345678910111213141516171819202122232425public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125;//返回容器的大小。这个方法应该被用来代替size()方法，因为 ConcurrentHashMap的容量大小可能会大于int的最大值。public long mappingCount() &#123; long n = sumCount(); return (n &lt; 0L) ? 0L : n;&#125;//迭代counterCells来统计sum的过程final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value;//所有counter的值求和 &#125; &#125; return sum;&#125; 数组扩容当ConcurrentHashMap中table元素个数达到了容量阈值（sizeCtl）时，则需要进行扩容操作。在put操作时最后一个会调用addCount(long x, int check)，该方法主要做两个工作：1.更新baseCount；2.检测是否需要扩容操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; // s = b + x，完成baseCount++操作； if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; // 多线程CAS发生失败时执行 fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; //判断是否进行扩容 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; //有线程正在进行扩容初始化nextTable操作，直接break if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //nextTable已经初始化，协助扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //扩容，对nextTable进行初始化 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125; &#125; addCount 函数保证只有单线程对 nextTable 进行初始化操作。 transfer()方法为ConcurrentHashMap扩容操作的核心方法。由于ConcurrentHashMap支持多线程扩容，而且也没有进行加锁，所以实现会变得有点儿复杂。整个扩容分为两部分： 构建一个nextTable，大小为table的两倍。 把table的数据复制到nextTable中。(这里允许多线程同时复制)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; // 每核处理的量小于16，则强制赋值16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating try &#123; @SuppressWarnings(&quot;unchecked&quot;) //构建一个nextTable对象，其容量为原来容量的两倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; // 连接点指针，用于标志位（fwd的hash值为-1，fwd.nextTable=nextTab） ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); // 当advance == true时，表明该节点已经处理过了 boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 控制 --i ,遍历原hash表中的节点 while (advance) &#123; int nextIndex, nextBound; if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; i = -1; advance = false; &#125; // 用CAS计算得到的transferIndex else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; // 已经完成所有节点复制了 if (finishing) &#123; nextTable = null; table = nextTab; // table 指向nextTable sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); //扩容阈值设置为原来容量的1.5倍 依然相当于现在容量的0.75倍 return; // 跳出死循环， &#125; // CAS 更扩容阈值，在这里面sizectl值减一，说明新加入一个线程参与到扩容操作 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit &#125; &#125; // 遍历的节点为null，则放入到ForwardingNode 指针节点 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // f.hash == -1 表示遍历到了ForwardingNode节点，意味着该节点已经处理过了 // 这里是控制并发扩容的核心 else if ((fh = f.hash) == MOVED) advance = true; // already processed else &#123; // 节点加锁,避免多线程复制同一个节点 synchronized (f) &#123; // 节点复制工作 if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // fh &gt;= 0 ,表示为链表节点 if (fh &gt;= 0) &#123; // 构造两个链表 一个是原链表 另一个是原链表的反序排列 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; // 在nextTable i 位置处插上链表 setTabAt(nextTab, i, ln); // 在nextTable i + n 位置处插上链表 setTabAt(nextTab, i + n, hn); // 在table i 位置处插上ForwardingNode 表示该节点已经处理过了 setTabAt(tab, i, fwd); // advance = true 可以执行--i动作，遍历节点 advance = true; &#125; // 如果是TreeBin，则按照红黑树进行处理，处理逻辑与上面一致 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // 扩容后树节点个数若&lt;=6，将树转链表 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125; &#125; 上面的源码有点儿长，稍微复杂了一些，在这里我们抛弃它多线程环境，我们从单线程角度来看： 为每个内核分任务，并保证其不小于16 检查nextTable是否为null，如果是，则初始化nextTable，使其容量为table的两倍 循环变量直到finished，利用tabAt方法获得i位置的元素（支持多线程复制） 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上。并将ForwardingNode 插入原节点位置，代表已经处理过了 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要unTreeify()操作，把处理的结果分别放在nextTable的i和i+n的位置上。并插入ForwardingNode 节点 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。 在多线程环境下，ConcurrentHashMap用两点来保证正确性：ForwardingNode和synchronized。 当一个线程遍历到的节点如果是ForwardingNode，则继续往后遍历。 如果不是，则将该节点加锁，防止其他线程进入，完成后设置ForwardingNode节点。 当其他线程处理该节点时可以看到已经处理过了，如此交叉进行，高效而又安全。 helpTransfer在添加、删除等方法里面都会调用，当前优先协助扩容。helpTransfer()方法为协助扩容方法，当调用该方法的时候，nextTable一定已经创建了，所以该方法主要则是进行复制工作。 12345678910111213141516171819final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; int sc; if (tab != null &amp;&amp; (f instanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; int rs = resizeStamp(tab.length); while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab); break; &#125; &#125; return nextTab; &#125; return table; &#125; 转换红黑树用于将过长的链表转换为TreeBin对象。但是他并不是直接转换，而是进行一次容量判断。 如果容量没有达到转换的要求(table.length&lt;64)，直接进行扩容操作并返回； 如果满足条件才链表的结构抓换为TreeBin ，这与HashMap不同的是： 1.根据table中index位置Node链表，重新生成一个hd为头结点的TreeNode 2.根据hd头结点，生成TreeBin树结构，并用TreeBin替换掉原来的Node对象。 123456789101112131415161718192021222324252627private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY)//如果table.length&lt;64 就扩大一倍 返回 tryPresize(n &lt;&lt; 1); else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; //构造了一个TreeBin对象 把所有Node节点包装成TreeNode放进去 for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null);//这里只是利用了TreeNode封装 而没有利用TreeNode的next域和parent域 if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; //在原来index的位置 用TreeBin替换掉原来的Node对象 setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125; &#125; get函数读取操作，不需要同步控制，比较简单 空tab，直接返回null 计算hash值，找到相应的bucket位置，为node节点直接返回，否则返回null12345678910111213141516171819public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 参考链接：https://blog.csdn.net/programmer_at/article/details/79715177#141-addcounthttps://blog.csdn.net/u010723709/article/details/48007881https://www.jianshu.com/p/c0642afe03e0http://cmsblogs.com/?p=2283]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-HashSet工作原理及实现]]></title>
    <url>%2F2019%2F05%2F12%2FJava-HashSet%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[Set概述关于Java集合的小抄是这么描述：所有Set几乎都是内部用一个Map来实现, 因为Map里的KeySet就是一个Set，而value是假值，全部使用同一个Object即可。 Set的特征也继承了那些内部的Map实现的特征。 HashSet：内部是HashMap。 LinkedHashSet：内部是LinkedHashMap。 TreeSet：内部是TreeMap的SortedSet。 ConcurrentSkipListSet：内部是ConcurrentSkipListMap的并发优化的SortedSet。 CopyOnWriteArraySet：内部是CopyOnWriteArrayList的并发优化的Set，利用其addIfAbsent（）方法实现元素去重，如前所述该方法的性能很一般。 基本操作我们可以看到HashSet类有这么一个属性 1private transient HashMap&lt;E,Object&gt; map; HashSet 所有的操作都是基于这个 map 展开的，map 的 value 值是空的 Object 对象。 1234567891011121314private static final Object PRESENT = new Object();public boolean add(E e) &#123; return map.put(e, PRESENT)==null;&#125;public boolean remove(Object o) &#123; return map.remove(o)==PRESENT;&#125;public boolean contains(Object o) &#123; return map.containsKey(o);&#125;public int size() &#123; return map.size();&#125; 总结HashSet 利用 HashMap 的 key 不能重复特性实现，所有操作都是基于 HashMap 实现的。其他 Set 的实现方法也都是基于不同的 Map 实现的，就不一一讲述了。]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-TreeMap工作原理及实现]]></title>
    <url>%2F2019%2F05%2F09%2FJava-TreeMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[TreeMap概述以红黑树实现，红黑树又叫自平衡二叉树： 对于任一节点而言，其到叶节点的每一条路径都包含相同数目的黑结点。 上面的规定，使得树的层数不会差的太远，使得所有操作的复杂度不超过 O（lgn），但也使得插入，修改时要复杂的左旋右旋来保持树的平衡。 支持iterator（）时按Key值排序，可按实现了Comparable接口的Key的升序排序，或由传入的Comparator控制。可想象的，在树上插入/删除元素的代价一定比HashMap的大。 支持SortedMap接口，如firstKey（），lastKey（）取得最大最小的key，或sub（fromKey, toKey）, tailMap（fromKey）剪取Map的某一段。 put函数如果存在的话，old value被替换；如果不存在的话，则新添一个节点，然后对做红黑树的平衡操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V put(K key, V value) &#123; //获取根节点 TreeMap.Entry&lt;K,V&gt; t = root; //如果根节点为空则新建树 if (t == null) &#123; compare(key, key); // type (and possibly null) check root = new TreeMap.Entry&lt;&gt;(key, value, null); size = 1; modCount++; return null; &#125; int cmp; TreeMap.Entry&lt;K,V&gt; parent; // 根据comparator获取key的父节点 Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) &#123; do &#123; parent = t; cmp = cpr.compare(key, t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; else &#123; if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; do &#123; parent = t; cmp = k.compareTo(t.key); if (cmp &lt; 0) t = t.left; else if (cmp &gt; 0) t = t.right; else return t.setValue(value); &#125; while (t != null); &#125; //节点不存在则新建 TreeMap.Entry&lt;K,V&gt; e = new TreeMap.Entry&lt;&gt;(key, value, parent); if (cmp &lt; 0) parent.left = e; else parent.right = e; //红黑树平衡调整 fixAfterInsertion(e); size++; modCount++; return null;&#125; get函数通过Comparable对比节点，当比较值等于0时就是key值对应的Entry。get函数则相对来说比较简单，以log(n)的复杂度进行get。 1234567891011121314151617181920212223242526272829303132333435363738394041424344public V get(Object key) &#123; Entry&lt;K,V&gt; p = getEntry(key); return (p==null ? null : p.value);&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; // Offload comparator-based version for sake of performance if (comparator != null) return getEntryUsingComparator(key); if (key == null) throw new NullPointerException(); @SuppressWarnings(&quot;unchecked&quot;) Comparable&lt;? super K&gt; k = (Comparable&lt;? super K&gt;) key; Entry&lt;K,V&gt; p = root; while (p != null) &#123; int cmp = k.compareTo(p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; &#125; return null;&#125;final Entry&lt;K,V&gt; getEntryUsingComparator(Object key) &#123; @SuppressWarnings(&quot;unchecked&quot;) K k = (K) key; Comparator&lt;? super K&gt; cpr = comparator; if (cpr != null) &#123; Entry&lt;K,V&gt; p = root; while (p != null) &#123; int cmp = cpr.compare(k, p.key); if (cmp &lt; 0) p = p.left; else if (cmp &gt; 0) p = p.right; else return p; &#125; &#125; return null;&#125; successor后继TreeMap是如何保证其迭代输出是有序的呢？其实从宏观上来讲，就相当于树的中序遍历(LDR)。我们先看一下迭代输出的步骤 123for(Entry&lt;Integer, String&gt; entry : tmap.entrySet()) &#123; System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 根据The enhanced for statement，for语句会做如下转换为： 1234for(Iterator&lt;Map.Entry&lt;String, String&gt;&gt; it = tmap.entrySet().iterator() ; tmap.hasNext(); ) &#123; Entry&lt;Integer, String&gt; entry = it.next(); System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 在it.next()的调用中会使用nextEntry调用successor这个是过的后继的重点，具体实现如下： 1234567891011121314151617181920212223static &lt;K,V&gt; TreeMap.Entry&lt;K,V&gt; successor(Entry&lt;K,V&gt; t) &#123; if (t == null) return null; else if (t.right != null) &#123; // 有右子树的节点，后继节点就是右子树的“最左节点” // 因为“最左子树”是右子树的最小节点 Entry&lt;K,V&gt; p = t.right; while (p.left != null) p = p.left; return p; &#125; else &#123; // 如果右子树为空，则寻找当前节点所在左子树的第一个祖先节点 // 因为左子树找完了，根据LDR该D了 Entry&lt;K,V&gt; p = t.parent; Entry&lt;K,V&gt; ch = t; // 保证左子树 while (p != null &amp;&amp; ch == p.right) &#123; ch = p; p = p.parent; &#125; return p; &#125;&#125; 怎么理解这个successor呢？只要记住，这个是中序遍历就好了，L-D-R。具体细节如下： a. 空节点，没有后继b. 有右子树的节点，后继就是右子树的“最左节点”c. 无右子树的节点，后继就是该节点所在左子树的第一个祖先节点 a.好理解，不过b, c，有点像绕口令啊，没关系，上图举个例子就懂了！ 有右子树的节点，节点的下一个节点，肯定在右子树中，而右子树中“最左”的那个节点则是右子树中最小的一个，那么当然是右子树的“最左节点”，就好像下图所示：无右子树的节点，先找到这个节点所在的左子树(右图)，那么这个节点所在的左子树的父节点(绿色节点)，就是下一个节点。 参考博客：https://yikun.github.io/2015/04/06/Java-TreeMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/http://calvin1978.blogcn.com/articles/collection.html]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-LinkedList工作原理及实现]]></title>
    <url>%2F2019%2F05%2F09%2FJava-LinkedList%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[LinkedList概述以双向链表实现。链表无容量限制，但双向链表本身使用了更多空间，也需要额外的链表指针操作。 按下标访问元素—get(i)/set(i,e) 要悲剧的遍历链表将指针移动到位(如果i&gt;数组大小的一半，会从末尾移起)。 插入、删除元素时修改前后节点的指针即可，但还是要遍历部分链表的指针才能移动到下标所指的位置，只有在链表两头的操作—add(), addFirst(),removeLast()或用iterator()上的remove()能省掉指针的移动。 LinkedList是一个简单的数据结构，与ArrayList不同的是，他是基于链表实现的。 Node类LinkedList的Node类是一个基本的双向链表 1234567891011private static class Node&lt;E&gt; &#123; E item; LinkedList.Node&lt;E&gt; next; LinkedList.Node&lt;E&gt; prev; Node(LinkedList.Node&lt;E&gt; prev, E element, LinkedList.Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125;&#125; add函数将新增的Node关联到链表最后的位置，实现如下： 123456789101112131415public boolean add(E e) &#123; linkLast(e); return true;&#125;void linkLast(E e) &#123; final Node&lt;E&gt; l = last; final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); last = newNode; if (l == null) first = newNode; else l.next = newNode; size++; modCount++;&#125; get函数12345678910111213141516171819public E get(int index) &#123; //判断是否下标越界 checkElementIndex(index); return node(index).item;&#125;Node&lt;E&gt; node(int index) &#123; //判断index是前半部分还是后半部分，找到对应位置数据 if (index &lt; (size &gt;&gt; 1)) &#123; Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; remove函数主要是通过prev.next = next;next.prev = prev;将前后Node关联起来，将index对应的Node从链表内移除。 12345678910111213141516171819202122232425262728public E remove(int index) &#123; checkElementIndex(index); return unlink(node(index));&#125;E unlink(Node&lt;E&gt; x) &#123; final E element = x.item; final Node&lt;E&gt; next = x.next; final Node&lt;E&gt; prev = x.prev; if (prev == null) &#123; first = next; &#125; else &#123; prev.next = next; x.prev = null; &#125; if (next == null) &#123; last = prev; &#125; else &#123; next.prev = prev; x.next = null; &#125; x.item = null; size--; modCount++; return element;&#125; 参考博客：http://calvin1978.blogcn.com/articles/collection.html]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-ArrayList工作原理及实现]]></title>
    <url>%2F2019%2F05%2F09%2FJava-ArrayList%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[ArrayList描述以数组实现。节约空间，但数组有容量限制。超出限制时会增加50%容量，用System.arraycopy()复制到新的数组，因此最好能给出数组大小的预估值。默认第一次插入元素时创建大小为10的数组。 按数组下标访问元素—get(i)/set(i,e) 的性能很高，这是数组的基本优势。 直接在数组末尾加入元素—add(e)的性能也高，但如果按下标插入、删除元素—add(i,e), remove(i), remove(e)，则要用System.arraycopy()来移动部分受影响的元素，性能就变差了，这是基本劣势。 add函数add函数代码如下： 12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125; ensureCapacityInternal自动扩容核心代码，具体实现如下： 123456789101112131415161718192021222324252627282930private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; //判断elementData是否等于空数组 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; //需求容量和默认容量相比，返回比较大的 return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 需求容量超过了数组容量，进行扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;private void grow(int minCapacity) &#123; // overflow-conscious code int oldCapacity = elementData.length; // 扩展为原来的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 如果扩为1.5倍还不满足需求，直接扩为需求容量 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 将数据拷贝到新数组上 elementData = Arrays.copyOf(elementData, newCapacity);&#125; ArrayList大小不满足需求时，大小将会自动扩容为原来的1.5倍。 get函数实现就比较简单，直接获取数组指定下标的元素返回即可。代码实现如下： 12345678910public E get(int index) &#123; //判断是否下标越界 rangeCheck(index); return elementData(index);&#125;E elementData(int index) &#123; //直接返回下标对应数据 return (E) elementData[index];&#125; remove函数12345678910111213141516public E remove(int index) &#123; //判断是否下标越界 rangeCheck(index); modCount++; //获取对应下标数据 E oldValue = elementData(index); //将下标后的数据前进一格 int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue;&#125; 其他函数还有一些其他函数，就简单说明下其实现。 contains：遍历数组，判断是否存在indexOf：遍历数组，返回下标addAll：判断数组大小是否满足需求，不满足则自动进行扩容。 参考博客：http://yikun.github.io/2015/04/04/Java-ArrayList%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java-HashMap工作原理及实现]]></title>
    <url>%2F2019%2F05%2F09%2FHashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[概述 从本文你可以学习到： 什么时候会使用HashMap？他有什么特点？ 你知道HashMap的工作原理吗？ 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？ 你知道hash的实现吗？为什么要这样实现？ 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？ 当我们执行下面的操作时： 123456789101112HashMap&amp;lt;String, Integer&amp;gt; map = new HashMap&amp;lt;String, Integer&amp;gt;();map.put(&quot;语文&quot;, 1);map.put(&quot;数学&quot;, 2);map.put(&quot;英语&quot;, 3);map.put(&quot;历史&quot;, 4);map.put(&quot;政治&quot;, 5);map.put(&quot;地理&quot;, 6);map.put(&quot;生物&quot;, 7);map.put(&quot;化学&quot;, 8);for(Entry&amp;lt;String, Integer&amp;gt; entry : map.entrySet()) &#123;System.out.println(entry.getKey() + &quot;: &quot; + entry.getValue());&#125; 运行结果是 12345678政治: 5生物: 7历史: 4数学: 2化学: 8语文: 1英语: 3地理: 6 发生了什么呢？下面是一个大致的结构，希望我们对HashMap的结构有一个感性的认识： 在官方文档中是这样描述HashMap的：Hash table based implementation of the Map interface. This implementation provides all of the optional map operations, and permits null values and the null key. (The HashMap class is roughly equivalent to Hashtable, except that it is unsynchronized and permits nulls.) This class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time. 几个关键的信息：基于Map接口实现、允许null键/值、非同步、不保证有序(比如插入的顺序)、也不保证序不随时间变化。 两个重要的参数 在HashMap中有两个很重要的参数，容量(Capacity)和负载因子(Load factor)Initial capacity The capacity is the number of buckets in the hash table, The initial capacity is simply the capacity at the time the hash table is created.Load factor The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased.简单的说，Capacity就是buckets的数目，Load factor就是buckets填满程度的最大比例。如果对迭代性能要求很高的话不要把capacity设置过大，也不要把load factor设置过小。当bucket填充的数目（即hashmap中元素的个数）大于capacity * load factor时就需要调整buckets的数目为当前的2倍。 put函数的实现 put函数大致的思路为： 对key的hashCode()做hash，然后再计算index; 如果没碰撞直接放到bucket里； 如果碰撞了，以链表的形式存在buckets后； 如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树； 如果节点已经存在就替换old value(保证key的唯一性) 如果bucket满了(超过load factor * current capacity)，就要resize。具体代码的实现如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public V put(K key, V value) &#123; // 对key的hashCode()做hash return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果tab为空则创建 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 计算key对应的index if ((p = tab[i = (n - 1) &amp; hash]) == null) //节点不存在，直接添加到哈希桶数组 tab[i] = newNode(hash, key, value, null); else &#123; // 节点存在（哈希碰撞） Node&lt;K,V&gt; e; K k; //如果链表的第一个就是查询值则直接返回 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 该链表转化为红黑树（jdk1.8之后） else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 依然是链表，链表数据没有超过8个 else &#123; //单向链表，一直向后获取直到查询到对应key值 for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 覆盖旧值并返回 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 超过 容量*负载因子(0.75)，调用 resize() 容量扩容为原来的两倍 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; get函数的实现在理解了put之后，get就很简单了。大致思路如下： bucket里的第一个节点，直接命中； 如果有冲突，则通过key.equals(k)去查找对应的entry若为树，则在树中通过key.equals(k)查找，O(logn)；若为链表，则在链表中通过key.equals(k)查找，O(n)。 具体代码的实现如下： 1234567891011121314151617181920212223242526272829public V get(Object key) &#123; Node&lt;K,V&gt; e; // 对key的hashCode()做hash return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //tab如果为空直接返回null if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 直接命中对应index上的值 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 未命中 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; hash函数的实现 在get和put的过程中，计算下标时，先对hashCode进行hash操作，然后再通过hash值进一步计算下标，如下图所示： 在对hashCode()计算hash时具体实现是这样的： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &amp;gt;&amp;gt;&amp;gt; 16);&#125; 可以看到这个函数大概的作用就是：高16bit不变，低16bit和高16bit做了一个异或。其中代码注释是这样写的：Computes key.hashCode() and spreads (XORs) higher bits of hash to lower. Because the table uses power-of-two masking, sets of hashes that vary only in bits above the current mask will always collide. (Among known examples are sets of Float keys holding consecutive whole numbers in small tables.) So we apply a transform that spreads the impact of higher bits downward. There is a tradeoff between speed, utility, and quality of bit-spreading. Because many common sets of hashes are already reasonably distributed (so don’t benefit from spreading), and because we use trees to handle large sets of collisions in bins, we just XOR some shifted bits in the cheapest possible way to reduce systematic lossage, as well as to incorporate impact of the highest bits that would otherwise never be used in index calculations because of table bounds.在设计hash函数时，因为目前的table长度n为2的幂，而计算下标的时候，是这样实现的(使用&amp;位操作，而非%求余)： 1(n - 1) &amp;amp; hash 设计者认为这方法很容易发生碰撞。为什么这么说呢？不妨思考一下，在n - 1为15(0x1111)时，其实散列真正生效的只是低4bit的有效位，当然容易碰撞了。因此，设计者想了一个顾全大局的方法(综合考虑了速度、作用、质量)，就是把高16bit和低16bit异或了一下。设计者还解释到因为现在大多数的hashCode的分布已经很不错了，就算是发生了碰撞也用O(logn)的tree去做了。仅仅异或一下，既减少了系统的开销，也不会造成的因为高位没有参与下标的计算(table长度比较小时)，从而引起的碰撞。如果还是产生了频繁的碰撞，会发生什么问题呢？作者注释说，他们使用树来处理频繁的碰撞(we use trees to handle large sets of collisions in bins)，在JEP-180中，描述了这个问题：Improve the performance of java.util.HashMap under high hash-collision conditions by using balanced trees rather than linked lists to store map entries. Implement the same improvement in the LinkedHashMap class. 之前已经提过，在获取HashMap的元素时，基本分两步： 首先根据hashCode()做hash，然后确定bucket的index； 如果bucket的节点的key不是我们需要的，则通过keys.equals()在链中找。 在Java 8之前的实现中是用链表解决冲突的，在产生碰撞的情况下，进行get时，两步的时间复杂度是O(1)+O(n)。因此，当碰撞很厉害的时候n很大，O(n)的速度显然是影响速度的。因此在Java 8中，利用红黑树替换链表，这样复杂度就变成了O(1)+O(logn)了，这样在n很大的时候，能够比较理想的解决这个问题，在Java 8：HashMap的性能提升一文中有性能测试的结果。 RESIZE的实现 当put时，如果发现目前的bucket占用程度已经超过了Load Factor所希望的比例，那么就会发生resize。在resize的过程，简单的说就是把bucket扩充为2倍，之后重新计算index，把节点再放到新的bucket中。resize的注释是这样描述的：Initializes or doubles table size. If null, allocates in accord with initial capacity target held in field threshold. Otherwise, because we are using power-of-two expansion, the elements from each bin must either stay at same index, or move with a power of two offset in the new table.大致意思就是说，当超过限制的时候会resize，然而又因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。怎么理解呢？例如我们从16扩展为32时，具体的变化如下所示： 因此元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 下面是代码的具体实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果有设置初始容量大小 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults //创建默认capacity = 16的初始容量大小 newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 总结 我们现在可以回答开始的几个问题，加深对HashMap的理解： 什么时候会使用HashMap？他有什么特点？ 是基于Map接口的实现，存储键值对时，它可以接收null的键值，是非同步的，HashMap存储着Entry(hash, key, value, next)对象。 你知道HashMap的工作原理吗？ 通过hash的方法，通过put和get存储和获取对象。存储对象时，我们将K/V传给put方法时，它调用hashCode计算hash从而得到bucket位置，进一步存储，HashMap会根据当前bucket的占用情况自动调整容量(超过Load Facotr则resize为原来的2倍)。获取对象时，我们将K传给get，它调用hashCode计算hash从而得到bucket位置，并进一步调用equals()方法确定键值对。如果发生碰撞的时候，Hashmap通过链表将产生碰撞冲突的元素组织起来，在Java 8中，如果一个bucket中碰撞冲突的元素超过某个限制(默认是8)，则使用红黑树来替换链表，从而提高速度。 你知道get和put的原理吗？equals()和hashCode()的都有什么作用？ 通过对key的hashCode()进行hashing，并计算下标( n-1 &amp; hash)，从而获得buckets的位置。如果产生碰撞，则利用key.equals()方法去链表或树中去查找对应的节点 你知道hash的实现吗？为什么要这样实现？ 在Java 1.8的实现中，是通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在bucket的n比较小的时候，也能保证考虑到高低bit都参与到hash的计算中，同时不会有太大的开销。 如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？ 如果超过了负载因子(默认0.75)，则会重新resize一个原来长度两倍的HashMap，并且重新调用hash方法。 关于Java集合的小抄中是这样描述的： 12345678910111213以Entry[]数组实现的哈希桶数组，用Key的哈希值取模桶数组的大小可得到数组下标。插入元素时，如果两条Key落在同一个桶（比如哈希值1和17取模16后都属于第一个哈希桶），我们称之为哈希冲突。JDK的做法是链表法，Entry用一个next属性实现多个Entry以单向链表存放。查找哈希值为17的key时，先定位到哈希桶，然后链表遍历桶里所有元素，逐个比较其Hash值然后key值。在JDK8里，新增默认为8的阈值，当一个桶里的Entry超过閥值，就不以单向链表而以红黑树来存放以加快Key的查找速度。当然，最好还是桶里只有一个元素，不用去比较。所以默认当Entry数量达到桶数量的75%时，哈希冲突已比较严重，就会成倍扩容桶数组，并重新分配所有原来的Entry。扩容成本不低，所以也最好有个预估值。取模用与操作（hash &amp; （arrayLength-1））会比较快，所以数组的大小永远是2的N次方， 你随便给一个初始值比如17会转为32。默认第一次放入元素时的初始值是16。iterator（）时顺着哈希桶数组来遍历，看起来是个乱序 本文转载自：https://yikun.github.io/2015/04/01/Java-HashMap%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/]]></content>
      <categories>
        <category>Java集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot生产环境快速禁用Swagger2]]></title>
    <url>%2F2019%2F05%2F08%2FSpringBoot%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E5%BF%AB%E9%80%9F%E7%A6%81%E7%94%A8Swagger2%2F</url>
    <content type="text"><![CDATA[方法一使用注解@Profile({&quot;dev&quot;,&quot;test&quot;}) 表示在开发或测试环境开启，而在生产关闭。@Profile使用的值是根据spring.profiles.active指定的环境参数，可以参考上一篇博客SpringBoot入门篇之多环境配置文件。 简单介绍下@Profile注解@Profile 注解的作用在不同的场景下，给出不同的类实例。比如在生产环境中给出的 DataSource 实例和测试环境给出的 DataSource 实例是不同的。 @Profile 的使用时，一般是在@Configuration 下使用，标注在类或者方法上，标注的时候填入一个字符串（例如”dev”），作为一个场景,或者一个区分。 实际上，很少通过上面的方式激活 Spring 容器中的 Profile，通常都是让 Spring 容器自动去读取 Profile 的值，然后自动设置。这些实现通常是具体框架实现或者虚拟机参数/环境变量等相关。 方法二使用注解@ConditionalOnProperty(name = &quot;swagger.enable&quot;, havingValue = &quot;true&quot;) 然后在对应的application.properties/application.yml配置文件中添加 swagger.enable = true即可开启，生产环境不填则默认关闭Swagger。 以上两种方法都可以成功根据当前环境禁用swagger2,效果如下： 参考博客：https://cloud.tencent.com/developer/article/1362768https://www.jb51.net/article/153492.htm]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot入门篇之多环境配置文件]]></title>
    <url>%2F2019%2F05%2F08%2FSpringBoot%E5%85%A5%E9%97%A8%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[大多数系统都具有2个及以上的环境，测试和生产的最低配置吧。那我们如何在不同环境下使用不同的配置文件？ 使用spring.profiles.active来分区配置spring boot允许你通过命名约定按照一定的格式(application-{profile}.properties)来定义多个配置文件，然后在application.properties通过 spring.profiles.active来具体激活一个或者多个配置文件，如果没有没有指定任何profile的配置文件的话，spring boot默认会启动application-default.properties。 配置文件的优先级application.properties和application.yml文件可以放在一下四个位置： 外置，在相对于应用程序运行目录的/congfig子目录里。 外置，在应用程序运行的目录里 内置，在config包内 内置，在Classpath根目录 这个列表按照优先级排序，也就是说，src/main/resources/config下application.properties覆盖src/main/resources下application.properties中相同的属性，如图：此外，如果你在相同优先级位置同时有application.properties和application.yml，那么application.yml里面的属性就会覆盖application.properties里的属性。 通过命令行设置属性值给不同的环境添加不同的端口属性server.port，然后根据指定不同的spring.profiles.active来切换使用。java -jar xxx.jar –spring.profiles.active=test在命令行运行时，连续的两个减号–就是对application.properties中的属性值进行赋值的标识。所以，java -jar xxx.jar –spring.profiles.active=test 命令，等价于我们在application.properties/application.yml中添加属性spring.profiles.active=test。 参考博客：https://www.toutiao.com/i6392145028591911425/?group_id=6392140402656805122&amp;group_flags=0]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法笔记]]></title>
    <url>%2F2019%2F05%2F07%2FMarkdown%E8%AF%AD%E6%B3%95%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[快速入门MarkDown语法，常用的格式。 标题 1 到 6 个 # ，对应到标题 1 到 6 阶 列表* - + 都可以，格式如下 123* Candy.* Gum.* Booze. 效果： Candy. Gum. Booze. 有序列表1231. Red2. Green3. Blue 效果： Red Green Blue 链接格式：[链接](地址) 效果：博客 图片格式：![](图片地址)效果： 代码格式：使用``进行包围 代码块格式：使用前后各四个`包围 加粗格式：**加粗**效果：加粗 斜体格式：*斜体*效果：斜体 下划线格式：&lt;u&gt;下划线&lt;/u&gt;效果：下划线 水平线格式：* * *效果： 任务格式：* [ ] 任务效果： 任务 表格格式：| 1 | 2 || --- | --- || 3 | 4 |效果: 1 2 3 4]]></content>
      <categories>
        <category>Markdown</category>
      </categories>
  </entry>
</search>
